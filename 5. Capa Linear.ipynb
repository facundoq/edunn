{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Linear\n",
    "\n",
    "En este ejercicio debés implementar la capa `Linear`, que pesa las $I$ variables de entrada para generar $O$ valores de salida mediante la matriz de pesos $w$, de tamaño $I × O$.\n",
    "\n",
    "Entonces, dada una entrada `x` de $N×I$ valores, donde $N$ es el tamaño de lote de ejemplos, la salida de la capa es `y=x . w`, donde $.$ es el producto matricial e `y` tiene tamaño $N×O$.\n",
    "\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[[1,-1]]` (tamaño $1×2$) y la capa `Linear` tiene como parámetros `w=[[2.0, 3.0],[4.0,5.0]]` (tamaño $2×2$), entonces la salida `y` será `x . w = [ [1,-1] . [2,4], [1,-1] . [3, 5] ] = [ 1*2+ (-1)*4, 1*3+ (-1)*5] = [-2, -2] `.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación e Inicialización\n",
    "\n",
    "La capa `Linear` tiene un vector de parámetros `w`, que debe crearse en base a un tamaño de entrada y de salida de la capa, que debe establecerse al crearse.\n",
    "\n",
    "Respecto a la inicialización, lo usual es hacerlo con valores aleatorios. Para ello, deberás implementar la clase `initializers.RandomNormal`, que inicializa los parámetros con una normal de media 0 y una desviación estándar que se configura al crearse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Nombre de la capa: Linear_196\nParámetros de la capa: {'w': array([[-1.63598503e-13,  9.87121059e-13,  8.08173120e-13],\n       [ 6.77675311e-13,  6.25871941e-13,  2.41782772e-12]])}\n\nVerificar que los pesos tengan media 0 y desviación std:\n\u001b[42m\u001b[37m\u001b[2mSUCCESS\u001b[0m Mean is 8.921784412165325e-13 :) (tolerance 1e-12)\n\u001b[42m\u001b[37m\u001b[2mSUCCESS\u001b[0m Mean is 3.984046799918877e-13 :) (tolerance 1e-12)\n\u001b[42m\u001b[37m\u001b[2mSUCCESS\u001b[0m Std is 7.719318209290646e-13 :) (tolerance 1e-12)\n\u001b[42m\u001b[37m\u001b[2mSUCCESS\u001b[0m Std is 6.264961873417658e-13 :) (tolerance 1e-12)\nVerificar de que las capas tienen valores distintos:\n\u001b[42m\u001b[37mSUCCESS\u001b[0m Arrays are different :) (tolerance 1e-13)\n"
    }
   ],
   "source": [
    "# Creamos una capa Linear con 2 valores de entrada y 3 de salida\n",
    "# inicializado con valores muestreados de una normal\n",
    "# con media 0 y desviación estándar 1e-12\n",
    "\n",
    "std=1e-12\n",
    "linear1=sn.Linear(2,3,initializer=sn.initializers.RandomNormal(std))\n",
    "print(f\"Nombre de la capa: {linear1.name}\")\n",
    "print(f\"Parámetros de la capa: {linear1.get_parameters()}\")\n",
    "print()\n",
    "\n",
    "linear2 = sn.Linear(2,3,initializer=sn.initializers.RandomNormal(std))\n",
    "\n",
    "w1 = linear1.get_parameters()[\"w\"]\n",
    "w2 = linear2.get_parameters()[\"w\"]\n",
    "\n",
    "print(\"Verificar que los pesos tengan media 0 y desviación std:\")\n",
    "utils.check_mean(w1,0,tol=std)\n",
    "utils.check_mean(w2,0,tol=std)\n",
    "utils.check_std(w1,std,tol=std)\n",
    "utils.check_std(w2,std,tol=std)\n",
    "\n",
    "print(\"Verificar de que las capas tienen valores distintos:\")\n",
    "utils.check_different(w1,w2,tol=std/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "\n",
    "Ahora que sabemos como crear e inicializar objetos de la capa `Linear`, comenzamos con el método `forward`, que podrás encontrar en el archivo `dense.py` de la carpeta `simplenn`.\n",
    "\n",
    "Para verificar que la implementación de `forward` es correcta, utilizamos el inicializador `Constant`, pero luego por defecto la capa debe seguir utilizando un inicializador aleatorio como `RandomNormal`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[42m\u001b[30mSUCCESS\u001b[0m Arrays are equal :) (tolerance 1e-12)\n\u001b[42m\u001b[30mSUCCESS\u001b[0m Arrays are equal :) (tolerance 1e-12)\n"
    }
   ],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "initializer = sn.initializers.Constant(w)\n",
    "\n",
    "layer=sn.Linear(2,3,initializer=initializer)\n",
    "y = np.array([[-22, -26, -30],\n",
    "              [ 22, 26,  30]])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "initializer = sn.initializers.Constant(-w)\n",
    "layer=sn.Linear(2,3,initializer=initializer)\n",
    "utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Además del cálculo de la salida de la capa, la misma debe poder propagar hacia atrás el gradiente del error de la red. Para eso, debés implementar el método `backward` que recibe $\\frac{δE}{δy}$, es decir, las derivadas parciales del error respecto a la salida (gradiente) de esta capa , y devolver $\\frac{δE}{δx}$, las derivadas parciales del error respecto de las entradas de esta capa. \n",
    "\n",
    "## `δEδx`\n",
    "Para la capa `Bias` el cálculo del gradiente respecto de la entrada `δEδx` es simple, ya que es el mismo caso que con la capa `AddConstant`. \n",
    "\n",
    "$ \\frac{δE}{δx} =\\frac{δE}{δy} $\n",
    "\n",
    "No obstante, para esta capa también deberás implementar el gradiente con respecto a los parámetros `b`, de modo que se puedan optimizar para minimizar el error. Entonces también deberás calcular `δEδb`. Recordemos que:\n",
    "\n",
    "$\n",
    "y([x_1,x2,...,x_f])= [x_1+b_1,x_2+b_2,...,x_n+b_f]\n",
    "$\n",
    "\n",
    "Entonces, utilizando la regla de la cadena:\n",
    "$\n",
    "\\frac{δE}{δb_i} = \\frac{δE}{δy} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δ x_i+b_i}{δb_i} = \\frac{δE}{δy_i} 1 = \\frac{δE}{δy_i}  \n",
    "$\n",
    "\n",
    "Es decir,\n",
    "\n",
    "$ \\frac{δE}{δx} =\\frac{δE}{δy} $\n",
    "\n",
    "## `δEδb` \n",
    "\n",
    "En el caso del gradiente del error con respecto a `b`, la fórmula es la misma, $ \\frac{δE}{δb} =\\frac{δE}{δy}$. Esto se debe a que $\\frac{δy_i}{δb_i} = \\frac{δ(x_i+b_i)}{δb_i} = \\frac{δ(x_i+b_i)}{δx_i} =1$. \n",
    "\n",
    "Es decir, si vemos tanto a $b$ como a $x$ como entradas a la capa, $x+b$ es simétrico en $x$ y $b$ y por ende también lo son sus derivadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[104m\u001b[30mLinear_202 layer:\u001b[0m\n\u001b[42m\u001b[30mSUCCESS\u001b[0m 2100 partial derivatives checked, 0 failed (tolerance 1e-07, 100 random input samples)\n"
    }
   ],
   "source": [
    "from test import check_gradient\n",
    "\n",
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "# Test derivatives of a Linear layer with random values for `w`\n",
    "layer=sn.Linear(features_in,features_out)\n",
    "check_gradient.check_gradient_layer_random_sample(layer,input_shape,samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}