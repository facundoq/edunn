{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Layer\n",
    "\n",
    "For classification problems, it's useful to have a model that can generate probability distributions as output. In this way, for a problem with `C` classes, the model can output a vector of `C` elements `f(x) = y`, where each `y_i` is a value between 0 and 1, indicating the probability that example `x` belongs to class `i`. Furthermore, since `y` represents a distribution, it must sum to one. Formally:\n",
    "\n",
    "$$\\sum_{i=1}^C y_i = 1$$\n",
    "\n",
    "A linear regression model can generate a vector of `C` elements with the _scores_ for each class, but these values will be in the range $(-\\infty, +\\infty)$, and thus, they cannot satisfy the properties of a probability distribution mentioned above. However, we can turn those scores into a probability distribution with a $softmax$ function.\n",
    "\n",
    "In this exercise, you need to implement the `Softmax` layer, which, given a vector `x` of `C` scores per class, converts it into a vector `y` of probabilities per class. To do this, implement the Softmax function. Formally:\n",
    "\n",
    "$$y =(y_1,y_2,...,y_C) = Softmax((x_1,x_2,...,x_C)) = Softmax(x)$$\n",
    "\n",
    "Where each $y_i$ is the probability of class $i$. \n",
    "\n",
    "For example, given class scores `[-5,100,100]`, the `Softmax` function will generate probabilities `[0,0.5,0.5]`.\n",
    "\n",
    " We call layers such as `Softmax` **activations** because they have no parameters and simply modify the values of a vector/tensor, generally in some non-linear way. You have actually encountered activation functions before, in the `AddConstant` layer!\n",
    "\n",
    "Implemment `Softmax`, which can be found in `edunn/models/activations.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "The `forward` method uses the following formula for `y`:\n",
    "\n",
    "$$y= \n",
    "\\frac{[e^{x_1},...,e^{x_c}]}{e^{x_1}+...+e^{x_c}} = \n",
    "\\frac{[e^{x_1},...,e^{x_c}]}{N}$$\n",
    "\n",
    "Or, viewed element by element, each value of $y$ is defined as:\n",
    "$$y_i(x) =  \\frac{e^{x_i}}{e^{x_1}+...+e^{x_c}} $$\n",
    "\n",
    "Here, we use the exponential function ($e^x$) to transform each score in the `x` vector from the range $(-\\infty, +\\infty)$ to the range $(0, +\\infty)$ since the exponential function can only output zero or positive values.\n",
    "\n",
    "Furthermore, $e^x$ is monotonically increasing in $x$, so higher values of $x$ lead to higher values of $e^x$, meaning that if a score is high, the probability will also be high, and vice versa.\n",
    "\n",
    "Now, in addition, each element is divided by the value $N$, which is used to normalize the values, achieving:\n",
    "1. Values between 0 and 1\n",
    "2. The sum of values equals 1\n",
    "\n",
    "That is, the axioms of a probability distribution as mentioned earlier.\n",
    "\n",
    "Implement the `forward` method of the `Softmax` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,100],\n",
    "             [0,100,0.0],\n",
    "             [100,100,0.0],\n",
    "             [50,50,0.0],\n",
    "             [1,1,1],],dtype=float)\n",
    "\n",
    "layer=nn.Softmax()\n",
    "y = np.array([[ 0, 0,  1],\n",
    "               [0, 1, 0.0],\n",
    "             [0.5, 0.5, 0.0],\n",
    "              [0.5, 0.5, 0.0],\n",
    "             [1/3,1/3,1/3]],dtype=float)\n",
    "\n",
    "utils.check_same(y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "The `backward` method of the Softmax function requires several steps because, due to normalization, each output of Softmax depends on each input.\n",
    "\n",
    "To keep this notebook concise, the details of the derivative calculation can be found in [this online resource](http://facundoq.github.io/guides/en/softmax.html).\n",
    "\n",
    "Implement the `backward` method of the `Softmax` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# number of random values of x and dEdy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds 3\n",
    "layer=nn.Softmax()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
