{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "In this exercise, you need to implement the `Linear` layer, which weights the $I$ input variables to produce $O$ output values using the weight matrix $w$ of size $I × O$.\n",
    "\n",
    "## Case with 1 Input and 1 Output\n",
    "\n",
    "In this case the math is similar to the well known 2D line equation $y = wx+b$. In this case $w$, $x$, $b$ and $y$ are all scalars, and we are just multiplying $x$ by $w$ and then adding $b$. \n",
    "\n",
    "\n",
    "\n",
    "## Case with I Inputs and O Outputs\n",
    "\n",
    "In the most general case, where $w$ is a matrix that linearly combines $I$ inputs to generate $O$ outputs, then $x \\in R^{1×I}$ and $y \\in R^{1×O}$. In this case, we define both $x$ and $y$ as _row vectors_.\n",
    "$$\n",
    "x = \\left( x_1, x_2, \\dots, x_I \\right)\\\\\n",
    "y = \\left( y_1, y_2, \\dots, y_O \\right)\n",
    "$$\n",
    "\n",
    "This decision is arbitrary: we could define both as column vectors, we could define $x$ as a column vector and $y$ as a row vector, or vice versa. Given how frameworks typically work, defining them as row vectors is the most common, which implies that $w$ is a matrix of size $I×O$, and the output of the layer $y$ is defined as:\n",
    "\n",
    "$$ y = x w$$\n",
    "\n",
    "Note that:\n",
    "* $x w$ is now a matrix multiplication\n",
    "* The order between $x$ and $w$ matters because matrix multiplication is not associative\n",
    "    * A $1×I$ array ($x$) multiplied by another $I×O$ array ($w$) results in a $1×O$ array ($y$)\n",
    "    * The reverse definition, $y=wx$, would require that $x$ and $y$ be column vectors, or that $w$ has size $O×I$,\n",
    "\n",
    "\n",
    "## Batches\n",
    "\n",
    "Layers receive not a single example, but a batch of examples. \n",
    "\n",
    "Given an input `x` of $N×I$ values, where $N$ is the batch size of examples, `y` has size $N×O$. The size of $w$ is not affected; it remains $I×O$, but now it has to work for multiple examples.\n",
    "\n",
    "For example, if the input `x` is `[[1,-1]]` (size $1×2$) and the `Linear` layer has parameters `w=[[2.0, 3.0],[4.0,5.0]]` (size $2×2$), then the output `y` will be `x . w = [ [1,-1] . [2,4], [1,-1] . [3, 5] ] = [ 1*2+ (-1)*4, 1*3+ (-1)*5] = [-2, -2] `.\n",
    "\n",
    "Your goal is to implement the `forward` and `backward` methods of this layer so that it can be used in a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation and Initialization\n",
    "\n",
    "The `Linear` layer has a parameter vector `w` that must be created based on the input and output size of the layer, which should be set upon creation.\n",
    "\n",
    "Regarding initialization, it is common to initialize the weights with random values. To do this, you will need to implement the `initializers.RandomNormal` class, which initializes the parameters with a normal distribution with mean 0 and a standard deviation that is configured upon creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear layer with 2 input and 3 output values\n",
    "# Initialize it with values sampled from a normal distribution\n",
    "\n",
    "std = 1e-12\n",
    "input_dimension = 2\n",
    "output_dimension = 3\n",
    "linear1 = nn.Linear(input_dimension, output_dimension, initializer=nn.initializers.RandomNormal(std))\n",
    "print(f\"Layer name: {linear1.name}\")\n",
    "print(f\"Layer parameters: {linear1.get_parameters()}\")\n",
    "print(\"(these values should change each time you run this cell)\")\n",
    "print()\n",
    "\n",
    "linear2 = nn.Linear(input_dimension, output_dimension, initializer=nn.initializers.RandomNormal(std))\n",
    "\n",
    "w1 = linear1.get_parameters()[\"w\"]\n",
    "w2 = linear2.get_parameters()[\"w\"]\n",
    "\n",
    "\n",
    "print(\"Check that weights have mean 0 and std deviation:\")\n",
    "utils.check_mean(w1, 0, tol=std)\n",
    "utils.check_mean(w2, 0, tol=std)\n",
    "utils.check_std(w1, std, tol=std)\n",
    "utils.check_std(w2, std, tol=std)\n",
    "\n",
    "print(\"Check that two layers have different initial values for w:\")\n",
    "utils.check_different(w1, w2, tol=std/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "Now that we know how to create and initialize `Linear` layer objects, let's move on to the `forward` method, which can be found in the `edunn/models/linear.py` file.\n",
    "\n",
    "To verify that the `forward` implementation is correct, we use the `Constant` initializer. However, by default the layer should use a random initializer like `RandomNormal`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two inputs with 2 features\n",
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "initializer = nn.initializers.Constant(w)\n",
    "# Initialize a 2x3 linear layer with specific weights\n",
    "layer = nn.Linear(2, 3, initializer=initializer)\n",
    "y = np.array([[-22, -26, -30],\n",
    "              [ 22, 26,  30]])\n",
    "\n",
    "# Check the result of the `forward`\n",
    "utils.check_same(y, layer.forward(x))\n",
    "\n",
    "# Repeat the above with different weights\n",
    "initializer = nn.initializers.Constant(-w)\n",
    "layer = nn.Linear(2, 3, initializer=initializer)\n",
    "utils.check_same(-y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "In the implementation of the `Bias` layer, the derivative formulas were relatively simple, and the complexity was mostly in how to use the framework and understand the difference between the derivative with respect to the input and the derivative with respect to the parameters.\n",
    "\n",
    "The `backward` method of the `Linear` layer requires calculating $\\frac{δE}{δy}$ and $\\frac{δE}{δw}$. In terms of computational tools, the implementation is very similar to that of the `Bias` layer, but the derivative formulas are more complicated.\n",
    "\n",
    "To avoid making this notebook too long, we leave [a detailed explanation of the derivative calculations](http://facundoq.github.io/guides/en/linear.html) for both $\\frac{δE}{δx}$ and $\\frac{δE}{δw}$. That'll help you implement the `backward` method for the `Linear` layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size = 2\n",
    "features_in = 3\n",
    "features_out = 5\n",
    "input_shape = (batch_size, features_in)\n",
    "\n",
    "# Test derivatives of a Linear layer with random values for `w`\n",
    "layer = nn.Linear(features_in, features_out)\n",
    "\n",
    "utils.check_gradient.common_layer(layer, input_shape, samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
