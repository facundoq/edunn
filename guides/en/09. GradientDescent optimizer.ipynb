{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a simple but effective optimization technique for training differentiable models.\n",
    "\n",
    "In each iteration of the algorithm, we calculate the derivative of the error with respect to each parameter `dEdp`, and update the weights in the opposite direction of the gradient. This update is mediated by the `α` parameter, which indicates the learning rate.\n",
    "\n",
    "The gradient descent algorithm is simple:\n",
    "\n",
    "```python\n",
    "for i in range(iterations):\n",
    "    for p in model.parameters()\n",
    "        # Use p[:] to modify the values of p\n",
    "        # and not create a new variable\n",
    "        p[:] = p - α * dEdp(x, y)\n",
    "```\n",
    "\n",
    "This pseudocode omits some cumbersome parts. In particular, the iteration over the input values `x` and output values `y` of the examples in their batched version, and the calculation of the error and derivatives `dEdp`.\n",
    "\n",
    "The `edunn` library includes the `BatchedGradientOptimizer` class, which takes care of these aspects and allows us to implement an optimizer very simply by creating a subclass of it and implementing the `optimize_batch` method. In this method, we only need to focus on optimizing the model using the gradients calculated with a batch of the dataset.\n",
    "\n",
    "For this exercise, we have created the `GradientDescent` class, which subclasses `BatchedGradientOptimizer`. Implement the crucial part of the `optimize_batch` method of `GradientDescent` to update the parameters based on the precomputed gradients.\n",
    "\n",
    "To test this optimizer, we will use a fake model and fake error that allow us to control the input to the optimizer. The flexibility of the `Model` class in `edunn` makes it very easy to do this by creating the `FakeModel` and `FakeError` classes, which effectively ignore their inputs and outputs and only serve to initialize 2 parameters with values 0 and return `[1, -1]` as derivatives for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake model with a parameter vector with an initial value of [0,0] and gradients that are always [1,-11]\n",
    "model = nn.FakeModel(parameter=np.array([0,0]), gradient=np.array([1, -1]))\n",
    "# Fake error function whose error is always 1 and derivatives are also 1\n",
    "error = nn.FakeError(error=1, derivative_value=1)\n",
    "\n",
    "# Fake dataset, which won't actually be used\n",
    "fake_samples = 3\n",
    "fake_x = np.random.rand(fake_samples, 10)\n",
    "fake_y = np.random.rand(fake_samples, 5)\n",
    "\n",
    "# Optimize the model for 1 epoch with lr=2\n",
    "optimizer = nn.GradientDescent(batch_size=fake_samples, epochs=1, lr=2, shuffle=False)\n",
    "history = optimizer.optimize(model, fake_x, fake_y, error, verbose=False)\n",
    "expected_parameters = np.array([-2, 2])\n",
    "utils.check_same(expected_parameters, model.get_parameters()[\"parameter\"])\n",
    "\n",
    "# Optimize the model for an *additional* 1 epoch with lr=2\n",
    "history = optimizer.optimize(model, fake_x, fake_y, error, verbose=False)\n",
    "expected_parameters = np.array([-4, 4])\n",
    "utils.check_same(expected_parameters, model.get_parameters()[\"parameter\"])\n",
    "    \n",
    "# Optimize the model for 3 more epochs, now with lr=1    \n",
    "optimizer = nn.GradientDescent(batch_size=fake_samples, epochs=3, lr=1, shuffle=False)\n",
    "history = optimizer.optimize(model, fake_x, fake_y, error, verbose=False)\n",
    "expected_parameters = np.array([-7, 7])\n",
    "utils.check_same(expected_parameters, model.get_parameters()[\"parameter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Linear Regression Model with Gradient Descent\n",
    "\n",
    "Now that we have all the elements, we can define and train our first `LinearRegression` model to estimate house prices using the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "import numpy as np\n",
    "from edunn import metrics, datasets\n",
    "\n",
    "x, y = datasets.load_regression(\"boston\")\n",
    "x = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "n, din = x.shape\n",
    "n, dout = y.shape\n",
    "print(\"Dataset sizes:\", x.shape, y.shape)\n",
    "\n",
    "# Network with two linear layers\n",
    "model = nn.LinearRegression(din, dout)\n",
    "error = nn.MeanError(nn.SquaredError())\n",
    "optimizer = nn.GradientDescent(lr=0.001, epochs=1000, batch_size=32)\n",
    "\n",
    "# Optimization algorithm\n",
    "history = optimizer.optimize(model, x, y, error)\n",
    "nn.plot.plot_history(history, error_name=error.name)\n",
    "\n",
    "\n",
    "print(\"Model Error:\")\n",
    "y_pred = model.forward(x)\n",
    "metrics.regression_summary(y, y_pred)\n",
    "nn.plot.regression1d_predictions(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "As an additional verification, we will calculate the optimal parameters for a linear regression model using sklearn and visualize the results. The error should be similar to our model's error (RMSE=3.27 or 3.28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(x, y)\n",
    "y_pred = model.predict(x)\n",
    "print(\"Scikit Learn Model Error:\")\n",
    "metrics.regression_summary(y, y_pred)\n",
    "print()\n",
    "\n",
    "nn.plot.regression1d_predictions(y, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbformat": 4,
  "nbformat_minor": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
