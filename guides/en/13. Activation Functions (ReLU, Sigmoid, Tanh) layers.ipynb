{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "A `neural network` is a model that combines transformations of various layers such as Linear Regression or Logistic Regression. However, to define a network, we must use _activation functions_, which allow for non-linear transformations of data. Otherwise, combining two layers of Linear Regression, for example, is equivalent to having only one layer since it can represent only a straight line. In this case, our network will not be more powerful, and it will only be more inefficient.\n",
    "\n",
    "There are different types of activation functions: we will see three of the most common ones, `relu`, `sigmoid` (or `logistic`), and `tanh` (hyperbolic tangent). Each one has different properties:\n",
    "\n",
    "* `relu`: It is efficient to calculate both its output and its derivative, and although its non-linearity is simple, it helps give the network greater approximation power. However, its output is in the range $[0,\\infty)$, so it is not suitable for some tasks.\n",
    "* `sigmoid`: It is efficient, although not as much as `relu`, and since its range is $(0,1)$, it can encode a value that can be interpreted as a probability.\n",
    "* `tanh`: It is similar to `sigmoid`, but its range is $(-1,1)$, allowing it to encode values as positive and negative, generating representations with feedback of these signs in the network.\n",
    "\n",
    "<img src=\"img/relu.png\" width=\"25%\" style=\"display:inline-block\"> <img src=\"img/sigmoid.png\" width=\"25%\" style=\"display:inline-block\"> <img src=\"img/tanh.png\" width=\"25%\" style=\"display:inline-block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Layer\n",
    "\n",
    "The `ReLU` (Rectified Linear Unit) function is extremely simple.\n",
    "\n",
    "<img src=\"img/relu.png\" width=50%>\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$ReLU(x) = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "    1 & \\text{if}  x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In terms of code, calculating `relu` for a single value is simple:\n",
    "\n",
    "```python\n",
    "def relu(x: float):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "In a shorter but slightly more difficult-to-understand form, we can write `ReLU` as:\n",
    "$$ReLU(x) = \\max(0,x)$$\n",
    "\n",
    "which also translates in code as \n",
    "\n",
    "```python\n",
    "def relu(x:float): \n",
    "    return max(x,0)\n",
    "```\n",
    "\n",
    "Keep in mind that you will receive a tensor of values. Nevertheless, `ReLU`, like other activation functions, is easy to calculate because it is applied _element-wise_. That is, `ReLU` of a vector is equivalent to applying `ReLU` to each of its values, as follows:\n",
    "\n",
    "$$ReLU((-2,4,7)) = ( ReLU(-2), ReLU(4), ReLU(7)) = (0,4,7)$$\n",
    "\n",
    "For the case of a matrix or a tensor in general, the process is the same, and numpy operators make it easy to generalize this to arbitrary tensors.\n",
    "\n",
    "Implement the `forward` method of the `ReLU` class in the `edunn/models/activations.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5,-7.2,5.3],\n",
    "             [-3.5,7.2,-5.3]])\n",
    "\n",
    "layer=nn.ReLU()\n",
    "y = np.array([[3.5,0,5.3],\n",
    "             [0,7.2,0]])\n",
    "nn.utils.check_same(y,layer.forward(x))\n",
    "\n",
    "# plot values\n",
    "nn.plot.plot_activation_function(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method of `ReLU`\n",
    "\n",
    "Fortunately, the `backward` method of `ReLU` is simple. By considering cases, we can see that if the forward method is:\n",
    "\n",
    "$$ReLU(x) = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "    x & \\text{if } x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We can derive each case separately, and then:\n",
    "$$\\frac{d ReLU(x)}{dx} = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "     1 & \\text{if } x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In the case of $x=0$, the derivative of $ReLU$ is not actually defined; however, in this case, we can redefine that derivative as 0 (or 1), and it will not significantly affect optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Number of random examples and their size to generate\n",
    "# samples of x and check derivatives\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Check derivatives of a ReLU function\n",
    "layer=nn.ReLU()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "nn.plot.plot_activation_function(nn.ReLU(), backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Layer\n",
    "\n",
    "The `sigmoid` function converts any value to the interval $(0,1)$.\n",
    "\n",
    "<img src=\"img/sigmoid.png\" width=50%>\n",
    "\n",
    "Its definition is:\n",
    "\n",
    "$$\n",
    "Sigmoid(x)= \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$ sigmoid(0)=\\frac{1}{1+1}=\\frac{1}{2}=0.5$$\n",
    "$$ sigmoid(1)=\\frac{1}{1+e^{-1}}=\\frac{1}{1+0.36}=0.73$$\n",
    "$$ sigmoid(-1)=\\frac{1}{1+e^{-(-1)}}=\\frac{1}{1+2.71}=0.26$$\n",
    "\n",
    "As we can see, the balance of the function is at the value $0$, for which the output is $0.5$; values greater than $0$ result in greater outputs, and vice versa. As a curiosity,  $sigmoid(x)=1-sigmoid(-x)$.\n",
    "\n",
    "Implement the `forward` method of the `Sigmoid` class in the `edunn/models/activations.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,-1]])\n",
    "\n",
    "layer=nn.Sigmoid()\n",
    "y = np.array([[0.5,0.73105858,0.26894142]])\n",
    "nn.utils.check_same(y,layer.forward(x),tol=1e-6)\n",
    "\n",
    "nn.plot.plot_activation_function(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method of `Sigmoid`\n",
    "\n",
    "The derivatives of the `backward` method of `Sigmoid` are simple, but we also want to simplify them further to make their computation more efficient.\n",
    "\n",
    "$$\n",
    "Sigmoid(x)= \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "With a bit of algebra, we can see get to:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d Sigmoid(x)}{dx} \n",
    "&= \\frac{d \\frac{1}{1+e^{-x}}}{dx} =\\frac{d (1+e^{-x})^{-1}}{dx} \\\\\n",
    "&= \\frac{d (1+e^{-x})^{-1}}{d(1+e^{-x})} \\frac{d (1+e^{-x})}{dx} & \\text{(chain rule with $g(x)=1+e^{-x}$)} \\\\\n",
    "&= \\frac{d  (1+e^{-x})^{-1} }{d(1+e^{-x})} (-e^{-x}) & \\text{(derivative of $1+e^{-x}$)} \\\\\n",
    "&=  -(1+e^{-x})^{-2} (-e^{-x}) & \\text{(derivative of $g(x)^{-1}=-g(x)^{-2}$)} \\\\\n",
    "&=  (1+e^{-x})^{-2} e^{-x}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Wait, that's *almost* another sigmoid in there. Let's factor it out:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(1+e^{-x})^{-2} e^{-x} \n",
    "&=  (\\frac{1}{1+e^{-x}})Â² e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 e^{-x}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "At this point, we have a nice formula for the derivative of `Sigmoid`, but as mentioned earlier. Plus, it can use the original value of `Sigmoid` for efficiency! However, to reduce numerical errors for small values of $Sigmoid(x)$ (when $x<<0$), we'll want to rewrite this a bit. Thus, we start with the last line of the above derivation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d Sigmoid(x)}{dx} &=  (1+e^{-x})^{-2} e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 (1-1+e^{-x})\\\\\n",
    "&=  Sigmoid(x)^2 (1-Sigmoid(x)^{-1})\\\\\n",
    "&=  Sigmoid(x) Sigmoid(x) (1-Sigmoid(x)^{-1})\\\\\n",
    "&=  Sigmoid(x) [Sigmoid(x) (1-Sigmoid(x)^{-1})]\\\\\n",
    "&=  Sigmoid(x) [Sigmoid(x) * 1- Sigmoid(x) Sigmoid(x)^{-1}]\\\\\n",
    "&=  Sigmoid(x) (Sigmoid(x) -1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This final formula $Sigmoid'(x) = Sigmoid(x)  (Sigmoid(x)-1)$ tells us that if we store the value of `Sigmoid(x)` during the `forward` pass, then for the `backward` pass, the derivative is simply calculated as $Sigmoid(x) (Sigmoid(x)-1)$, which only requires vector addition and multiplication.\n",
    "\n",
    "Implement the `backward` method of the `Sigmoid` class and verify it with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Number of random examples and their size to generate\n",
    "# samples of x and check derivatives\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Check derivatives of a Sigmoid function\n",
    "layer=nn.Sigmoid()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "nn.plot.plot_activation_function(layer,backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TanH Layer\n",
    "\n",
    "The `tanh` (hyperbolic tangent) function converts any value to the interval $(-1,1)$.\n",
    "\n",
    "<img src=\"img/tanh.png\" width=35%>\n",
    "\n",
    "$tanh$ is a [hyperbolic](https://en.wikipedia.org/wiki/Hyperbolic_functions) geometric function. Like $tan(x)=\\frac{cos(x)}{sin(x)}$, $tanh$ is defined in terms of hyperbolic cosine and sine:\n",
    "\n",
    "$$\n",
    "tanh(x)= \\frac{cosh(x)}{sinh(x)} = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}[t]\n",
    "tanh(0)  &= \\frac{1-1}{1+1} &&= \\frac{0}{2} &&= 0 \\\\\n",
    "tanh(1)  &= \\frac{e-e^{-1}}{e+e^{-1}} &&= \\frac{2.35}{3.08} &&= 0.76 \\\\\n",
    "tanh(1) &= \\frac{e^{-1}-e}{e^{-1}+e} &&= \\frac{-2.35}{3.08} &&= -0.76\n",
    "\\end{aligned} \n",
    " $$\n",
    "\n",
    "The balance of the function is at the value $0$, for which the output is $0$; values greater than $0$ result in greater outputs, and vice versa. Therefore, `tanh` is an odd function: $tanh(x)=-tanh(-x)$.\n",
    "\n",
    "$TanH$ is very similar to the `Sigmoid` function. In fact, with the following graph, we can see that $TanH$ is simply `Sigmoid`, but:\n",
    "\n",
    "* Multiplied by two (to convert the range $(0,1)$ to $(0,2)$).\n",
    "* Minus 1 (to convert the range $(0,2)$ to $(-1,1)$).\n",
    "* Multiplying $x$ by 2, so that we compress the $x$ axis and the curves are the same.\n",
    "\n",
    "<img src=\"img/sigmoid.png\" width=35%> \n",
    "\n",
    "Therefore [it can be defined](http://facundoq.github.io/edunn/material/en/sigmoid_tanh) directly based on `Sigmoid`:\n",
    "\n",
    "$$\n",
    "tanh(x) = sigmoid(2x)*2-1\n",
    "$$\n",
    "\n",
    "This form will be much more convenient for implementation, as we can reuse the `Sigmoid` layer for both the forward and backward passes.\n",
    "\n",
    "Implement the `forward` method of the `TanH` class in the `edunn/models/activations.py` file _using the `Sigmoid` layer_ (we have already defined a variable for it). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0.5,-0.5]])\n",
    "\n",
    "layer=nn.TanH()\n",
    "y = np.array([[ 0., 0.46211716, -0.46211716]])\n",
    "nn.utils.check_same(y,layer.forward(x),tol=1e-6)\n",
    "\n",
    "nn.plot.plot_activation_function(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method of `TanH`\n",
    "\n",
    "The derivatives of the `backward` method of `TanH` can be obtained based on those of `Sigmoid`. Since\n",
    "\n",
    "$$\n",
    "tanh(x) = sigmoid(2x)*2-1\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "tanh'(x) = (sigmoid'(2x)*2)*2 = sigmoid'(2x)*4\n",
    "$$\n",
    "\n",
    "In other words, the derivative of `tanh` simply consists of multiplying the derivative of `Sigmoid` by two.\n",
    "\n",
    "Implement the `backward` method of the `TanH` class and verify it with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Number of random examples and their size to generate\n",
    "# samples of x and check derivatives\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Check derivatives of a Sigmoid function\n",
    "layer=nn.TanH()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "nn.plot.plot_activation_function(layer,backward=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
