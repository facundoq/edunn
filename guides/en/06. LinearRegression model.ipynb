{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model: `forward`\n",
    "\n",
    "A Linear Regression model is formed by first applying a linear layer $f(x)=wx$ and then a bias layer $f(x)=x+b$, resulting in the function $f(x)=wx+b$. Instead of viewing $f(x)$ as $wx+b$, we can see it as `x -> Linear -> Bias -> y`, i.e., as a sequence of layers, each transforming the input `x` to obtain the output `y`.\n",
    "\n",
    "In terms of code, the `forward` method of a `LinearRegression` model is the composition of the `forward` methods of the `Linear` and `Bias` layers:\n",
    "```\n",
    "y_linear = linear.forward(x)\n",
    "y = bias.forward(y_linear)\n",
    "```\n",
    "\n",
    "Implement the `forward` method of the `LinearRegression` model in the `edunn/models/linear_regression.py` file. Remember, that you don't need to perform any computation per se. Instead, chain the `forward` functions from other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "b = np.array([1,2,3])\n",
    "linear_initializer = nn.initializers.Constant(w)\n",
    "bias_initializer = nn.initializers.Constant(b)\n",
    "layer=nn.LinearRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "y = np.array([[-21, -24, -27],\n",
    "              [ 23, 28,  33]])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "linear_initializer = nn.initializers.Constant(-w)\n",
    "bias_initializer = nn.initializers.Constant(-b)\n",
    "layer=nn.LinearRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model: `backward`\n",
    "\n",
    "The `backward` method of a `LinearRegression` model is the *inverse* composition of the `backward` methods of the `Linear` and `Bias` layers.\n",
    "\n",
    "That is, the derivative of the error at the output of the model first passes through `Bias`, which calculates the derivative with respect to its parameters and returns it as `dEdbias` (the only parameter is $b$ in this case). But `Bias` will also return `dEdx_bias`, the derivative of the error with respect its input. But the input to `Bias` is actuall the output of `Linear`. Therefore, we can do the same as with the `forward` to backpropagate the gradient, but in reverse order.\n",
    "\n",
    "This is the first (simple) example of the application of the *backpropagation* algorithm! This time, with only two models/layers. Later, we will generalize it with the `Sequential` model.\n",
    "\n",
    "In this case, we also help you by combining the gradient dictionaries of each layer into a single large gradient dictionary of `LinearRegression`. To achieve that, we use the `**` python operator, which unpacks a dictionary with `{**dict1, **dict2}`,  to combine them again afterwards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size = 2\n",
    "din = 3 # input dimension\n",
    "dout = 5 # output dimension\n",
    "input_shape = (batch_size, din)\n",
    "\n",
    "# Verify derivatives of a Linear Regression model\n",
    "# with random values for `w`, `b`, and `x`, the input\n",
    "layer = nn.LinearRegression(din, dout)\n",
    "\n",
    "utils.check_gradient.common_layer(layer, input_shape, samples=samples)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our first complex model! But we still can't train it. For that, we need:\n",
    "\n",
    "1. A dataset with samples (x,y)\n",
    "2. A loss function\n",
    "3. An optimization algorithm for that loss function that can run with the dataset and model.\n",
    "\n",
    "In the following guides, we will implement (2) and then (3). Using a sample dataset (1) from the `edunn` library we will test the Linear Regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
