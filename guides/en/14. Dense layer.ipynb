{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Layer\n",
    "\n",
    "The `Linear`, `Bias`, and activation layers (`Sigmoid`, `ReLU`, `TanH`, etc.) are often used together in the form `dense(x) = activation(w*x+b)`, where `activation` is an activation function. This layer is commonly referred to as `FullyConnected` or, as we'll call it here, `Dense`, and the name comes from the fact that each output of the layer depends on *all* inputs, plus a few bells and whistles.\n",
    "\n",
    "In this exercise, you need to implement the `Dense` layer. But don't do it from scratch;use the `Linear`, `Bias`, and activation layers directly without copying their code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation and Initialization\n",
    "\n",
    "The `Dense` layer should have a parameter vector `w`, another vector `b`, and a specific activation function.\n",
    "\n",
    "To implement it, we will use three *internal layers*: `Linear`, `Bias`, and the activation layer, which we'll call `Activation`. For convenience, we'll also allow specifying the activation using a string like `relu`, `sigmoid`, or `tanh`. In this case, we've already defined the constructor `__init__`, which assigns the corresponding internal layer objects `self.linear`, `self.bias`, and `self.activation`, and allows specifying the initializers for each of them.\n",
    "\n",
    "We've also provided you with the implementation of `get_parameters`, which combines the parameter dictionaries of each sub-layer into a single dictionary of gradients for the `Dense` layer.\n",
    "\n",
    "We recommend studying the code of these two methods (`__init__` and `get_parameters`) to understand how they work. They will help you implement the `forward` and `backward` methods for `Dense`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dense layer with 2 input values and 3 output values\n",
    "# and ReLU activation\n",
    "# The linear layer is initialized randomly\n",
    "# While the bias layer is initialized with zeros\n",
    "\n",
    "input_dimension=2\n",
    "output_dimension=3\n",
    "activation=\"relu\"\n",
    "dense1=nn.Dense(input_dimension,output_dimension,\n",
    "                 activation_name=\"relu\",\n",
    "                 linear_initializer=nn.initializers.RandomNormal(),\n",
    "                 bias_initializer=nn.initializers.Constant(0),\n",
    "                 )\n",
    "print(f\"Layer name: {dense1.name}\")\n",
    "print(f\"Layer's parameter w: {dense1.get_parameters()['w']}\")\n",
    "print(\"(should change every time you run this cell)\")\n",
    "print(f\"Layer's parameter b: {dense1.get_parameters()['b']}\")\n",
    "print(\"(should always be 0)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "Now that we know how to create and initialize `Dense` layer objects, let's start with the `forward` method, which you can find in the `dense.py` file in the `edunn/models` folder.\n",
    "\n",
    "To implement the forward pass, you should take the input `x` and use it to call the `forward` method of the internal layers of type `Linear`, `Bias`, and `Activation`.\n",
    "\n",
    "To verify that the `forward` implementation is correct, we use the `Constant` initializer twice, but afterward, the layer continues to use a random initializer like `RandomNormal` by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "b = np.array([1,2,3])\n",
    "linear_initializer = nn.initializers.Constant(w)\n",
    "bias_initializer = nn.initializers.Constant(b)\n",
    "layer=nn.Dense(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "y = np.array([[-21, -24, -27],\n",
    "              [ 23, 28,  33]])\n",
    "\n",
    "nn.utils.check_same(y,layer.forward(x))\n",
    "\n",
    "linear_initializer = nn.initializers.Constant(-w)\n",
    "bias_initializer = nn.initializers.Constant(-b)\n",
    "layer=nn.Dense(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "nn.utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "To implement the `backward` method, you should also call the `backward` method of the `self.linear`, `self.bias`, and `self.activation` variables in the correct order and manner. Hint: it's the reverse of the `forward` method.\n",
    "\n",
    "In this case, we also help you by combining the gradient dictionaries of each layer into a single large gradient dictionary for `Dense` using the `**dict` operator, which unpacks a dictionary, and `{**dict1, **dict2}`, which combines them again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "# Test derivatives of a Dense layer with random values for `w`\n",
    "layer=nn.Dense(features_in,features_out,activation_name='relu')\n",
    "\n",
    "nn.utils.check_gradient.common_layer(layer,input_shape,samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
