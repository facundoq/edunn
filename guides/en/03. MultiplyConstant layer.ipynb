{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiplyConstant Layer\n",
    "\n",
    "In this exercise, you need to implement the `MultiplyConstant` layer, which multiplies each of its inputs by a constant value to generate its output. It works similarly to `AddConstant`, but in this case, it performs multiplication instead of addition, and thus its derivatives are slightly more complicated.\n",
    "\n",
    "For example, if the input `x` is `[3.5, -7.2, 5.3]` and the `MultiplyConstant` layer is created with the constant `2`, then the output `y` will be `[7.0, -14.4, 10.6]`.\n",
    "\n",
    "Your goal is to implement the `forward` and `backward` methods for this layer so that it can be used in a neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "The `forward` method calculates the output `y` based on the input `x`, as explained above. In formal terms, if the constant to be multiplied is $C$ and the input to the layer is $x = [x_1, x_2, ..., x_n]$, then the output $y$ is:\n",
    "\n",
    "$\n",
    "y([x_1, x_2, ..., x_n]) = [x_1 * C, x_2 * C, ..., x_n * C]\n",
    "$\n",
    "\n",
    "We start with the `forward` method of the `MultiplyConstant` class, which can be found in the `activations.py` file in the `edunn/models` folder. You need to complete the code between the comments:\n",
    "\n",
    "```\n",
    "### YOUR IMPLEMENTATION START  ###\n",
    "```\n",
    "and\n",
    "\n",
    "```\n",
    "### YOUR IMPLEMENTATION END  ###\n",
    "```\n",
    "\n",
    "Then, verify your implementation with the following cell that checks with a layer that multiplies by 2 and another that multiplies by -2. If both checks are correct, you will see two messages with <span style='background-color:green;color:white;'>success</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5, -7.2, 5.3],\n",
    "              [-3.5, 7.2, -5.3]])\n",
    "\n",
    "layer = nn.MultiplyConstant(2)\n",
    "y = np.array([[7.0, -14.4, 10.6],\n",
    "              [-7.0, 14.4, -10.6]])\n",
    "utils.check_same(y, layer.forward(x))\n",
    "\n",
    "layer = nn.MultiplyConstant(-2)\n",
    "y = -np.array([[7.0, -14.4, 10.6],\n",
    "               [-7.0, 14.4, -10.6]])\n",
    "utils.check_same(y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "In addition to calculating the output of the layer, it must be able to propagate the gradient of the network's error backward. To do this, you need to implement the `backward` method, which receives $\\frac{δE}{δy}$, the partial derivatives of the error with respect to the output (gradient) of this layer, and returns $\\frac{δE}{δx}$, the partial derivatives of the error with respect to the inputs of this layer.\n",
    "\n",
    "For the `MultiplyConstant` layer, calculating the gradient is slightly more complicated than for `AddConstant`. Let's go through the derivation:\n",
    "\n",
    "We have:\n",
    "\n",
    "$y_i(x) = x_i * C$\n",
    "\n",
    "And we want to calculate $\\frac{δE}{δx_i}$. Using the chain rule, we can write:\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i}$\n",
    "\n",
    "Now, $\\frac{δy}{δx_i}$ is straightforward as it's just the constant $C$:\n",
    "\n",
    "$\\frac{δy}{δx_i} = C$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * C$\n",
    "\n",
    "In vector form for the input vector $x$, we get:\n",
    "\n",
    "$\\frac{δE}{δx} = [\\frac{δE}{δy_1} * C, \\frac{δE}{δy_2} * C, ..., \\frac{δE}{δy_n} * C] = \\frac{δE}{δy} * C$\n",
    "\n",
    "So, the layer simply propagates the gradients from the next layer, but multiplied by the constant $C$.\n",
    "\n",
    "Complete the code in the `backward` function of the `MultiplyConstant` layer between the comments:\n",
    "\n",
    "```\n",
    "### YOUR IMPLEMENTATION START  ###\n",
    "```\n",
    "and\n",
    "\n",
    "```\n",
    "### YOUR IMPLEMENTATION END  ###\n",
    "```\n",
    "\n",
    "Then, verify with the following cell for a layer that multiplies by 3 and another that multiplies by -4. If both checks are correct, you will see two messages with <span style='background-color:green;color:white;'>success</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "# Number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape = (5, 2)\n",
    "\n",
    "# Test derivatives of a MultiplyConstant layer that multiplies by 3\n",
    "layer = nn.MultiplyConstant(3)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)\n",
    "\n",
    "# Test derivatives of a MultiplyConstant layer that multiplies by -4\n",
    "layer = nn.MultiplyConstant(-4)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
