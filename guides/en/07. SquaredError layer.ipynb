{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared Error Layer\n",
    "\n",
    "In this exercise, you need to implement the error layer `SquaredError` found in `eduu/models/squared_error.py`, which allows you to calculate the error for a batch of examples.\n",
    "\n",
    "Error layers are different from normal layers for two reasons:\n",
    "\n",
    "1. They not only take the output of the previous layer as input but also the expected value from the previous layer (`y` and `y_true`).\n",
    "2. For a batch of $n$ examples, their output is a vector of size $n$. In other words, they indicate the error value for each example with a scalar (real number).\n",
    "\n",
    "The error layer should also be able to perform the `backward` operation to propagate the error gradient backward through the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "The `forward` method of the `SquaredError` layer should simply calculate the squared Euclidean distance between `y`, the values produced by the network, and `y_true`, the expected values.\n",
    "\n",
    "For example, if $y=[2,-2]$ and $y_{true}=[3,3]$, then the output of the layer is:\n",
    "\n",
    "$$E(y,y_{true})=d_2(y,y_{true})=d_2([2,-2],[3,3])=(2-3)^2+(-2-3)^2 = 1^2+(-5)^2=26$$\n",
    "\n",
    "In general, given two vectors $a=[a_1,\\dots,a_n]$ and $b=[b_1,\\dots,b_n]$, the squared Euclidean distance $d_2$ is:\n",
    "\n",
    "$$\n",
    "d_2(a,b) = d_2([a_1,\\dots,a_n],[b_1,\\dots,b_n]) =(a_1-b_1)^2+\\dots+(a_n-b_n)^2\n",
    "$$\n",
    "\n",
    "In the case of a batch of examples, the calculation is independent for each example. It's important that the sum of squared differences is calculated per example (row) and not per feature (column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[2,-2],\n",
    "             [-4,4]])\n",
    "y_true = np.array([[3,3],\n",
    "             [-5,2]])\n",
    "\n",
    "layer=nn.SquaredError()\n",
    "E=np.array([[26],[5]])\n",
    "utils.check_same(E,layer.forward(y,y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "Now that you can calculate the error of a network, great! This is the final layer of the network when it's being trained. Therefore, the backward method of an error layer does not receive $\\frac{δE}{δy}$; in fact, it should calculate it directly from $y$, $y_{true}$, and the error definition. Also, there are no parameters involved.\n",
    "\n",
    "So, in this case, the derivative is simple. We just need to calculate $\\frac{δE}{δy}$, the derivative of the error with respect to the output computed by the network, $y$.\n",
    "In this case, $E$ is symmetric with respect to its inputs, so let's call it $a$ and $b$ again, and then calculate the derivative with respect to element $i$ of $a$ (the derivative with respect to $b$ would be the same):\n",
    "\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa_i} = \\frac{δ((a_1-b_1)^2+\\dots+(a_n-b_n)^2)}{δa_i} \\\\\n",
    "= \\frac{δ((a_i-b_i)^2)}{δa_i} = 2 (a_i-b_i) \\frac{δ((a_i-b_i))}{δa_i} \\\\\n",
    "= 2 (a_i-b_i) 1 = 2 (a_i-b_i)\n",
    "$$\n",
    "Generalizing for the entire vector $a$, we get:\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa} = 2 (a-b)\n",
    "$$\n",
    "Where $a-b$ is a vector subtraction.\n",
    "\n",
    "Again, as this error is calculated for each example, the calculations are independent for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size = 2\n",
    "features_in = 3\n",
    "features_out = 5\n",
    "input_shape = (batch_size, features_in)\n",
    "\n",
    "layer = nn.SquaredError()\n",
    "utils.check_gradient.squared_error(layer, input_shape, samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
