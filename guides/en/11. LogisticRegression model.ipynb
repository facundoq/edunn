{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: `forward`\n",
    "\n",
    "A Logistic Regression model is formed by applying the `Softmax` function to a Linear Regression model. This function converts the output vector of Linear Regression into a vector representing a probability distribution.\n",
    "\n",
    "The function for Logistic Regression is $f(x) = softmax(wx + b)$. However, as we did with the `LinearRegression` model, we can view this model as the application of:\n",
    "* A `Linear` layer $f(x) = wx$,\n",
    "* A `Bias` layer $f(x) = x + b$,\n",
    "* A `Softmax` layer $f(x) = softmax(x)$.\n",
    "\n",
    "In other words, we have the following sequence of transformations: `x → Linear → Bias → Softmax → y`.\n",
    "\n",
    "Implement the `forward` method of the `LogisticRegression` model in the `edunn/models/logistic_regression.py` file. For this, we have already defined and initialized internal class models `Linear`, `Bias`, and `Softmax`; you just need to call their respective `forward` methods in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,0],\n",
    "             [0,1],\n",
    "             [1,1]])\n",
    "\n",
    "w = np.array([[100,0,0],\n",
    "              [0,100,0],\n",
    "              ])\n",
    "b = np.array([0,0,0])\n",
    "linear_initializer = nn.initializers.Constant(w)\n",
    "bias_initializer = nn.initializers.Constant(b)\n",
    "layer=nn.LogisticRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "y = np.array([[1, 0,0],\n",
    "              [0, 1,0],\n",
    "              [0.5,0.5,0]\n",
    "             ])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "y = np.array([[0, 0.5,0.5],\n",
    "              [0.5, 0,0.5],\n",
    "              [0,0,1]\n",
    "             ])\n",
    "linear_initializer = nn.initializers.Constant(-w)\n",
    "bias_initializer = nn.initializers.Constant(-b)\n",
    "layer=nn.LogisticRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: `backward`\n",
    "\n",
    "The `backward` method of a `LogisticRegression` model is the *inverse* composition of the `backward` methods of the `Linear`, `Bias`, and `Softmax` layers. Remember that these are applied in the reverse order compared to the `forward` method.\n",
    "\n",
    "In this case, we also help you by combining the gradient dictionaries of each layer into a single large gradient dictionary for `LogisticRegression` using the `**` python operator to unpack and repack dictionaries with `{**dict1, **dict2}`.\n",
    "\n",
    "Implement the `backward` method of the `LogisticRegression` model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size = 2\n",
    "din = 3  # input dimension\n",
    "dout = 5  # output dimension\n",
    "input_shape = (batch_size, din)\n",
    "\n",
    "# Check the derivatives of a Logistic Regression model with random values of `w`, `b`, and `x`, the input\n",
    "layer = nn.LogisticRegression(din, dout)\n",
    "\n",
    "utils.check_gradient.common_layer(layer, input_shape, samples=samples, tolerance=1e-5)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Logistic Regression\n",
    "\n",
    "Now that we have all necessary ingredients, we can define and train our first Logistic Regression model to classify the flowers in the [Iris dataset](https://www.kaggle.com/uciml/iris).\n",
    "\n",
    "In this case, we will train the model with the mean squared error function. However, while this form of error will work for this problem, it makes the optimization problem non-convex, and therefore, there is no unique global minimum. \n",
    "\n",
    "Later, we will implement the Cross-Entropy error function, designed specifically to deal with outputs that represent probability distributions. For now, let's just use `SquaredError`, knowing it is suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "\n",
    "# Load data, where `y` has \"onehot\" encoding\n",
    "# `y` has as many columns as classes\n",
    "# if example i is of class 2, for example, then\n",
    "# y[i,2]=1 and the rest of the values of y[i,:] are 0\n",
    "x, y, classes = nn.datasets.load_classification(\"iris\", onehot=True)\n",
    "# Data normalization\n",
    "x = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "n, din = x.shape\n",
    "n, dout = y.shape\n",
    "\n",
    "print(\"Dataset sizes:\", x.shape, y.shape)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = nn.LogisticRegression(din, dout)\n",
    "# Mean Squared Error\n",
    "error = nn.MeanError(nn.SquaredError())\n",
    "optimizer = nn.GradientDescent(lr=0.1, epochs=1000, batch_size=32)\n",
    "\n",
    "# Optimization algorithm\n",
    "history = optimizer.optimize(model, x, y, error)\n",
    "nn.plot.plot_history(history, error_name=error.name)\n",
    "\n",
    "\n",
    "print(\"Model Error:\")\n",
    "y_pred = model.forward(x)\n",
    "nn.metrics.classification_summary_onehot(y, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbformat": 4,
  "nbformat_minor": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
