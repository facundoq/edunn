{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense, Fully Connected, or Linear Regression Layers\n",
    "\n",
    "The most common layer in a neural network is a layer that implements the function `y = x * w + b`, where `x` is the input, `y` is the output, and `b` is a bias vector, and `w` is a weight matrix. However, implementing this layer can be challenging. Instead, we will separate the implementation into two parts.\n",
    "\n",
    "* The `Bias` layer, which only adds `b` to its input, i.e., `y = x + b`\n",
    "* The `Linear` layer, which only multiplies its input by the weight matrix `w`, i.e., `y = w * x`\n",
    "* By combining these two layers, we can achieve the functionality of the traditional layer called `Dense` or `FullyConnected` in other libraries. This will allows us to use  a linear regression model with the function `y(x) = w * x + b` to solve many problems!\n",
    "\n",
    "We will begin with the `Bias` layer, the simpler of the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Layer\n",
    "\n",
    "In this exercise, you need to implement the `Bias` layer, which adds a different value to each of its inputs to generate its output. This value is _not_ constant but rather a parameter of the network.\n",
    "\n",
    "For example, if the input `x` is `[3.5, -7.2]` and the `Bias` layer has parameters `[2.0, 3.0]`, then the output `y` will be `[3.5, -7.2] + [2.0, 3.0] = [5.5, -4.2]`.\n",
    "\n",
    "Your goal is to implement the `forward` and `backward` methods for this layer so that it can be used in a neural network.\n",
    "\n",
    "This layer works for arrays that have the same size as the `Bias` layer's parameters (excluding the batch dimension).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation and Initialization\n",
    "\n",
    "The `Bias` layer has a parameter vector `b`, which must be created and initialized in some way. Additionally, this parameter is registered in the layer so that it can be accessed later. \n",
    "\n",
    "\n",
    "Take a look at the file `edunn/models/bias.py` for the implementation of the `__init__` method of the `Bias` layer to see how the weight is created. You'll see that it will employ an `Initializer` to determine its initial value.\n",
    "\n",
    "When creating the layer, we can pass an object of type `Initializer` that will create and assign the initial value to the parameter `b`. By default, `b` is initialized with zeros using the `initializers.Zero`. \n",
    "\n",
    "\n",
    "Examining the implementation of the `Zero` class in `edunn/initializers.py`, we can see that:\n",
    "* It inherits from `Initializer`\n",
    "* It implements the `initialize(self, p: np.ndarray)` method, which receives a numpy array for initialization\n",
    "* It uses `p[:]` to initialize to 0 instead of `p = 0`. There are two important reasons for this:\n",
    "    * Using `p = 0` would only change the _local variable_ `p` instead of changing the _numpy array_ that `p` points to.\n",
    "    * By using `p[:]`, we are changing the __content__ of the parameter array, which belongs to a class like `Bias` or another that we will implement later.\n",
    "\n",
    "Once the class is created, we can obtain the parameter vector `p` from the `Bias` class using the `get_parameters()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bias layer with 2 input/output values\n",
    "bias = nn.Bias(2, initializer=nn.initializers.Zero())\n",
    "print(f\"Layer Name: {bias.name}\")\n",
    "print(f\"Layer Parameters: {bias.get_parameters()}\")\n",
    "print()\n",
    "\n",
    "# By default, the initializer is 'Zero'\n",
    "bias2 = nn.Bias(2)\n",
    "print(f\"Layer Name: {bias2.name}\")\n",
    "print(f\"Layer Parameters: {bias.get_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Parameters by Name\n",
    "\n",
    "The `get_parameters()` method returns a dictionary of parameters, because a layer can have more than one parameter. Given that we already know the name of the only parameter in this layer, we can access it by its name in string form, `'b'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bias layer with 2 input/output values\n",
    "bias = nn.Bias(2, initializer=nn.initializers.Zero())\n",
    "print(f\"Layer Name: {bias.name}\")\n",
    "print(f\"Layer Parameter 'b': {bias.get_parameters()['b']}\")\n",
    "print()\n",
    "\n",
    "# By default, the initializer is 'Zero'\n",
    "bias2 = nn.Bias(2)\n",
    "print(f\"Layer Name: {bias2.name}\")\n",
    "print(f\"Layer Parameter 'b': {bias2.get_parameters()['b']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a Constant Initializer\n",
    "\n",
    "While sometimes parameters are initialized to `0`, it's most common to initialize them with some constant value.\n",
    "\n",
    "Before starting with the implementation of the `Bias` class, you should implement the `Constant` initializer that assigns a constant value or array to the parameter. This allows, for example, initializing `b` with all values of `3` or with a vector of values `[1, 2, 3, 4]`.\n",
    "\n",
    "Find the `Constant` class in the `edunn/initializers.py` module and implement the `initialize` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bias layer with 2 output values (and input values as well)\n",
    "# All parameters are initialized to 3\n",
    "value = 3\n",
    "bias = nn.Bias(2, initializer=nn.initializers.Constant(value))\n",
    "\n",
    "print(f\"Layer Name: {bias.name}\")\n",
    "print(f\"Layer Parameter 'b': {bias.get_parameters()['b']}\")\n",
    "utils.check_same(bias.get_parameters()['b'], np.array([3, 3]))\n",
    "print()\n",
    "\n",
    "# Create a Bias layer with initial values 1, 2, 3, 4. \n",
    "# Note that we are ensuring that the number of values of the Constant initializer match those of the bias array\n",
    "\n",
    "value = np.array([1, 2, 3, 4])\n",
    "bias = nn.Bias(4, initializer=nn.initializers.Constant(value))\n",
    "\n",
    "print(f\"Layer Name: {bias.name}\")\n",
    "print(f\"Layer Parameter 'b': {bias.get_parameters()['b']}\")\n",
    "\n",
    "utils.check_same(bias.get_parameters()['b'], value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "Now that we know how to create and initialize `Bias` layer objects, let's begin with the `forward` method, which can be found in the file `edunn/models/bias.py`.\n",
    "\n",
    "If the parameters to be added are $[b_1, b_2, ..., b_f]$ and the input to the layer is $x = [x_1, x_2, ..., x_f]$, then the output $y$ is:\n",
    "\n",
    "$\n",
    "y([x_1, x_2, ..., x_f]) = [x_1 + b_1, x_2 + b_2, ..., x_f + b_f]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5, -7.2, 5.3],\n",
    "              [-3.5, 7.2, -5.3]])\n",
    "\n",
    "initializer = nn.initializers.Constant(np.array([2, 3, 4]))\n",
    "\n",
    "layer = nn.Bias(3, initializer=initializer)\n",
    "y = np.array([[5.5, -4.2, 9.3],\n",
    "              [-1.5, 10.2, -1.3]])\n",
    "utils.check_same(y, layer.forward(x))\n",
    "\n",
    "initializer = nn.initializers.Constant(-np.array([2, 3, 4]))\n",
    "layer = nn.Bias(3, initializer=initializer)\n",
    "y = np.array([[1.5, -10.2, 1.3],\n",
    "              [-5.5, 4.2, -9.3]]\n",
    "             )\n",
    "utils.check_same(y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "In addition to calculating its output, the \n",
    "layer must also be able to backpropagate the gradient of the network's error. Therefore, you need to implement the `backward` method, which receives $\\frac{δE}{δy}$, the partial derivatives of the error with respect to the output (gradient) of this layer, and returns $\\frac{δE}{δx}$, the partial derivatives of the error with respect to the inputs of this layer.\n",
    "\n",
    "## δE/δx\n",
    "For the `Bias` layer, the gradient calculation with respect to the input `dE_dx` is simple since it is the same as the case with the `AddConstant` layer.\n",
    "\n",
    "$ \\frac{δE}{δx} =\\frac{δE}{δy} $\n",
    "\n",
    "\n",
    "Given that\n",
    "$\n",
    "y([x_1, x_2, ..., x_f]) = [x_1 + b_1, x_2 + b_2, ..., x_f + b_f]\n",
    "$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{δE}{δx_i} = \\frac{δE}{δy_i} \\frac{δy_i}{δx_i} = \\frac{δE}{δy_i} \\frac{δ(x_i+b_i)}{δb_i} = \\frac{δE}{δy_i} \\cdot 1 = \\frac{δE}{δy_i}\n",
    "$\n",
    "\n",
    "What leads to\n",
    "\n",
    "$\n",
    "\\frac{δE}{δx} = \\frac{δE}{δy}\n",
    "$\n",
    "\n",
    "\n",
    "## δE/δb\n",
    "\n",
    "For this layer, you also need to calculate the gradient with respect to the parameters `b` so that they can be optimized to minimize the error. Therefore, you also need to calculate `dE_db`. Remember that:\n",
    "\n",
    "$\n",
    "y([x_1, x_2, ..., x_f]) = [x_1 + b_1, x_2 + b_2, ..., x_f + b_f]\n",
    "$\n",
    "\n",
    "Then, applying the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{δE}{δb_i} = \\frac{δE}{δy_i} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δ (x_i + b_i)}{δb_i} = \\frac{δE}{δy_i} 1 = \\frac{δE}{δy_i}  \n",
    "$\n",
    "\n",
    "What leads to\n",
    "\n",
    "$\n",
    "\\frac{δE}{δx} =\\frac{δE}{δy}\n",
    "$\n",
    "\n",
    "\n",
    "However, in the case of the gradient of the error with respect to `b`, the formula is the same, $ \\frac{δE}{δb} =\\frac{δE}{δy} $. This is because $ \\frac{δy_i}{δb_i} = \\frac{δ(x_i + b_i)}{δb_i} = \\frac{δ(x_i + b_i)}{δx_i} = 1 $. In other words, if we view both `b` and `x` as inputs to the layer, $ x + b $ is symmetric in `x` and `b`, and thus, their derivatives are also symmetric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random values and batch size\n",
    "# to generate values of x and δEδy for gradient checking\n",
    "samples = 100\n",
    "batch_size = 2\n",
    "\n",
    "# Dimensions of input and output for the layer, and initializer\n",
    "features = 4\n",
    "input_shape = (batch_size, features)\n",
    "initializer = nn.initializers.Constant(np.array(range(features)))\n",
    "\n",
    "# Verify gradients of a Bias layer with b=[0,1,2,3]\n",
    "layer = nn.Bias(features)\n",
    "utils.check_gradient.common_layer(layer, input_shape, samples=samples)\n",
    "\n",
    "initializer = nn.initializers.Constant(-np.array(range(features)))\n",
    "# Verify gradients of a Bias layer with b=[0,-1,-2,-3]\n",
    "layer = nn.Bias(features)\n",
    "utils.check_gradient.common_layer(layer, input_shape, samples=samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
