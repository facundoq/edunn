{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AddConstant Layer\n",
    "\n",
    "To start easy, in this exercise you'll need to implement the `AddConstant` layer, which adds a constant value to each of its inputs to generate its output.\n",
    "\n",
    "For example, if the input `x` is `[3.5, -7.2, 5.3]` and the AddConstant layer adds the value `3.0` to its input, then the output `y` will be `[6.5, -4.2, 8.3]`.\n",
    "\n",
    "Your goal is to implement the `forward` and `backward` methods for this layer so that it can be used in a neural network.\n",
    "\n",
    "This layer works for input arrays of any size, including vectors, matrices, or arrays with more dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "The `forward` method calculates the output `y` based on the input `x`, as explained above. In formal terms, if the constant to be added is $C$ and the input to the layer is $x = [x_1, x_2, ..., x_n]$, then the output $y$ is:\n",
    "\n",
    "$\n",
    "y([x_1, x_2, ..., x_n]) = [x_1 + C, x_2 + C, ..., x_n + C]\n",
    "$\n",
    "\n",
    "We start with the `forward` method of the `AddConstant` class. You need to complete the code between the comments:\n",
    "\n",
    "```\n",
    "\"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "```\n",
    "and\n",
    "\n",
    "```\n",
    "\"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "```\n",
    "\n",
    "Between these lines there's some default code (in this case `y = np.zeros_like(x)`) just to avoid errors from the Python interpreter until you write your implementation. Feel free to remove this line to provide a clean implementation!\n",
    "\n",
    "Don't forget to execute the cell after adding your code!\n",
    "\n",
    "Then, verify with the following cell for a layer that adds 3 and another that adds -3. If both checks are correct, you will see two messages with <span style='background-color:green;color:white;'>success</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn import Model\n",
    "\n",
    "class AddConstant(Model):\n",
    "    \"\"\"\n",
    "    A layer that adds a constant. This layer has NO parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: float, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param x: input vector/matrix\n",
    "        :return: `x + a`, constant value, stored in `self.value`\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "        y = np.zeros_like(x)\n",
    "        \"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self, dE_dy: np.ndarray):\n",
    "        \"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "        dE_dx = np.zeros_like(dE_dy)\n",
    "        \"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "\n",
    "        dE_dp = {}  # no parameters, no derivatives\n",
    "        return dE_dx, dE_dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5, -7.2, 5.3],\n",
    "             [-3.5, 7.2, -5.3]])\n",
    "\n",
    "layer = AddConstant(3)\n",
    "y = np.array([[6.5, -4.2, 8.3],\n",
    "              [-0.5, 10.2, -2.3]])\n",
    "utils.check_same(y, layer.forward(x))\n",
    "\n",
    "layer = AddConstant(-3)\n",
    "y = np.array([[0.5, -10.2, 2.3],\n",
    "              [-6.5, 4.2, -8.3]])\n",
    "utils.check_same(y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "In addition to calculating its output with `forward`, the layer must also be able to propagate the gradient of the network's error backward. To do this, you need to implement the `backward` method, which receives $\\frac{δE}{δy}$, the partial derivatives (gradient) of the error with respect to the output of this layer, and returns $\\frac{δE}{δx}$, the partial derivatives of the error with respect to the inputs of this layer.\n",
    "\n",
    "For the `AddConstant` layer, calculating the gradient of the output $y$ with respect to the input $x$ is straightforward. However, we need to return the gradient of $E$ with respect to $x$. Let's recall the form of the output:\n",
    "\n",
    "$\n",
    "y([x_1, x_2, ..., x_n]) = [x_1 + C, x_2 + C, ..., x_n + C]\n",
    "$\n",
    "\n",
    "Our goal is to calculate $\\frac{δE}{δx_i}$. For that, we'll focus on $\\frac{δE}{δx_i}$, which is the derivative of the error with respect to a specific input. Then, applying the chain rule, we can write $\\frac{δE}{δx_i}$ as:\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\sum_j \\frac{δE}{δy_j} * \\frac{δy_j}{δx_i}$\n",
    "\n",
    "We can consider each element of the output separately:\n",
    "\n",
    "$y_i(x) = x_i + C$\n",
    "\n",
    "And therefore:\n",
    "\n",
    "$\\frac{δy_i}{δx_i} = 1 + 0 = 1$\n",
    "\n",
    "Since there is no interaction between elements with different indices (i.e., $y_i$ depends only on $x_i$), in the sum above, if $i \\neq j$, then $\\frac{δy_j}{δx_i} = 0$. Thus, we can remove the summation and use the chain rule only with $y_i$:\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\sum_j \\frac{δE}{δy_j} * \\frac{δy_j}{δx_i} = \\frac{δE}{δy_i} * \\frac{δy_i}{δx_i}$\n",
    "\n",
    "Knowing that $\\frac{δy_i}{δx_i} = 1$\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\frac{δE}{δy_i} * 1 = \\frac{δE}{δy_i}$\n",
    "\n",
    "Writing it in vector form for the vector x:\n",
    "\n",
    "$\\frac{δE}{δx} = [\\frac{δE}{δy_1}, \\frac{δE}{δy_2}, ..., \\frac{δE}{δy_n}] = \\frac{δE}{δy}$\n",
    "\n",
    "What does this equation mean? Simply that the layer just propagates the gradients from the next layer to the previous one.\n",
    "\n",
    "Note that for simplicity, in the code we call these vectors `δEδy` and `δEδx`. Also, remember that in this case $C$ is a constant and NOT a network parameter, so we do not need to calculate $\\frac{δE}{δC}$.\n",
    "\n",
    "How do we know if the implementation is correct? Well, gradient checking is done with the `check_gradient_layer_random_sample` function. This function generates random samples of `x` and `δEδy`, and then compares the analytical gradient (your implementation) with the *numerical gradient*. The numerical gradient is costly to compute, but can be computed with the same algorithm for different functions.\n",
    "\n",
    "The numerical gradient _approximates_ the partial derivatives using the derivative formula:\n",
    "\n",
    "$\\frac{δf(x)}{δx} = \\lim_{h→0} \\frac{f(x+h) - f(x)}{h}$ \n",
    "\n",
    "with a very small value of $h$ ($h=10^{-12}$). In truth, for a better approximation, it uses the _centered derivative_, whose formula is:\n",
    "\n",
    " $\\frac{δf(x)}{δx} \\~= \\frac{f(x+h) - f(x-h)}{2h}$. \n",
    " \n",
    "This gradient checking technique is a standard method to verify the correct implementation of a neural network.\n",
    "\n",
    "Complete the code in the `backward` function of the `AddConstant` layer between the comments:\n",
    "\n",
    "\n",
    "```\n",
    "\"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "```\n",
    "and\n",
    "\n",
    "```\n",
    "\"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "```\n",
    "\n",
    "Don't forget to execute the cell after adding your code!\n",
    "\n",
    "Then, verify with the following cell for a layer that adds 3 and another that adds -3. If both checks are correct, you will see two messages with <span style='background-color:green;color:white;'>success</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "# Number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape = (5, 2)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds 3\n",
    "layer = AddConstant(3)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds -4\n",
    "layer = AddConstant(-4)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Name\n",
    "\n",
    "Layer names are automatically assigned when an object of the same class is created, and ideally, they should be unique to distinguish different layers even if they are of the same type.\n",
    "\n",
    "By default, when you execute `AddConstant(3)`, an object of this layer is created, and it is given the name `AddConstant_i`, where `i` increments automatically as we create objects of the same `AddConstant` class.\n",
    "\n",
    "You can also specify the layer name manually to keep it fixed using the `name` parameter. For example, with `AddConstant(3, name=\"A layer that adds 3\")`\n",
    "\n",
    "All layers must follow the convention of having a `name` parameter for the library to identify them. Also, in a given session, model names should be unique to simplify the implementation of some parts of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = AddConstant(3)\n",
    "print(c1.name)\n",
    "\n",
    "c2 = AddConstant(3)\n",
    "print(c2.name)\n",
    "\n",
    "c3 = AddConstant(3, name=\"My first layer :)\")\n",
    "print(c3.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
