{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Sequential` Model for Neural Networks\n",
    "\n",
    "\n",
    "We have already implemented layers/models of all kinds: dense, activation functions, error layers, etc. Additionally, we have initializers and an optimizer based on stochastic gradient descent, as well as models that combine other layers like `LinearRegression` and `LogisticRegression`. \n",
    "\n",
    "To take the next step and define simple neural networks, we will implement the `Sequential` model. This model generalizes the ideas applied in `LinearRegression`, `LogisticRegression`, and `Dense`, allowing us to create a layer based on other layers. In previous cases, the layers to be used were predefined. `Sequential` will allow us to use any combination of layers we want.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a `Sequential` Model\n",
    "\n",
    "\n",
    "A `Sequential` model should be created with a list of other models/layers. This way, we specify what transformations and in what order will be performed to obtain the network's output.\n",
    "\n",
    "We can see several examples where we create a linear regression or logistic regression model or a Dense layer based on the `Sequential` model.\n",
    "\n",
    "`Sequential` also has a very useful method, `summary()`, which allows us to obtain a description of the layers and their parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din=5\n",
    "dout=3\n",
    "\n",
    "# Create a linear regression model\n",
    "layers = [nn.Linear(din,dout), nn.Bias(dout)]\n",
    "linear_regression = nn.Sequential(layers, name=\"Linear Regression\")\n",
    "print(linear_regression.summary())\n",
    "\n",
    "\n",
    "# Create a linear regression model without the auxiliary variable `layers`\n",
    "linear_regression = nn.Sequential([nn.Linear(din,dout),\n",
    "                                   nn.Bias(dout),\n",
    "                                  ], name=\"Linear Regression\")\n",
    "print(linear_regression.summary())\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_regression = nn.Sequential([nn.Linear(din,dout),\n",
    "                                   nn.Bias(dout),\n",
    "                                   nn.Softmax(dout)\n",
    "                                  ], name=\"Logistic Regression\")\n",
    "print(logistic_regression.summary())\n",
    "\n",
    "\n",
    "# Create a Dense layer with ReLU activation\n",
    "dense_relu = nn.Sequential([nn.Linear(din,dout),\n",
    "                           nn.Bias(dout),\n",
    "                           nn.ReLU(dout)\n",
    "                          ], name=\"Dense Layer with ReLU Activation\")\n",
    "print(dense_relu.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Networks with `Sequential`\n",
    "\n",
    "We will also create our first multi-layer neural networks by adding more layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a network with two Dense layers, both with internal dimensionality of 3\n",
    "network_layer2 = nn.Sequential([nn.Dense(din, 3, \"relu\"),\n",
    "                               nn.Dense(3, dout, \"id\")\n",
    "                      ], name=\"Two-Layer Network\")\n",
    "print(network_layer2.summary())\n",
    "\n",
    "\n",
    "\n",
    "# Create a network with 4 Dense layers\n",
    "# Internal dimensions are 2, 4, and 3\n",
    "# The final layer uses softmax activation\n",
    "network_layer4 = nn.Sequential([nn.Dense(din, 2, \"relu\"),\n",
    "                               nn.Dense(2, 4, \"tanh\"),\n",
    "                               nn.Dense(4, 3, \"sigmoid\"),\n",
    "                               nn.Dense(3, dout, \"softmax\"),\n",
    "                      ], name=\"Four-Layer Network\")\n",
    "print(network_layer4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Sequential` Model Parameters\n",
    "\n",
    "The `Sequential` model also allows you to easily retrieve the parameters of all its internal models. For this purpose, we have already implemented the `get_parameters` method, which allows you to obtain _all_ the parameters of the internal models, but renamed so that if, for example, two models have the same parameter names, those names will not be repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameter names of network_layer2\")\n",
    "print(network_layer2.get_parameters().keys())\n",
    "\n",
    "print(\"Parameter names of network_layer4\")\n",
    "print(network_layer4.get_parameters().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Sequential` `forward` Method\n",
    "\n",
    "\n",
    "Now, let's implement the `forward` method for `Sequential`. Given an input `x` and a sequence of models `M_1, M_2, ..., M_n` within `Sequential`, we must calculate the output `y` as:\n",
    "\n",
    "$$ y = M_n(...(M_2(M_1(x))...)$$\n",
    "\n",
    "In code terms, we need to iterate through the possible models (starting with the first one) and apply the `forward` method.\n",
    "\n",
    "```python\n",
    "for m in models:\n",
    "    x = m.forward(x)\n",
    "return x\n",
    "```\n",
    "\n",
    "Implement the `forward` method for the `Sequential` class in `edunn/models/sequential.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "b = np.array([1,2,3])\n",
    "linear_initializer = nn.initializers.Constant(w)\n",
    "bias_initializer = nn.initializers.Constant(b)\n",
    "layer = nn.Sequential([nn.Linear(2, 3, initializer=linear_initializer),\n",
    "                     nn.Bias(3, initializer=bias_initializer)\n",
    "                    ])\n",
    "y = np.array([[-21, -24, -27],\n",
    "              [23, 28, 33]])\n",
    "\n",
    "nn.utils.check_same(y, layer.forward(x))\n",
    "\n",
    "linear_initializer = nn.initializers.Constant(-w)\n",
    "bias_initializer = nn.initializers.Constant(-b)\n",
    "layer = nn.Sequential([nn.Linear(2, 3, initializer=linear_initializer),\n",
    "                     nn.Bias(3, initializer=bias_initializer)\n",
    "                    ])\n",
    "nn.utils.check_same(-y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Sequential` `backward` Method\n",
    "\n",
    "\n",
    "Similar to the `Dense` layer, to implement the `backward` method, you should also call the `backward` method of each of the models in the reverse order compared to the forward pass. Given a tensor `δEδy` that contains the derivatives of the error with respect to each value of the output `y`, we need to calculate:\n",
    "* `δEδx`, the derivative of the error with respect to the input `x`\n",
    "* `δEδp_i`, the derivative of the error with respect to each parameter `p_i`\n",
    "\n",
    "To achieve this, we need to iterate through the possible models (starting with the last one) and apply the `backward` method, propagating the error backward and collecting the most important information, which is the derivatives of the error with respect to the parameters. In code terms,\n",
    "\n",
    "```python\n",
    "δEδp = {}\n",
    "for m_i in reverse(models):\n",
    "    δEδy, δEδp_i = m_i.backward(δEδy)\n",
    "    add gradients of δEδp_i to δEδp\n",
    "return δEδy, δEδp\n",
    "```\n",
    "\n",
    "In this case, we also provide the `merge_gradients` function, which you can call as `self.merge_gradients(layer, δEδp, gradients)`. This function allows you to add the parameters' `δEδp` of the layer `layer` to the final gradients dictionary `gradients` that should be returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size = 2\n",
    "features_in = 3\n",
    "features_out = 5\n",
    "input_shape = (batch_size, features_in)\n",
    "\n",
    "# Test derivatives of a Sequential model with random values for `w`\n",
    "layer = nn.Sequential([nn.Linear(features_in, features_out),\n",
    "                     nn.Bias(features_out),\n",
    "                     nn.ReLU()\n",
    "                    ])\n",
    "nn.utils.check_gradient.common_layer(layer, input_shape, samples=samples)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have implemented all the basic functions of a neural network library!\n",
    "\n",
    "Now, let's define some neural networks to improve performance compared to linear models (Linear Regression and Logistic Regression).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
