{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import edunn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Layer\n",
    "\n",
    "In this exercise, you need to implement the `CrossEntropyWithLabels` error layer, which allows you to calculate the error of a model that outputs probabilities in terms of distances between distributions.\n",
    "\n",
    "In this case, `WithLabels` indicates that the true probability distribution (obtained from the dataset) is actually encoded with labels. For example, for a problem with `C=3` classes, if an example is of class 2 (counting from 0), then its label is `2`. This is a convenient way to specify that its encoding as a probability distribution would be `[0,0,1]`, which is a vector of `3` elements, where element `2` (again, counting from 0) has a probability of 1, and the rest are 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Method\n",
    "\n",
    "The `forward` method of the `CrossEntropyWithLabels` layer assumes that its input `y` is a probability distribution, i.e., `C` positive values that sum to 1, where `C` is the number of classes. Similarly, `y_true` is a label indicating which class out of the `C` classes is correct.\n",
    "\n",
    "For example, if $y=(0.3,0.4,0.3)$ and $y_{true}=2$, then there will be a considerable error since the value $y_{true}=2$ indicates that the distribution $y=(0,0,1)$ is expected. So, the values $0.3$ and $0.4$ for classes 0 and 1 should decrease, and the value $0.3$ for class 2 should increase.\n",
    "\n",
    "Cross entropy quantifies this error by calculating the negative logarithm of the probability of the correct class ($-ln(y_{y_{true}})$), in this case, class 2 ($-ln(y_2)$). So,\n",
    "\n",
    "$$CrossEntropy(y,y_{true}) = CrossEntropy((0.3,0.4,0.3),2) = -ln(0.3) = 1.20$$\n",
    "\n",
    "Again, in this case, the value $0.3$ was chosen because it is at index 2 of the vector $y$, meaning another way to write the above would be:\n",
    "\n",
    "$$E(y,y_{true}) = -ln(y_{y_{true}}) = -ln(0.3) = 1.20$$\n",
    "\n",
    "The reason for using the function $-ln(0.3)$ to penalize is that if the probability for the correct class is 1, then\n",
    "\n",
    "$$-ln(y_{y_{true}}) = -ln(1) = -0 = 0$$\n",
    "\n",
    "and there is no penalty. Otherwise, the output of $-ln$ will be positive and indicate an error. This way, it penalizes that the probability of the correct class does not reach 1. This can be visualized easily in a graph of the function $-ln(x)$:\n",
    "\n",
    "<img src=\"img/cross_entropy.png\" width=\"400\">\n",
    "\n",
    "Finally, since the values of $y$ are normalized, it is not necessary to penalize that the rest of the probabilities are greater than 0; if the error leads to the probability of the correct class being 1, then the rest must be 0. For this reason (among others), cross entropy is a good combination with the softmax function for training classification models.\n",
    "\n",
    "In the case of a batch of examples, the calculation is independent for each example.\n",
    "\n",
    "Implement the `forward` method of the `CrossEntropyWithLabels` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[1,0],\n",
    "             [0.5,0.5],\n",
    "              [0.5,0.5],\n",
    "             ])\n",
    "y_true = np.array([0,0,1])\n",
    "\n",
    "\n",
    "layer = nn.CrossEntropyWithLabels()\n",
    "E = -np.log(np.array([[1],[0.5],[0.5]]))\n",
    "\n",
    "nn.utils.check_same(E, layer.forward(y_true, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Method\n",
    "\n",
    "Since the derivation of the equations for the `backward` method of cross entropy is a bit long, we provide [this note](http://facundoq.github.io/guides/crossentropy_derivative) with the derivation of all cases.\n",
    "\n",
    "Again, since this error is for each example, the calculations are independent for each row.\n",
    "\n",
    "Implement the `backward` method of the `CrossEntropyWithLabels` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size = 2\n",
    "features_in = 3\n",
    "features_out = 5\n",
    "input_shape = (batch_size, features_in)\n",
    "\n",
    "\n",
    "layer = nn.CrossEntropyWithLabels()\n",
    "nn.utils.check_gradient.cross_entropy_labels(layer, input_shape, samples=samples, tolerance=1e-5)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Applied to Flower Classification\n",
    "\n",
    "Now that we have all the elements, we can define and train our first logistic regression model to classify flowers in the [Iris dataset](https://www.kaggle.com/uciml/iris).\n",
    "\n",
    "Now, we can do it with Cross Entropy; although in this case, the results in terms of accuracy are similar, the model has a convex error, making optimization easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data with labels as outputs\n",
    "# (note: class labels start at 0)\n",
    "x, y, classes = nn.datasets.load_classification(\"iris\")\n",
    "# Normalize the data\n",
    "x = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "n, din = x.shape\n",
    "# Calculate the number of classes\n",
    "classes = y.max() + 1\n",
    "print(\"Sizes of x and y:\", x.shape, y.shape)\n",
    "\n",
    "# Logistic Regression model, \n",
    "# with `din` input dimensions (4 for Iris)\n",
    "# and `classes` output dimensions (3 for Iris)\n",
    "model = nn.LogisticRegression(din, classes)\n",
    "# Mean Squared Error\n",
    "error = nn.MeanError(nn.CrossEntropyWithLabels())\n",
    "optimizer = nn.GradientDescent(lr=0.1, epochs=1000, batch_size=32)\n",
    "\n",
    "# Optimization algorithm\n",
    "history = optimizer.optimize(model, x, y, error)\n",
    "nn.plot.plot_history(history, error_name=error.name)\n",
    "\n",
    "\n",
    "print(\"Model Metrics:\")\n",
    "y_pred = model.forward(x)\n",
    "y_pred_labels = nn.utils.onehot2labels(y_pred)\n",
    "nn.metrics.classification_summary(y, y_pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbformat": 4,
  "nbformat_minor": 4
 }
}
