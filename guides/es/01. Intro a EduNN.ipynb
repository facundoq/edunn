{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Librer√≠a **edunn**\n",
    "\n",
    "[edunn](https://github.com/facundoq/edunn) es una librer√≠a para definir y entrenar redes neuronales basada en [Numpy](https://numpy.org/), dise√±ada para ser simple de _entender_. \n",
    "\n",
    "A√∫n m√°s importante, fue dise√±ada para que sea simple de _implementar_. Es decir, su uso **principal** es como objeto de aprendizaje para comprender como se implementan las redes neuronales modernas en frameworks como [Keras](https://keras.io/) o [Pytorch](https://pytorch.org/). \n",
    "\n",
    "No obstante, tambi√©n es simple para _utilizar_. Por ejemplo, para definir y entrenar una red neuronal para clasificaci√≥n con de tres capas con distintas funciones de activaci√≥n, podemos escribir un c√≥digo muy similar al de estos frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "\n",
    "dataset_name = \"iris\"\n",
    "x, y, classes = nn.datasets.load_classification(dataset_name)\n",
    "n, din = x.shape\n",
    "n_classes = y.max() + 1\n",
    "\n",
    "# Definici√≥n del modelo\n",
    "layers = [nn.Linear(din, 10),\n",
    "          nn.Bias(10),\n",
    "          nn.ReLU(),\n",
    "\n",
    "          nn.Linear(10, n_classes),\n",
    "          nn.Bias(n_classes),\n",
    "          nn.Softmax()\n",
    "          ]\n",
    "\n",
    "model = nn.Sequential(layers)\n",
    "print(\"Arquitectura de la Red:\")\n",
    "print(model.summary())\n",
    "\n",
    "error = nn.MeanError(nn.CrossEntropyWithLabels())\n",
    "# Algoritmo de optimizaci√≥n\n",
    "optimizer = nn.GradientDescent(lr=0.1, epochs=3000, batch_size=32)\n",
    "\n",
    "# Algoritmo de optimizaci√≥n\n",
    "print(\"Entrenando red con descenso de gradiente:\")\n",
    "history = optimizer.optimize(model, x, y, error, verbose=False)\n",
    "\n",
    "# Reporte del desempe√±o\n",
    "y_pred = model.forward(x)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "print(f\"Accuracy final del modelo en el conjunto de entrenamiento: {nn.metrics.accuracy(y, y_pred_labels) * 100:0.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conocimiento previo\n",
    "Para poder implementar la librer√≠a, asumimos que ya has adquirido los conceptos b√°sicos de redes neuronales: \n",
    "\n",
    "* Capas\n",
    "    * Capas Lineales\n",
    "    * Funciones de Activaci√≥n\n",
    "    * Composici√≥n de capas\n",
    "    * M√©todos forward y backward\n",
    "* Algoritmo de propagaci√≥n hacia atr√°s (backpropagation)\n",
    "* Descenso de gradiente\n",
    "    * C√°lculo de gradientes\n",
    "    * Optimizaci√≥n b√°sica por gradientes\n",
    "* C√≥mputo/entrenamiento por lotes (batches)\n",
    "\n",
    "Tambi√©n se asume conocimiento de Python y de Numpy, as√≠ como del manejo de bases de datos tabulares y de im√°genes.\n",
    "\n",
    "\n",
    "# Componentes de la librer√≠a\n",
    "\n",
    "Describimos los componentes b√°sicos de la librer√≠a utilizados en el c√≥digo anterior, para proveer el contexto de los ejercicios a realizar. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√≥dulo **datasets**\n",
    "\n",
    "\n",
    "El m√≥dulo `edunn.datasets` permite cargar algunos conjuntos de datos de prueba f√°cilmente. Estos conjuntos de datos se utilizar√°n para verificar y experimentar con los modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "\n",
    "dataset_name = \"study2d\"\n",
    "x, y, classes = nn.datasets.load_classification(dataset_name)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)\n",
    "n, din = x.shape\n",
    "n_classes = y.max() + 1\n",
    "\n",
    "print(f\"El conjunto de datos {dataset_name} tiene {n} ejemplos, {din} caracter√≠sticas por ejemplo y {n_classes} clases: {classes}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para ver qu√© otros conjuntos de datos para clasificaci√≥n o regresi√≥n tiene el m√≥dulo `datasets` de `edunn` (que accedemos como `nn.datasets`), se puede ejecutar `nn.datasets.get_classification_names()` y `nn.datasets.get_regression_names()` y obtener una lista de nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los conjuntos de datos de clasificaci√≥n disponibles son:\")\n",
    "print(nn.datasets.get_classification_names())\n",
    "print()\n",
    "\n",
    "print(\"Los conjuntos de datos de regresi√≥n disponibles son:\")\n",
    "print(nn.datasets.get_regression_names())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases y m√≥dulos de edunn\n",
    "\n",
    "Para usar `edunn`, importamos la librer√≠a y la llamamos `nn` de modo que sea m√°s f√°cil de tipear.\n",
    "```python\n",
    "import edunn as nn\n",
    "```\n",
    "\n",
    "La librer√≠a tiene una clase fundamental, `Model`. Esta es la superclase de los modelos/capas que implementaremos, y define dos m√©todos abstractos para que implementen sus subclases:\n",
    "\n",
    "* `forward(x)`: computa la salida `y` dada una entrada `x`. \n",
    "    * Como asunci√≥n para simplificar la librer√≠a, los modelos s√≥lo podr√°n tener una entrada y una salida, que deben ser un arreglo de numpy (excepto los de error). En la pr√°ctica, veremos que esta no es una limitaci√≥n importante.\n",
    "* `backward(dEdy)`: computa el gradiente del error respecto a la entrada (`dEdx`), utilizando el gradiente del error respecto a la salida (`dEdy`). Si el modelo/capa tiene par√°metros, tambi√©n calcula el gradiente respecto a estos par√°metros.\n",
    "    * `backward` permite hacer una implementaci√≥n desacoplada del algoritmo backpropagation.\n",
    "    * Utilizando el `backward` de un modelo, se puede optimizarlo mediante descenso de gradiente.\n",
    "\n",
    "\n",
    "La librer√≠a tiene varias clases de distintas capas/modelos:\n",
    "\n",
    "* Las clases `Linear`, `Bias`, que permiten crear capas con las funciones $wx$ y $x+b$ respectivamente. En estos casos, $w$ y $b$ son par√°metros a optimizar. Combinando estas capas se puede formar una capa densa tradicional que calcula $wx+b$.\n",
    "* Las clases `TanH`, `ReLU` y `Softmax`, que permiten crear capas con las funciones de activaci√≥n de esos nombres.\n",
    "* La clase `Sequential` para crear redes secuenciales, donde donde la salida de cada capa es la entrada de la capa siguiente, y hay solo una capa inicial y una final.\n",
    "\n",
    "Cada una de estas clases es una subclase de `Model`, y por ende permite hacer las 2 operaciones fundamentales, `forward` y `backward`. En adelante, usaremos la palabra _capa_ como sin√≥nimo de modelo, es decir, de una subclase de `Model`. Esta terminolog√≠a, si bien es un poco inexacta, es est√°ndar en el campo de las redes neuronales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(din,10),\n",
    "          nn.Bias(10),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(10,20),\n",
    "          nn.Bias(20),\n",
    "          nn.TanH(),\n",
    "          nn.Linear(20,n_classes),\n",
    "          nn.Bias(n_classes),\n",
    "          nn.Softmax()\n",
    "          ]\n",
    "model = nn.Sequential(layers)\n",
    "print(\"Resumen del modelo:\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capas/Modelos de error\n",
    "\n",
    "Los modelos requieren medir su error. Para eso `edunn` tambi√©n tiene algunas capas/modelos de error, que reciben en su `forward` dos entradas, la calculada por la red y la esperada. Tenemos dos tipos de capas:\n",
    "\n",
    "* Aquellas que permiten calcular el error de la red _para cada ejemplo por separado_, como `CrossEntropyWithLabels`, o `SquaredError`\n",
    "* Aquellas que permiten combinar los errores de cada ejemplo para generar un error que sea escalar.\n",
    "    * La capa `MeanError`  permite calcular el error promedio de otra capa de error, como la capa `CrossEntropyWithLabels` y `SquaredError` que mencionamos.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cross_entropy_error = nn.MeanError(nn.CrossEntropyWithLabels())\n",
    "mean_squared_error = nn.MeanError(nn.SquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizadores \n",
    "\n",
    "Para entrenar un modelo, podemos utilizar un objeto `Optimizer`, cuyo m√©todo `optimize` permite, dados arreglos `x` e `y` y una funci√≥n de error, entrenar un modelo para minimizar ese error en este conjunto de datos. \n",
    "Para este entrenamiento, debe especificarse un algoritmo de optimizaci√≥n. En este caso utilizamos descenso de gradiente simple con la clase `GradientDescent`, una tasa de aprendizaje de `0.1`, `100` √©pocas y un tama√±o de lote de 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de optimizaci√≥n\n",
    "optimizer = nn.GradientDescent(lr=0.001, epochs=100, batch_size=8)\n",
    "\n",
    "# Optimizaci√≥n\n",
    "history = optimizer.optimize(model, x, y, mean_cross_entropy_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Por √∫ltimo, podemos utilizar y evaluar el modelo:\n",
    "* El m√©todo `forward` permite obtener la salida de un modelo. \n",
    "    * Para la clase Sequential, que est√° compuesta por varias capas, `forward` devuelve la salida de la √∫ltima capa, sin el error\n",
    "    * Para un problema de clasificaci√≥n, debemos calcular el argmax ya que la salida son probabilidades de clase para cada ejemplo.\n",
    "\n",
    "Adem√°s, `edunn` tiene algunas funcionalidades extra para simplificar el uso de las redes:\n",
    "\n",
    "* El m√≥dulo `metrics` tiene algunas funciones para evaluar m√©tricas de desempe√±o del mismo.\n",
    "* El m√≥dulo `plot` tiene algunas funciones para monitorear el entrenamiento del modelo (`plot_history`) y, en el caso en que el problema sea de pocas dimensiones (1 o 2), tambi√©n visualizar las fronteras de decisi√≥n o la funci√≥n ajustada (`plot_model_dataset_2d_classification`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot.plot_history(history)\n",
    "\n",
    "# Reporte del desempe√±o\n",
    "y_pred = model.forward(x)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "print(f\"Accuracy final del modelo: {nn.metrics.accuracy(y, y_pred_labels) * 100:0.2f}%\")\n",
    "\n",
    "if din == 2:\n",
    "    # Visualizaci√≥n del modelo, solo si tiene 2 dimensiones\n",
    "    nn.plot.plot_model_dataset_2d_classification(x, y, model, title=dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como habr√°s notado, si bien pudimos definir la red y ejecutar el m√©todo `optimize` para pedirle al modelo que se entrene con descenso de gradiente, el accuracy obtenido es muy malo, es decir ¬°la red no aprende! \n",
    "\n",
    "Esto _no_ es un error: no est√°n implementados ninguno de los m√©todos correspondientes de los modelos (Bias, Linear, etc) ni el optimizador `GradientDescent`. \n",
    "\n",
    "**Tu** tarea ser√° implementar las distintas capas/modelos de la librer√≠a `edunn`, as√≠ como algunos inicializadores y algoritmos de optimizaci√≥n, para que este c√≥digo funcione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementaci√≥n de referencia \n",
    "\n",
    "El [repositorio de edunn](https://github.com/facundoq/edunn) contiene una implementaci√≥n de referencia, que se enfoca en ser f√°cil de entender, y no en la eficiencia de c√≥mputo.\n",
    "\n",
    "En base al c√≥digo de esa implementaci√≥n de referencia, y un programa que lo procesa, se gener√≥ una versi√≥n de edunn en donde se quitaron partes cruciales de la implementaci√≥n de cada capa y otras clases.\n",
    "\n",
    "Para poder reimplementar la librer√≠a, tendr√°s que buscar las l√≠neas de c√≥digo entre los comentarios \n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION START \"\"\"```\n",
    "\n",
    "y\n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION END \"\"\"```\n",
    "\n",
    " y completar con el c√≥digo correspondiente.\n",
    "\n",
    "En todos los casos, es importante enfocarse en buscar una implementaci√≥n f√°cil de entender y que sea correcta, y dejar de lado la eficiencia para una implementaci√≥n posterior.\n",
    "\n",
    "Si bien esta gu√≠a de implementaci√≥n est√° en espa√±ol, la implementaci√≥n de la librer√≠a se ha realizado en ingl√©s para que sea m√°s f√°cil relacionar los conceptos con los de otras librer√≠as.\n",
    "\n",
    "Los siguientes notebooks te guiar√°n en la implementaci√≥n de cada `Model` (modelo), tanto en el m√©todo forward y el backward, y m√©todos importantes de otras clases.\n",
    "\n",
    "En caso de duda, siempre puedes consultar una soluci√≥n posible en la [implementaci√≥n de referencia](https://github.com/facundoq/edunn/tree/main/edunn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan de IMPLEMENTACION\n",
    "\n",
    "Comenzar√°s implementando una *capa* muy simple: `AddConstant`. La palabra *capa* es algo equivalente a *modelo*, pero enfatiza que el modelo est√° destinado a combinarse con otra *capa* para formar un *modelo* m√°s grande.\n",
    "\n",
    "\n",
    "En particular, esta capa realiza una funci√≥n muy simple y no tiene par√°metros:\n",
    "\n",
    "* `AddConstant` agrega una constante a una matriz\n",
    "\n",
    "Por lo tanto, la implementaci√≥n de los m√©todos `forward` y `backward` correspondientes ser√° sencilla y te permitir√° comenzar a familiarizarte con \"edunn\" y la metodolog√≠a.\n",
    "\n",
    "Despu√©s de eso, comenzaremos a implementar capas m√°s complejas, como `Bias` y `Linear` para formar modelos `LinearRegression` con su funci√≥n de error m√°s com√∫n: `SquaredError`, y un `MeanError` para promediar el valor por muestra de errores en un lote de muestras.\n",
    "En ese punto, tambi√©n implementaremos un optimizador `GradientDescent` para poner a prueba nuestros modelos. Despu√©s de eso, nos sumergiremos en capas m√°s complejas como \"Softmax\", para realizar la clasificaci√≥n. Finalmente, implementaremos el modelo \"Secuencial\" para componer varias capas en una red neuronal modular completa.\n",
    "\n",
    "¬°Esperamos que te diviertas mucho programando tu primera red neuronal modular! üï∏Ô∏èüöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
