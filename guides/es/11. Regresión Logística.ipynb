{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Regresión Logística: `forward`\n",
    "\n",
    "Un modelo de Regresión Logística se forma aplicando la función `Softmax` a un modelo de Regresión Lineal. De esta forma, esta función convierte al vector de salida de la Regresión Lineal en un vector que representa una distribución de probabilidad.\n",
    "\n",
    "La función de la Regresión logística es $f(x)=softmax(wx+b)$. No obstante, como hicimos con el modelo `LinearRegression`, podemos ver este modelo como la aplicación de\n",
    "* Una capa `Linear` $f(x)=wx$,\n",
    "* Una capa `Bias` $f(x)=x+b$\n",
    "* Una capa `Softmax` $f(x)=softmax(x)$\n",
    "\n",
    "Es decir, tenemos la siguiente secuencia de transformaciońes `x → Linear → Bias → Softmax → y`. \n",
    "\n",
    "\n",
    "Implementa el método `forward` del modelo `LogisticRegression` en el archivo `edunn/models/logistic_regression.py`. Para eso, ya definimos e inicializamos modelos internos de clase `Linear`, `Bias` y `Softmax`; solo tenés que llamar a sus `forward`s respectivos en el orden adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,0],\n",
    "             [0,1],\n",
    "             [1,1]])\n",
    "\n",
    "w = np.array([[100,0,0],\n",
    "              [0,100,0],\n",
    "              ])\n",
    "b = np.array([0,0,0])\n",
    "linear_initializer = nn.initializers.Constant(w)\n",
    "bias_initializer = nn.initializers.Constant(b)\n",
    "layer=nn.LogisticRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "y = np.array([[1, 0,0],\n",
    "              [0, 1,0],\n",
    "              [0.5,0.5,0]\n",
    "             ])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "y = np.array([[0, 0.5,0.5],\n",
    "              [0.5, 0,0.5],\n",
    "              [0,0,1]\n",
    "             ])\n",
    "linear_initializer = nn.initializers.Constant(-w)\n",
    "bias_initializer = nn.initializers.Constant(-b)\n",
    "layer=nn.LogisticRegression(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Regresión Logística: `backward`\n",
    "\n",
    "El método `backward` de un modelo de `LogisticRegression` es la composición *inversa* de las funciones `backward` de las capas `Linear`, `Bias`, y `Softmax`. Recordá que estas se aplican en el orden contrario al método forward.\n",
    "\n",
    "En este caso, también te ayudamos combinando el diccionario de gradientes de cada capa en un gran diccionario único de gradientes de `LogisticRegression` utilizando el operador `**` que desarma un diccionario, con `{**dict1, **dict2}` que los vuelve a combinar.\n",
    "\n",
    "Implementá el método `backward` del modelo `LogisticRegression`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "din=3 # dimensión de entrada\n",
    "dout=5 # dimensión de salida\n",
    "input_shape=(batch_size,din)\n",
    "\n",
    "# Verificar las derivadas de un modelo de Regresión Logística\n",
    "# con valores aleatorios de `w`, `b`, y `x`, la entrada\n",
    "layer=nn.LogisticRegression(din,dout)\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples,tolerance=1e-5)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística aplicada\n",
    "\n",
    "Ahora que tenemos todos los elementos, podemos definir y entrenar nuestro primer modelo regresión logística para clasificar las flores del conjunto de datos de [Iris](https://www.kaggle.com/uciml/iris).\n",
    "\n",
    "En este caso, vamos a entrenar el modelo con la función de error cuadrático medio; no obstante, si bien esta forma del error funcionará para este problema, hace el que problema de optimización no sea _convexo_ y por ende no haya un único mínimo global. Más adelante, implementaremos la función de error de _Entropía Cruzada_, diseñada específicamente para lidiar con salidas que representan distribuciones de probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "import numpy as np\n",
    "from edunn import metrics,datasets\n",
    "\n",
    "# Cargar datos, donde `y` tiene codificación \"onehot\"\n",
    "# `y` tiene tantas columnas como clases\n",
    "# si el ejemplo i es de clase 2, por ejemplo, entonces\n",
    "# y[i,2]=1 y el resto de los valores de y[i,:] valen 0\n",
    "# (nota: las etiquetas de clase comienzan en 0)\n",
    "x,y,classes=datasets.load_classification(\"iris\",onehot=True)\n",
    "# normalización de los datos\n",
    "x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "n, din = x.shape\n",
    "n, dout = y.shape\n",
    "\n",
    "print(\"Dataset sizes:\", x.shape,y.shape)\n",
    "\n",
    "#Modelo de regresión logística\n",
    "model = nn.LogisticRegression(din,dout)\n",
    "# Error cuadrático medio\n",
    "error = nn.MeanError(nn.SquaredError())\n",
    "optimizer = nn.GradientDescent(lr=0.1,epochs=1000,batch_size=32)\n",
    "\n",
    "# Algoritmo de optimización\n",
    "history = optimizer.optimize(model,x,y,error)\n",
    "nn.plot.plot_history(history,error_name=error.name)\n",
    "\n",
    "\n",
    "print(\"Error del modelo:\")\n",
    "y_pred=model.forward(x)\n",
    "metrics.classification_summary_onehot(y,y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
