{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso de gradiente\n",
    "\n",
    "El descenso de gradiente es una técnica de optimización simple pero efectiva para entrenar modelos derivables.\n",
    "\n",
    "En cada iteración del algoritmo, se calcula la derivada del error respecto a cada uno de los parámetros `δEδp`, actualizan los pesos en la dirección contraria al gradiente. Esta actualización está mediada por el parámetro `α` que indica la tasa de aprendizaje. \n",
    "\n",
    "El algoritmo de descenso de gradiente es simple:\n",
    "\n",
    "```python\n",
    "for i in range(iteraciones):\n",
    "    for p in model.parameters()\n",
    "        # usamos p[:] para modificar los valores de p\n",
    "        # y no crear una nueva variable\n",
    "        p[:] = p - α * δEδp(x,y)\n",
    "```\n",
    "\n",
    "Este pseudocódigo obvia algunas partes engorrosas. En particular, la iteración sobre los valores de entrada `x` y salida `y` de los ejemplos, en su versión por `batches`, y el cálculo del error y las derivadas `δEδp`. \n",
    "\n",
    "La librería `edunn` cuenta con la clase `BatchedGradientOptimizer` que se encarga de eso, y nos permite implementar un optimizador de forma muy simple creando una subclase de ella, e implementando el método `optimize_batch`, en donde solo tenemos preocuparnos por optimizar el modelo utilizando las derivadas calculadas con un batch del conjunto de datos. \n",
    "\n",
    "Para este ejercicio, hemos creado la clase `GradientDescent`, que subclasifica a `BatchedGradientOptimizer`. Implemente, entonces, la parte crucial del método `optimize_batch` de `GradientDescent`, para que actualice los parámetros en base a los los gradientes ya calculados.\n",
    "\n",
    "Para probar este optimizador, vamos a utilizar un modelo falso y error falso que nos permitan controlar de manera la entrada al optimizador. La flexiblidad de la clase `Model` de `edunn` permite hacer esto muy fácilmente creando las clases `FakeModel` y `FakeError`, que ignoran realmente sus entradas y salidas, y solo sirven para que `FakeModel` inicialice 2 parámetros con valore 0 y retorne `[-1,1]` como derivada para ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo falso con un vector de parámetros con valor inicial [0,0] y gradientes que siempre son [1,-11]\n",
    "model = nn.FakeModel(parameter=np.array([0,0]),gradient=np.array([1, -1]))\n",
    "# función de error falso cuyo error es siempre 1 y las derivadas también\n",
    "error = nn.FakeError(error=1,derivative_value=1)\n",
    "\n",
    "# Conjunto de datos falso, que no se utilizará realmente\n",
    "fake_samples = 3\n",
    "fake_x = np.random.rand(fake_samples,10)\n",
    "fake_y = np.random.rand(fake_samples,5)\n",
    "\n",
    "# Optimizar el modelo por 1 época con lr=2\n",
    "optimizer = nn.GradientDescent(batch_size=fake_samples,epochs=1,lr=2,shuffle=False)\n",
    "history = optimizer.optimize(model,fake_x,fake_y,error,verbose=False)\n",
    "expected_parameters=np.array([-2,2])\n",
    "utils.check_same(expected_parameters,model.get_parameters()[\"parameter\"])\n",
    "\n",
    "# Optimizar el modelo por 1 época *adicional* con lr=2\n",
    "history = optimizer.optimize(model,fake_x,fake_y,error,verbose=False)\n",
    "expected_parameters=np.array([-4,4])\n",
    "utils.check_same(expected_parameters,model.get_parameters()[\"parameter\"])\n",
    "    \n",
    "# Optimizar el modelo por 3 épocas más, ahora con con lr=1    \n",
    "optimizer = nn.GradientDescent(batch_size=fake_samples,epochs=3,lr=1,shuffle=False)\n",
    "history = optimizer.optimize(model,fake_x,fake_y,error,verbose=False)\n",
    "expected_parameters=np.array([-7,7])\n",
    "utils.check_same(expected_parameters,model.get_parameters()[\"parameter\"])    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un modelo de Regresión Lineal con Descenso de gradiente\n",
    "\n",
    "Ahora que tenemos todos los elementos, podemos definir y entrenar nuestro primer modelo `RegresionLineal` para estimar el precio de casas utilizando el conjunto de datos de [Casas de Boston](https://www.kaggle.com/c/boston-housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edunn as nn\n",
    "import numpy as np\n",
    "from edunn import metrics,datasets\n",
    "\n",
    "x,y=datasets.load_regression(\"boston\")\n",
    "x = (x-x.mean(axis=0))/x.std(axis=0)\n",
    "n, din = x.shape\n",
    "n, dout = y.shape\n",
    "print(\"Dataset sizes:\", x.shape,y.shape)\n",
    "\n",
    "#Red con dos capas lineales\n",
    "model = nn.LinearRegression(din,dout)\n",
    "error = nn.MeanError(nn.SquaredError())\n",
    "optimizer = nn.GradientDescent(lr=0.001,epochs=1000,batch_size=32)\n",
    "\n",
    "# Algoritmo de optimización\n",
    "history = optimizer.optimize(model,x,y,error)\n",
    "nn.plot.plot_history(history,error_name=error.name)\n",
    "\n",
    "\n",
    "print(\"Error del modelo:\")\n",
    "y_pred=model.forward(x)\n",
    "metrics.regression_summary(y,y_pred)\n",
    "nn.plot.regression1d_predictions(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación con sklearn\n",
    "\n",
    "Como verificación adicional, calcularemos los parámetros óptimos de un modelo de regresión lineal con sklearn, y visualizamos los resultados. El error debería ser similar al de nuestro modelo (RMSE=3.27 o 3.28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model=linear_model.LinearRegression()\n",
    "model.fit(x,y)\n",
    "y_pred=model.predict(x)\n",
    "print(\"Error del modelo:\")\n",
    "metrics.regression_summary(y,y_pred)\n",
    "print()\n",
    "\n",
    "nn.plot.regression1d_predictions(y,y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
