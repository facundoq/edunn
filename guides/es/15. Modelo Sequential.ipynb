{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import simplenn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo `Sequential` para Redes Neuronales\n",
    "\n",
    "\n",
    "Ya hemos implementado capas/modelos de todo tipo: densas, funciones de activación, de error, etc. Además, tenemos inicializadores, un optimizador basado en descenso de gradiente estocástico, y modelos que combinan otras capas como `LinearRegression` y `LogisticRegression`. \n",
    "\n",
    "Para dar el siguiente paso y poder definir redes neuronales simples, vamos a implementar el modelo `Sequential`. Este modelo generaliza las ideas aplicadas en `LinearRegression`, `LogisticRegression` y `Dense`, es decir, crear una capa en base a otras. En los casos anteriores, las capas a utilizar estaban predefinidas. `Sequential` nos permitirá utilizar cualquier combinación de capas que querramos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de un modelo `Sequential`\n",
    "\n",
    "\n",
    "Un modelo `Sequential` debe crearse con una lista de otros modelos/capas. De esta manera, específicaremos qué transformaciones y en qué orden se realizarán para obtener la salida de la red.\n",
    "\n",
    "Podemos ver varios ejemplos en donde creamos un modelo de regresión lineal, logística, o una capa Dense en base al modelo `Sequential`.\n",
    "\n",
    "`Sequential` también tiene un método muy útil, `summary()`, que nos permite obtener una descripción de las capas y sus parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "din=5\n",
    "dout=3\n",
    "\n",
    "# Creamos un modelo de regresión lineal \n",
    "layers = [sn.Linear(din,dout), sn.Bias(dout)]\n",
    "linear_regression = sn.Sequential(layers,name=\"Regresión Lineal\")\n",
    "print(linear_regression.summary())\n",
    "\n",
    "\n",
    "# Creamos un modelo de regresión lineal, pero sin la variable auxiliar `layers`\n",
    "linear_regression = sn.Sequential([sn.Linear(din,dout),\n",
    "                       sn.Bias(dout),\n",
    "                      ],name=\"Regresión Lineal\")\n",
    "print(linear_regression.summary())\n",
    "\n",
    "# Creamos un modelo de regresión logística \n",
    "logistic_regression = sn.Sequential([sn.Linear(din,dout),\n",
    "                       sn.Bias(dout),\n",
    "                       sn.Softmax(dout)\n",
    "                      ],name=\"Regresión Logística\")\n",
    "print(logistic_regression.summary())\n",
    "\n",
    "\n",
    "# Creamos un modelo tipo capa Dense con activación ReLU\n",
    "dense_relu = sn.Sequential([sn.Linear(din,dout),\n",
    "                       sn.Bias(dout),\n",
    "                       sn.ReLU(dout)\n",
    "                      ],name=\"Capa tipo Dense con activación ReLU\")\n",
    "print(dense_relu.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes de varias capas con `Sequential`\n",
    "\n",
    "También vamos a crear nuestras primeras redes neuronales de varias capas, simplemente agregando más capas al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creamos una red con dos capas Dense, y una dimensionalidad de 3 interna\n",
    "network_layer2 = sn.Sequential([sn.Dense(din,3,\"relu\"),\n",
    "                               sn.Dense(3,dout,\"id\")\n",
    "                      ],name=\"Red de dos capas\")\n",
    "print(network_layer2.summary())\n",
    "\n",
    "\n",
    "\n",
    "# Creamos una red con 4 capas Dense\n",
    "# dimensiones internas de 2, 4 y 3\n",
    "# y función de activación final softmax\n",
    "network_layer4 = sn.Sequential([sn.Dense(din,2,\"relu\"),\n",
    "                               sn.Dense(2,4,\"tanh\"),\n",
    "                               sn.Dense(4,3,\"sigmoid\"),\n",
    "                               sn.Dense(3,dout,\"softmax\"),\n",
    "                      ],name=\"Red de dos capas\")\n",
    "print(network_layer4.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramétros de `Sequential`\n",
    "\n",
    "El modelo `Sequential`  también permite obtener fácilmente los parámetros de todos sus modelos internos. Para eso ya hemos implementado el método `get_parameters` que permite obtener _todos_ los parámetros de los modelos internos, pero renombrados para que si, por ejemplo, dos modelos tienen el mismo nombre de sus parámetros, estos nombres no se repitan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombres de los parámetros de network_layer2\")\n",
    "print(network_layer2.get_parameters().keys())\n",
    "\n",
    "print(\"Nombres de los parámetros de network_layer4\")\n",
    "print(network_layer4.get_parameters().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método `forward` de `Sequential`\n",
    "\n",
    "\n",
    "Vamos ahora a implementar el método `forward` de `Sequential`. Para eso, dada una entrada `x`, y una sucesión de modelos `M_1,M_2,...,M_n` de `Sequential`, debemos calcular la salida `y` como:\n",
    "\n",
    "$$ y = M_n(...(M_2(M_1(x))...)$$\n",
    "\n",
    "En términos de código, debemos iterar por los posibles modelos (empezando por el primero) y aplicar el método `forward`\n",
    "\n",
    "````python\n",
    "for m in models:\n",
    "    x = m.forward(x)\n",
    "return x\n",
    "````\n",
    "\n",
    "Implementá `forward` para la clase `Sequential` en `simplenn/models/sequential.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "b = np.array([1,2,3])\n",
    "linear_initializer = sn.initializers.Constant(w)\n",
    "bias_initializer = sn.initializers.Constant(b)\n",
    "layer=sn.Sequential([sn.Linear(2,3,initializer=linear_initializer),\n",
    "                     sn.Bias(3,initializer=bias_initializer)\n",
    "                    ])\n",
    "y = np.array([[-21, -24, -27],\n",
    "              [ 23, 28,  33]])\n",
    "\n",
    "sn.utils.check_same(y,layer.forward(x))\n",
    "\n",
    "linear_initializer = sn.initializers.Constant(-w)\n",
    "bias_initializer = sn.initializers.Constant(-b)\n",
    "layer=sn.Sequential([sn.Linear(2,3,initializer=linear_initializer),\n",
    "                     sn.Bias(3,initializer=bias_initializer)\n",
    "                    ])\n",
    "sn.utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método `backward`\n",
    "\n",
    "\n",
    "Al igual que con `Dense`, para implementar el `backward`, también deberás llamar al `backward` de cada uno de los modelos en el orden _inverso_ al del forward. Dado un tensor `δEδy` que contiene las derivadas del error respecto a cada valor de la salida `y`, debemos calcular:\n",
    "* `δEδx`, la derivada del error respecto a la entrada `x`\n",
    "* `δEδp_i`, la derivada del error respecto a cada parámetro `p_i`\n",
    "\n",
    "Para ello, debemos iterar por los posibles modelos (empezando por el último) y aplicar el método `backward`, propagando el error para atrás, y recolectando en el proceso lo más importante, que son las derivadas del error respecto a los parámetros. En términos de código,\n",
    "\n",
    "````python\n",
    "δEδp = {}\n",
    "for m_i in reverse(models):\n",
    "    δEδy, δEδp_i = m_i.backward(δEδy)\n",
    "    agregar los gradientes de δEδp_i a δEδp\n",
    "return δEδy,δEδp\n",
    "````\n",
    "En este caso, también te ayudamos con la función `merge_gradients` que podés llamar como `self.merge_gradients(layer,δEδp,gradients)`. Esta función te permite agregar los parámetros `δEδp` de la capa `layer` al diccionario de gradientes final `gradients` que se debe retornar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "# Test derivatives of a Sequential model with random values for `w`\n",
    "layer=sn.Sequential([sn.Linear(features_in,features_out),\n",
    "                     sn.Bias(features_out),\n",
    "                     sn.ReLU()\n",
    "                    ])\n",
    "sn.utils.check_gradient.common_layer(layer,input_shape,samples=samples)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !Felicitaciones! \n",
    "\n",
    "!Implementaste todas las funciones básicas de una librería de redes neuronales!\n",
    "\n",
    "Ahora vamos a definir algunas redes neuronales para mejorar el desempeño respecto de los modelos lineales (Regresión Lineal y Regresión Logística)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
