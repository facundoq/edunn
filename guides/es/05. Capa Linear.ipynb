{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Linear\n",
    "\n",
    "En este ejercicio debés implementar la capa `Linear`, que pesa las $I$ variables de entrada para generar $O$ valores de salida mediante la matriz de pesos $w$, de tamaño $I × O$.\n",
    "\n",
    "## Caso con 1 entrada y 1 salida\n",
    "\n",
    "## Caso con I entradas y S salidas\n",
    "\n",
    "En el caso más general, donde $w$ es una matriz que combina $I$ entradas de forma lineal para generar $O$ salidas, entonces $x \\in R^{1×I}$ e $y \\in R^{1×O}$. En este caso definimos entonces a $x$ e $y$ como _vectores fila_. \n",
    "$$\n",
    "x = \\left( x_1, x_2, \\dots, x_I \\right)\\\\\n",
    "y = \\left( y_1, y_2, \\dots, y_O \\right)\n",
    "$$\n",
    "\n",
    "Esta decisión es arbitraria: podrían definirse ambos como vectores columna, podríamos definir a $x$  como vector columna y a $y$ como fila, o viceversa. Dada la forma en que funcionan los frameworks, la definición como vector fila es la más usual, y entonces implica $w$ sea una matriz de tamaño $I×O$, y que la salida de la capa $y$ ahora se defina como:\n",
    "\n",
    "$$ y = x w$$\n",
    "\n",
    "Notamos que\n",
    "* $x w$ ahora es un producto matricial\n",
    "* En este caso es importante el orden entre $x$ y $w$, ya que el producto de matrices no es asociativo\n",
    "\t* Un arreglo de $1×I$ ($x$) multiplicado por otro de $I×O$ ($w$) da como resultado un arreglo de $1×O$ ($y$)\n",
    "\t* La definición inversa, $y=wx$, requeriría que $x$ e $y$ sean vectores columna, o que $w$ tenga tamaño $O×I$, \n",
    "\n",
    "\n",
    "## Lotes\n",
    "\n",
    "Las capas reciben no un solo ejemplo, sino un lote de los mismos. Entonces, dada una entrada `x` de $N×I$ valores, donde $N$ es el tamaño de lote de ejemplos, `y` tiene tamaño $N×O$. El tamaño de $w$ no se ve afectado; sigue siendo $I×O$.\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[[1,-1]]` (tamaño $1×2$) y la capa `Linear` tiene como parámetros `w=[[2.0, 3.0],[4.0,5.0]]` (tamaño $2×2$), entonces la salida `y` será `x . w = [ [1,-1] . [2,4], [1,-1] . [3, 5] ] = [ 1*2+ (-1)*4, 1*3+ (-1)*5] = [-2, -2] `.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación e Inicialización\n",
    "\n",
    "La capa `Linear` tiene un vector de parámetros `w`, que debe crearse en base a un tamaño de entrada y de salida de la capa, que debe establecerse al crearse.\n",
    "\n",
    "Respecto a la inicialización, lo usual es hacerlo con valores aleatorios. Para ello, deberás implementar la clase `initializers.RandomNormal`, que inicializa los parámetros con una normal de media 0 y una desviación estándar que se configura al crearse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una capa Linear con 2 valores de entrada y 3 de salida\n",
    "# inicializado con valores muestreados de una normal\n",
    "# con media 0 y desviación estándar 1e-12\n",
    "\n",
    "std=1e-12\n",
    "input_dimension=2\n",
    "output_dimension=3\n",
    "linear1=nn.Linear(input_dimension,output_dimension,initializer=nn.initializers.RandomNormal(std))\n",
    "print(f\"Nombre de la capa: {linear1.name}\")\n",
    "print(f\"Parámetros de la capa : {linear1.get_parameters()}\")\n",
    "print(\"(deben cambiar cada vez que vuelvas a correr esta celda)\")\n",
    "print()\n",
    "\n",
    "linear2 = nn.Linear(input_dimension,output_dimension,initializer=nn.initializers.RandomNormal(std))\n",
    "\n",
    "w1 = linear1.get_parameters()[\"w\"]\n",
    "w2 = linear2.get_parameters()[\"w\"]\n",
    "\n",
    "\n",
    "print(\"Verificar que los pesos tengan media 0 y desviación std:\")\n",
    "utils.check_mean(w1,0,tol=std)\n",
    "utils.check_mean(w2,0,tol=std)\n",
    "utils.check_std(w1,std,tol=std)\n",
    "utils.check_std(w2,std,tol=std)\n",
    "\n",
    "print(\"Verificar de que dos capas capas tienen valores iniciales de w distintos:\")\n",
    "utils.check_different(w1,w2,tol=std/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "\n",
    "Ahora que sabemos como crear e inicializar objetos de la capa `Linear`, comenzamos con el método `forward`, que podrás encontrar en el archivo `linear.py` de la carpeta `edunn/models`.\n",
    "\n",
    "Para verificar que la implementación de `forward` es correcta, utilizamos el inicializador `Constant`, pero luego por defecto la capa debe seguir utilizando un inicializador aleatorio como `RandomNormal`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "initializer = nn.initializers.Constant(w)\n",
    "\n",
    "layer=nn.Linear(2,3,initializer=initializer)\n",
    "y = np.array([[-22, -26, -30],\n",
    "              [ 22, 26,  30]])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "initializer = nn.initializers.Constant(-w)\n",
    "layer=nn.Linear(2,3,initializer=initializer)\n",
    "utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "En la implementación de la capa `Bias` las fórmulas de las derivadas eran relativamente sencillas, y la complejidad estaba más que todo en cómo utilizar el framework y comprender la diferencia entre la derivada de la entrada y la de los parámetros. \n",
    "\n",
    "El método backward de la capa `Linear` requiere calcular $\\frac{δE}{δy}$ y $\\frac{δE}{δw}$. En términos mecánicos, la implementación es muy similar a la de `Bias`, pero las fórmulas de las derivadas son más complicadas.\n",
    "\n",
    "Para no alargar demasiado este cuaderno, te dejamos [una explicación detallada del cálculo de las derivadas](http://facundoq.github.io/guides/linear.html), tanto para $\\frac{δE}{δx}$ como para $\\frac{δE}{δw}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "# Test derivatives of a Linear layer with random values for `w`\n",
    "layer=nn.Linear(features_in,features_out)\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
