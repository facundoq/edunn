{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librería **simplenn**\n",
    "\n",
    "[simplenn](https://github.com/facundoq/simplenn) es una librería para definir y entrenar redes neuronales basada en [Numpy](https://numpy.org/), diseñada para ser simple de _entender_. \n",
    "\n",
    "Aún más importante, fue diseñada para que sea simple de _implementar_. Es decir, su uso **principal** es como objeto de aprendizaje para comprender como se implementan las redes neuronales modernas en frameworks como [Keras](https://keras.io/) o [Pytorch](https://pytorch.org/). \n",
    "\n",
    "No obstante, también es simple para _utilizar_. Por ejemplo, para definir y entrenar una red neuronal para clasificación con de tres capas con distintas funciones de activación, podemos escribir un código muy similar al de estos frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Arquitectura de la Red:\n",
      "-------------------------------\n",
      "Model Sequential_0:\n",
      "Linear_0 → params: 40\n",
      "Bias_0 → params: 10\n",
      "ReLU_0 → params: 0\n",
      "Linear_1 → params: 200\n",
      "Bias_1 → params: 20\n",
      "TanH_0 → params: 0\n",
      "Linear_2 → params: 30\n",
      "Bias_2 → params: 3\n",
      "Softmax_0 → params: 0\n",
      "Total parameters: 303\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d1a18a6fef4cae9d0180d8105a2b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "#features of input (20) must match first dimension of W (10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10f413d11c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Algoritmo de optimización\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/nn/simplenn/optimizer.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, model, x, y, error_layer, verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mepoch_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mbatch_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mepoch_error\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mbatch_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mepoch_error\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/nn/simplenn/optimizer.py\u001b[0m in \u001b[0;36moptimize_batch\u001b[0;34m(self, model, x, y_true, error_layer, epoch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mErrorModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mδEδy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/nn/simplenn/models/sequential.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         '''\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/nn/simplenn/models/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# check sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mdin\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"#features of input ({d}) must match first dimension of W ({din})\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: #features of input (20) must match first dimension of W (10)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplenn import datasets\n",
    "\n",
    "dataset_name=\"iris\"\n",
    "x,y,classes = datasets.load_classification(dataset_name)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)\n",
    "n,din=x.shape\n",
    "n_classes=y.max()+1\n",
    "\n",
    "import simplenn as sn\n",
    "\n",
    "# Definición del modelo\n",
    "layers = [sn.Linear(din,10),\n",
    "          sn.Bias(10),\n",
    "          sn.ReLU(),\n",
    "          sn.Linear(10,20),\n",
    "          sn.Bias(20),\n",
    "          sn.TanH(),\n",
    "          sn.Linear(10,n_classes),\n",
    "          sn.Bias(n_classes),\n",
    "          sn.Softmax()\n",
    "          ]\n",
    "\n",
    "model = sn.Sequential(layers)\n",
    "print(\"Arquitectura de la Red:\")\n",
    "print(model.summary())\n",
    "\n",
    "error = sn.MeanError(sn.CrossEntropyWithLabels())\n",
    "# Algoritmo de optimización\n",
    "optimizer = sn.StochasticGradientDescent(lr=0.001,epochs=1000,batch_size=32)\n",
    "\n",
    "# Algoritmo de optimización\n",
    "history = optimizer.optimize(model,x,y,error)\n",
    "sn.plot.plot_history(history)\n",
    "\n",
    "\n",
    "# Reporte del desempeño\n",
    "y_pred = model.forward(x)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "print(f\"Accuracy final del modelo: {sn.metrics.accuracy(y,y_pred_labels)*100:0.2f}%\")\n",
    "\n",
    "\n",
    "if din ==2:\n",
    "    # Visualización del modelo\n",
    "    sn.plot.plot_model_dataset_2d_classification(x,y,model,title=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conocimiento previo\n",
    "Para poder implementar la librería, asumimos que ya has adquirido los conceptos básicos de redes neuronales: \n",
    "\n",
    "* Capas\n",
    "    * Capas Lineales\n",
    "    * Funciones de Activación\n",
    "    * Composición de capas\n",
    "    * Métodos forward y backward\n",
    "* Algoritmo de propagación hacia atrás (backpropagation)\n",
    "* Descenso de gradiente\n",
    "    * Cálculo de gradientes\n",
    "    * Optimización básica por gradientes\n",
    "* Cómputo/entrenamiento por lotes (batches)\n",
    "\n",
    "También se asume conocimiento de Python y de Numpy, así como del manejo de bases de datos tabulares y de imágenes.\n",
    "\n",
    "# Componentes de la librería\n",
    "\n",
    "Describimos los componentes básicos de la librería utilizados en el código anterior, para proveer el contexto de los ejercicios a realizar. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo **datasets**\n",
    "\n",
    "\n",
    "El módulo `simplenn.datasets` que permite cargar algunos conjuntos de datos de prueba fácilmente, de modo de verificar y experimentar con los modelos\n",
    "\n",
    "```python\n",
    "dataset_name=\"study_2d_easy\"\n",
    "x,y,classes = datasets.load(dataset_name)\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)\n",
    "n,din=x.shape\n",
    "n_classes=y.max()+1\n",
    "```\n",
    "\n",
    "Para ver qué otros conjuntos de datos para clasificación o regresión tiene módulo  `datasets`, podés ejecutar `datasets.get_classification_names()` y `datasets.get_regression_names()` y obtener una lista de nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los conjuntos de datos de clasificación disponibles son:\n",
      "dict_keys(['iris', 'study1d', 'study2d', 'study2d_easy'])\n",
      "\n",
      "Los conjuntos de datos de regresión disponibles son:\n",
      "dict_keys(['study1d', 'study2d', 'boston', 'wine_red', 'wine_white', 'insurance', 'real_state'])\n",
      "\n",
      "Clases del dataset 'study2d':\n",
      "['Failed', 'Passed']\n"
     ]
    }
   ],
   "source": [
    "print(\"Los conjuntos de datos de clasificación disponibles son:\")\n",
    "print(datasets.get_classification_names())\n",
    "print()\n",
    "\n",
    "print(\"Los conjuntos de datos de regresión disponibles son:\")\n",
    "print(datasets.get_regression_names())\n",
    "print()\n",
    "\n",
    "# Ejemplo de carga de otro dataset\n",
    "dataset_name=\"study2d\"\n",
    "x,y,classes=datasets.load_classification(dataset_name)\n",
    "print(f\"Clases del dataset '{dataset_name}':\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clases y módulos de simplenn\n",
    "\n",
    "Para usar `simplenn`, importamos la librería y la llamamos `sn` de modo que sea más legible su uso.\n",
    "```python\n",
    "import simplenn as sn\n",
    "```\n",
    "\n",
    "La librería tiene varias clases para crear modelos:\n",
    "\n",
    "\n",
    "* Las clases `Linear`, `Bias`, que permiten crear capas con las funciones $wx$ y $x+b$ respectivamente. En estos casos, $w$ y $b$ son parámetros a optimizar. Combinando estas capas se puede formar una capa densa tradicional que calcula $wx+b$.\n",
    "* Las clases`TanH`, `ReLU` y `Softmax`, que permiten crear capas con las funciones de activación de esos nombres.\n",
    "* La clase `CrossEntropyWithLabels`, una capa que permite calcular el error de la red basado en entropía cruzada  _para cada ejemplo_. Esta capa además se combina con `MeanError` que permite calcular el error promedio de otra capa de error que calcula el mismo _para cada ejemplo_, como la capa `CrossEntropyWithLabels` que mencionamos, u otras como `MeanSquaredError` para el error cuadrático medio.\n",
    "* La clase `Sequential` para crear redes secuenciales, donde donde la salida de cada capa es la entrada de la capa siguiente, y hay solo una capa inicial y una final.\n",
    "    \n",
    "Cada una de estas clases es una subclase de `Model`, y por ende permite hacer 2 operaciones:\n",
    "* `forward(x)`: computa la salida `y` dada una entrada `x`. \n",
    "    * Como asunción para simplificar la librería, los modelos sólo podrán tener una entrada y una salida, que deben ser un arreglo de numpy (excepto los de error). En la práctica, veremos que esta no es una limitación importante.\n",
    "* `backward(δEδy)`: computa el gradiente del error respecto a la entrada (`δEδx`), utilizando el gradiente del error respecto a la salida (`δEδy`). Si el modelo/capa tiene parámetros, también calcula el gradiente respecto a estos parámetros.\n",
    "    * `backward` permite hacer una implementación desacoplada del algoritmo backpropagation.\n",
    "    * Utilizando el `backward` de un modelo, se puede optimizarlo mediante descenso de gradiente.\n",
    "\n",
    "```python\n",
    "layers = [sn.Linear(din,10),\n",
    "          sn.Bias(10),\n",
    "          sn.ReLU(),\n",
    "          sn.Linear(10,20),\n",
    "          sn.Bias(20),\n",
    "          sn.TanH(),\n",
    "          sn.Linear(20,n_classes),\n",
    "          sn.Bias(n_classes),\n",
    "          sn.Softmax()\n",
    "          ]\n",
    "error = sn.MeanError(sn.CrossEntropyWithLabels())\n",
    "model = sn.Sequential(layers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar un modelo, podemos utilizar un objeto `Optimizer`, cuyo método `optimize` permite, dados arreglos `x` e `y` y una función de error, entrenar un modelo para minimizar ese error en este conjunto de datos. \n",
    "Para este entrenamiento, debe especificarse un algoritmo de optimización. En este caso utilizamos descenso de gradiente simple con la clase `GradientDescent`, una tasa de aprendizaje de `0.1`, `100` épocas y un tamaño de lote de 16.\n",
    "\n",
    "```python\n",
    "# Algoritmo de optimización\n",
    "optimizer = sn.StochasticGradientDescent(lr=0.001,epochs=1000,batch_size=32)\n",
    "\n",
    "# Optimización\n",
    "history = optimizer.optimize(model,x,y,error)\n",
    "sn.plot.plot_history(history)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Por último, podemos utilizar y evaluar el modelo:\n",
    "* El método `forward` permite obtener la salida de un modelo. \n",
    "    * Para la clase Sequential, que está compuesta por varias capas, `forward` devuelve la salida de la última capa, sin el error\n",
    "    * Para un problema de clasificación, debemos calcular el argmax ya que la salida son probabilidades de clase para cada ejemplo.\n",
    "* El módulo `metrics` tiene algunas funciones para evaluar métricas de desempeño del mismo.\n",
    "* El módulo `plot` tiene algunas funciones para monitorear el entrenamiento del modelo (`plot_history`) y, en el caso en que el problema sea de pocas dimensiones (1 o 2), también visualizar las fronteras de decisión o la función ajustada (`plot_model_dataset_2d_classification`)\n",
    "\n",
    "```python\n",
    "sn.plot.plot_history(history)\n",
    "\n",
    "# Reporte del desempeño\n",
    "y_pred = model.forward(x)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "print(f\"Accuracy final del modelo: {sn.metrics.accuracy(y,y_pred_labels)*100:0.2f}%\")\n",
    "\n",
    "\n",
    "if din ==2:\n",
    "    # Visualización del modelo, solo si tiene 2 dimensiones\n",
    "    sn.plot.plot_model_dataset_2d_classification(x,y,model,title=dataset_name)\n",
    "```\n",
    "\n",
    "Como habrás notado, si bien pudimos definir así la red y ejecutar el método `optimize` para pedirle al modelo que se entrene con descenso de gradiente, esta red no aprende, ya que no están implementados correctamente ninguno de los métodos correspondientes de los modelos (Bias, Linear, etc) y el optimizador StochasticGradientDescent. \n",
    "\n",
    "Tu objetivo es implementar las distintas capas/modelos de la librería `simplenn`, así como algunos inicializadores y algoritmos de optimización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de referencia \n",
    "\n",
    "El [repositorio de simplenn](https://github.com/facundoq/simplenn) contiene una implementación de referencia, que se enfoca en ser fácil de entender, y no en la eficiencia de cómputo.\n",
    "\n",
    "En base al código de esa implementación de referencia, y un programa que lo procesa, se generó una versión de simplenn en donde se quitaron partes cruciales de la implementación de cada capa y otras clases.\n",
    "\n",
    "Para poder reimplementar la librería, tendrás que buscar las líneas de código entre los comentarios `\"\"\" COMPLETAR COMIENZO \"\"\"` y `\"\"\" COMPLETAR FIN \"\"\"` y completar con el código correspondiente.\n",
    "\n",
    "En todos los casos, es importante enfocarse en buscar una implementación fácil de entender y que sea correcta, y dejar de lado la eficiencia para una implementación posterior.\n",
    "\n",
    "Si bien esta guía de implementación está en español, la implementación de la librería se ha realizado en inglés para que sea más fácil relacionar los conceptos con los de otras librerías.\n",
    "\n",
    "Los siguientes notebooks te guiarán en la implementación de cada Modelo, tanto en el método forward y el backward, y métodos importantes de otras clases.\n",
    "\n",
    "En caso de duda, siempre puedes consultar la [implementación de referencia](https://github.com/facundoq/simplenn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
