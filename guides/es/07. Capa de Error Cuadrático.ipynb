{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa de Error Cuadrático\n",
    "\n",
    "En este ejercicio debés implementar la capa de error `SquaredError`, que permite calcular el error de un lote de ejemplos. \n",
    "\n",
    "Las capas de error son diferentes a las capas normales por dos motivos:\n",
    "\n",
    "1. No solo tienen como entrada la salida de la capa anterior, sino también el valor esperado de la capa anterior (`y` e `y_true`). Deben subclasificar `ErrorModel` en lugar de `Model`.\n",
    "2. Para un lote de $n$ ejemplos, su salida es un vector de tamaño $n$. Es decir, indican el valor del error de cada ejemplo con un escalar (número real).\n",
    "\n",
    "La capa de error también debe poder realizar la operación `backward` de modo de poder propagar hacia atrás el gradiente del error a la red. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "\n",
    "El método `forward` de la capa `SquaredError` simplemente debe calcular la distancia euclídea al cuadrado entre `y`, los valores producidos por la red, e `y_true`, el valor esperado por la misma.\n",
    "\n",
    "Por ejemplo, si $y=[2,-2]$ y $y_{true}=[3,3]$, entonces la salida de la capa es:\n",
    "\n",
    "$$E(y,y_{true})=d_2(y,y_{true})=d_2([2,-2],[3,3])=(2-3)^2+(-2-3)^2 = 1^2+(-5)^2=1+25=26$$\n",
    "\n",
    "En general, dados dos vectores $a=[a_1,\\dots,a_n]$ y $b=[b_1,\\dots,b_n]$, la distancia euclídea al cuadrado $d_2$ es:\n",
    "\n",
    "$$\n",
    "d_2(a,b)= d_2([a_1,\\dots,a_n],[b_1,\\dots,b_n]) =(a_1-b_1)^2+\\dots+(a_n-b_n)^2\n",
    "$$\n",
    "\n",
    "En el caso de un lote de ejemplos, el cálculo es independiente para cada ejemplo. Es importante entonces que la suma de las diferencias al cuadrado se haga por cada ejemplo (fila) y no por cada característica (columna).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[2,-2],\n",
    "             [-4,4]])\n",
    "y_true = np.array([[3,3],\n",
    "             [-5,2]])\n",
    "\n",
    "\n",
    "layer=nn.SquaredError()\n",
    "E=np.array([[26],[5]])\n",
    "utils.check_same(E,layer.forward(y,y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Ahora puedes calcular el error de una red, bien! Esta es la capa final de la red cuando se está entrenando. Por ende el método backward de una capa de error no recibe $\\frac{δE}{δy}$; de hecho, debe calcularlo directamente a partir de $y$, $y_{true}$, y la definición del error. Además tampoco hay parámetros.\n",
    "\n",
    "Por ende, en este caso, la derivada es simple. Solo debemos calcular $\\frac{δE}{δy}$, la derivada del error respecto a la salida calculada por la red, $y$.\n",
    "En este caso $E$ es simétrico respecto de sus entradas, así que llamemosla nuevamente $a$ y $b$, y entonces calculemos la derivada respecto del elemento $i$ de $a$ (la de $b$ sería igual):\n",
    "\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa_i} = \\frac{δ((a_1-b_1)^2+\\dots+(a_n-b_n)^2)}{δa_i} \\\\\n",
    "= \\frac{δ((a_i-b_i)^2)}{δa_i} = 2 (a_i-b_i) \\frac{δ((a_i-b_i))}{δa_i} \\\\\n",
    "= 2 (a_i-b_i) 1 = 2 (a_i-b_i)\n",
    "$$\n",
    "Generalizando para todo el vector $a$, entonces:\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa} = 2 (a-b)\n",
    "$$\n",
    "Donde $a-b$ es una resta entre vectores.\n",
    "\n",
    "Nuevamente, como este error es por cada ejemplo, entonces los cálculos son independientes en cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "\n",
    "layer=nn.SquaredError()\n",
    "utils.check_gradient.squared_error(layer,input_shape,samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
