{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplenn import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Dense\n",
    "\n",
    "Las capas `Linear`, `Bias` y de activación (`Sigmoid`, `ReLU`, `TanH` y otras) suelen usarse en conjunto con la forma `dense(x) = activation(w*x+b)`, donde `activation` es alguna de estas funciones de activación. Esta capa suele denominarse `FullyConnected` o, como la llamaremos acá, `Dense`; el nombre viene de que cada salida de la capa depende de *todas* las entradas.\n",
    "\n",
    "En este ejercicio debés implementar la capa `Dense`, pero utilizando las capas `Linear`, `Bias` y de activaciones directamente, sin copiar su código.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación e Inicialización\n",
    "\n",
    "La capa `Dense` debería tener un vector de parámetros `w`, otro vector `b` cada uno con un inicializador particular. Además, debería una función de activación.\n",
    "\n",
    "Pero para implementarla, vamos a utilizar tiene 3 *capas internas*: `Linear`, `Bias` y la de activación que llamaremos `Activation`. Por comodidad vamos también vamos a permitir especificar la activación con un string como `relu`, `sigmoid` o `tanh`.  En este caso, te ayudamos ya definiendo el constructor `__init__`, que asigna a las variables de instancia `self.linear`, `self.bias` y `self.activation` los objetos de las *capas internas* correspondientes y permite especificar los inicializadores de las mismas.\n",
    "\n",
    "También te ayudamos implementando `get_parameters`, que combina el diccionario de parámetros de cada subcapa en un gran diccionario único de gradientes de `Dense`.\n",
    "\n",
    "Te recomendamos que estudies el código de estos dos métodos (`__init__` y `get_parameters`) para ver como funcionan. Te ayudarán para luego implementar el `forward` y `backward` de `Dense`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la capa: Dense_77(relu)\n",
      "Parámetro w de la capa: [[ 3.00844903e-10 -2.08481134e-10 -2.96463558e-11]\n",
      " [ 1.65211056e-11  2.11435491e-11 -1.95025209e-11]]\n",
      "(debe cambiar cada vez que vuelvas a correr esta celda)\n",
      "Parámetro b de la capa: [0. 0. 0.]\n",
      "(debe valer siempre 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos una capa Densa con 2 valores de entrada y 3 de salida\n",
    "# y activación ReLU\n",
    "# La capa lineal se inicializa de forma aleatoria\n",
    "# Mientras que la capa de bias en 0\n",
    "\n",
    "input_dimension=2\n",
    "output_dimension=3\n",
    "activation=\"relu\"\n",
    "dense1=sn.Dense(input_dimension,output_dimension,\n",
    "                 activation_name=\"relu\",\n",
    "                 linear_initializer=sn.initializers.RandomNormal(),\n",
    "                 bias_initializer=sn.initializers.Constant(0),\n",
    "                 )\n",
    "print(f\"Nombre de la capa: {dense1.name}\")\n",
    "print(f\"Parámetro w de la capa: {dense1.get_parameters()['w']}\")\n",
    "print(\"(debe cambiar cada vez que vuelvas a correr esta celda)\")\n",
    "print(f\"Parámetro b de la capa: {dense1.get_parameters()['b']}\")\n",
    "print(\"(debe valer siempre 0)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método `forward`\n",
    "\n",
    "\n",
    "Ahora que sabemos como crear e inicializar objetos de la capa `Dense`, comenzamos con el método `forward`, que podrás encontrar en el archivo `dense.py` de la carpeta `simplenn/models`.\n",
    "\n",
    "Para implementar el forward, deberás tomar el `x` de entrada y usarlo llamar al `forward` de las capas internas de tipo `Linear`, `Bias` y la `Activation`.\n",
    "\n",
    "Para verificar que la implementación de `forward` es correcta, utilizamos ambas veces el inicializador `Constant`, pero luego por defecto la capa sigue utilizando un inicializador aleatorio como `RandomNormal` por defecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3,-7],\n",
    "             [-3,7]])\n",
    "\n",
    "w = np.array([[2, 3, 4],[4,5,6]])\n",
    "b = np.array([1,2,3])\n",
    "linear_initializer = sn.initializers.Constant(w)\n",
    "bias_initializer = sn.initializers.Constant(b)\n",
    "layer=sn.Dense(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "y = np.array([[-21, -24, -27],\n",
    "              [ 23, 28,  33]])\n",
    "\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "linear_initializer = sn.initializers.Constant(-w)\n",
    "bias_initializer = sn.initializers.Constant(-b)\n",
    "layer=sn.Dense(2,3,linear_initializer=linear_initializer,bias_initializer=bias_initializer)\n",
    "utils.check_same(-y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método `backward`\n",
    "\n",
    "\n",
    "Para implementar el `backward`, también deberás llamar al `backward` de las variables `self.linear`, `self.bias` y `self.activation` en el orden y forma correcta. Pista: es el contrario al del `forward`.\n",
    "\n",
    "En este caso, también te ayudamos combinando el diccionario de gradientes de cada capa en un gran diccionario único de gradientes de `Dense` utilizando el operador `**dict` que desarma un diccionario y `{**dict1, **dict2}` que los vuelve a combinar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[104m\u001b[30mDense_78(relu) layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 2600 partial derivatives checked (100 random input samples)\n"
     ]
    }
   ],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "# Test derivatives of a Dense layer with random values for `w`\n",
    "layer=sn.Dense(features_in,features_out,activation_name='relu')\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
