{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplenn import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capas Dense o Fully Connected o de Regresión Lineal\n",
    "\n",
    "\n",
    "La capa más común de una red es una capa que implementa la función `y = xw+b`, donde `x` es la entrada, `y` la salida, y `b` es un vector de sesgos y `w` es una matriz de pesos. No obstante, implementar esta capa puede ser difícil. En lugar de eso, separaremos la implementación en dos partes.\n",
    "\n",
    "* La capa Bias, que solo sumará `b` a su entrada, es decir `y=x+b`\n",
    "* La capa Linear, que sólo multiplicará a la entrada por la matriz de pesos `w`, es decir `y=w*x`\n",
    "* Combinando ambas capas, podremos recuperar la funcionalidad de la capa tradicional llamada `Dense` o `FullyConnected` en otras librerías, que por si sola nos permite resolver problemas con un modelo de regresión lineal.\n",
    "\n",
    "Comenzamos entonces con la capa `Bias`, la más simple de las dos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Bias\n",
    "\n",
    "\n",
    "En este ejercicio debés implementar la capa `Bias`, que suma un valor distinto a cada una de sus entradas para generar su salida. Este valor _NO_ es constante, sino que esun parámetro de la red\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[3.5,-7.2]` y la capa `Bias` tiene como parámetros `[2.0, 3.0]`, entonces la salida `y` será `[5.5,-4.2]`.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n",
    "\n",
    "Esta capa funciona para arreglos que tengan el mismo tamaño que los parámetros de la capa `Bias` (sin contar la dimensión de lote o batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación e Inicialización\n",
    "\n",
    "La capa `Bias` tiene un vector de parámetros `b`, que debe crearse e inicializarse de alguna forma. Además, este parámetro se registra en la capa para poder se accedido posteriormente:\n",
    "\n",
    "```python\n",
    "class Bias(Layer):\n",
    "    def __init__(self,output_size:int,initializer:Initializer=Zero(),name=None):\n",
    "            super().__init__(name=name)\n",
    "            b = initializer.create( (output_size,))\n",
    "            self.register_parameter(\"b\", b)\n",
    "```\n",
    "\n",
    "Vemos que al crear la capa podemos pasar un objeto de tipo `Initializer` que va a crear y asignarle el valor inicial al parámetro `b`. Por defecto, `b` se inicializa con ceros utilizando la clase `initializers.Zero`. \n",
    "\n",
    "\n",
    "```python\n",
    "class Zero(Initializer):\n",
    "    def initialize(self,p:np.ndarray):\n",
    "        p[:]=0\n",
    "```\n",
    "\n",
    "Examinando la implementación de esta clase, podemos ver que:\n",
    "* Hereda de Initializer\n",
    "* Implementa el método `initialize(self,p:np.ndarray)` que recibe un arreglo de numpy para inicializar\n",
    "* Utiliza `p[:]` para inicializar en 0 en lugar de `p=0`. Hay dos razones importantes para esto:\n",
    "    * Utilizar `p=0` solo cambiaría la _variable local_ `p` en lugar de cambiar el _arreglo_ de numpy al cual `p` apunta\n",
    "    * Al utilizar `p[:]` estamos cambiando el __contenido__ del arreglo de parámetros, que pertenece a una clase como `Bias` u otra que implementemos luego.\n",
    "\n",
    "Una vez creada la clase, podemos obtener el vector de parámetros `p` de la clase `Bias`con el método `get_parameters()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la capa: Bias_0\n",
      "Parámetros de la capa: {'b': array([0., 0.])}\n",
      "\n",
      "Nombre de la capa: Bias_1\n",
      "Parámetros de la capa: {'b': array([0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "# Creamos una capa Bias con 2 valores de entrada/salida\n",
    "bias=sn.Bias(2,initializer=sn.initializers.Zero())\n",
    "print(f\"Nombre de la capa: {bias.name}\")\n",
    "print(f\"Parámetros de la capa: {bias.get_parameters()}\")\n",
    "print()\n",
    "\n",
    "# Por defecto, el inicializador ya es `Zero`\n",
    "bias2=sn.Bias(2)\n",
    "print(f\"Nombre de la capa: {bias2.name}\")\n",
    "print(f\"Parámetros de la capa: {bias.get_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceso a los parámetros por nombre\n",
    "\n",
    "El método `get_parameters()` devuelve un diccionario de parámetros, ya que admite tener más de un parámetro por capa.\n",
    "\n",
    "\n",
    "Dado que ya sabemos en este caso cuál es el nombre del único parámetro de la capa, podemos acceder al mismo con su nombre en string `'b'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la capa: Bias_2\n",
      "Parámetro 'b' de la capa: [0. 0.]\n",
      "\n",
      "Nombre de la capa: Bias_3\n",
      "Parámetro 'b' de la capa: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Creamos una capa Bias con 2 valores de entrada/salida\n",
    "bias=sn.Bias(2,initializer=sn.initializers.Zero())\n",
    "print(f\"Nombre de la capa: {bias.name}\")\n",
    "print(f\"Parámetro 'b' de la capa: {bias.get_parameters()['b']}\")\n",
    "print()\n",
    "\n",
    "# Por defecto, el inicializador ya es `Zero`\n",
    "bias2=sn.Bias(2)\n",
    "print(f\"Nombre de la capa: {bias2.name}\")\n",
    "print(f\"Parámetro 'b' de la capa: {bias2.get_parameters()['b']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de un inicializador constante\n",
    "\n",
    "Antes de comenzar con la implementación de la clase `Bias`, debés implementar el inicializador `Constant` que le asigna un valor o arreglo constante al parámetro, de modo que por ejemplo, se pueda inicializar `b` con todos valores `3` o con un vector de valores `[1,2,3,4]`. \n",
    "\n",
    "Buscá la clase `Constant` en el modulo `simplenn/initializers.py` e implementa el método `initialize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la capa: Bias_4\n",
      "Parámetro 'b' de la capa: [3. 3.]\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n",
      "\n",
      "Nombre de la capa: Bias_5\n",
      "Parámetro 'b' de la capa: [1. 2. 3. 4.]\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "# Creamos una capa Bias con 2 valores de salida (y también de entrada). \n",
    "#Los parámetros están todos inicializados con 3\n",
    "valor = 3\n",
    "bias=sn.Bias(2,initializer=sn.initializers.Constant(valor))\n",
    "\n",
    "print(f\"Nombre de la capa: {bias.name}\")\n",
    "print(f\"Parámetro 'b' de la capa: {bias.get_parameters()['b']}\")\n",
    "utils.check_same(bias.get_parameters()['b'],np.array([3,3]))\n",
    "print()\n",
    "\n",
    "\n",
    "# Creamos una capa Bias con valores iniciales  1,2,3,4\n",
    "valor = np.array([1,2,3,4])\n",
    "bias=sn.Bias(4,initializer=sn.initializers.Constant(valor))\n",
    "\n",
    "print(f\"Nombre de la capa: {bias.name}\")\n",
    "print(f\"Parámetro 'b' de la capa: {bias.get_parameters()['b']}\")\n",
    "\n",
    "utils.check_same(bias.get_parameters()['b'],valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "\n",
    "Ahora que sabemos como crear e inicializar objetos de la capa `Bias`, comenzamos con el método `forward`, que podrás encontrar en el archivo `bias.py` de la carpeta `simplenn/models`.\n",
    "\n",
    "Si los parámetros a sumar son $[b_1,...,b_f]$ y la entrada a la capa es $x = [x_1,x_2,...,x_f] $, entonces la salida $y$ es:\n",
    "\n",
    "$\n",
    "y([x_1,x2,...,x_f])= [x_1+b_1,x_2+b_2,...,x_n+b_f]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3.5,-7.2,5.3],\n",
    "             [-3.5,7.2,-5.3]])\n",
    "\n",
    "initializer = sn.initializers.Constant(np.array([2,3,4]))\n",
    "\n",
    "layer=sn.Bias(3,initializer=initializer)\n",
    "y = np.array([[ 5.5, -4.2,  9.3],\n",
    "              [-1.5, 10.2, -1.3]])\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "initializer = sn.initializers.Constant(-np.array([2,3,4]))\n",
    "layer=sn.Bias(3,initializer=initializer)\n",
    "y = np.array([[  1.5, -10.2,   1.3],\n",
    "              [ -5.5,   4.2,  -9.3]]\n",
    ")\n",
    "utils.check_same(y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Además del cálculo de la salida de la capa, la misma debe poder propagar hacia atrás el gradiente del error de la red. Para eso, debés implementar el método `backward` que recibe $\\frac{δE}{δy}$, es decir, las derivadas parciales del error respecto a la salida (gradiente) de esta capa , y devolver $\\frac{δE}{δx}$, las derivadas parciales del error respecto de las entradas de esta capa. \n",
    "\n",
    "## `δEδx`\n",
    "Para la capa `Bias` el cálculo del gradiente respecto de la entrada `δEδx` es simple, ya que es el mismo caso que con la capa `AddConstant`. \n",
    "\n",
    "$ \\frac{δE}{δx} =\\frac{δE}{δy} $\n",
    "\n",
    "No obstante, para esta capa también deberás implementar el gradiente con respecto a los parámetros `b`, de modo que se puedan optimizar para minimizar el error. Entonces también deberás calcular `δEδb`. Recordemos que:\n",
    "\n",
    "$\n",
    "y([x_1,x2,...,x_f])= [x_1+b_1,x_2+b_2,...,x_n+b_f]\n",
    "$\n",
    "\n",
    "Entonces, utilizando la regla de la cadena:\n",
    "$\n",
    "\\frac{δE}{δb_i} = \\frac{δE}{δy} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δy_i}{δb_i} = \\frac{δE}{δy_i} \\frac{δ x_i+b_i}{δb_i} = \\frac{δE}{δy_i} 1 = \\frac{δE}{δy_i}  \n",
    "$\n",
    "\n",
    "Es decir,\n",
    "\n",
    "$ \\frac{δE}{δx} =\\frac{δE}{δy} $\n",
    "\n",
    "## `δEδb` \n",
    "\n",
    "En el caso del gradiente del error con respecto a `b`, la fórmula es la misma, $ \\frac{δE}{δb} =\\frac{δE}{δy}$. Esto se debe a que $\\frac{δy_i}{δb_i} = \\frac{δ(x_i+b_i)}{δb_i} = \\frac{δ(x_i+b_i)}{δx_i} =1$. \n",
    "\n",
    "Es decir, si vemos tanto a $b$ como a $x$ como entradas a la capa, $x+b$ es simétrico en $x$ y $b$ y por ende también lo son sus derivadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[104m\u001b[30mBias_17 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1200 partial derivatives checked (100 random input samples)\n",
      "\u001b[104m\u001b[30mBias_18 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1200 partial derivatives checked (100 random input samples)\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de valores aleatorios y tamaño de lote \n",
    "# para generar valores de x y δEδy para la prueba de gradientes\n",
    "samples = 100\n",
    "batch_size=2\n",
    "\n",
    "# Dimensiones de entrada y salida de la capa, e inicializador\n",
    "features=4\n",
    "input_shape=(batch_size,features)\n",
    "initializer = sn.initializers.Constant(np.array(range(features)))\n",
    "\n",
    "# Verificar derivadas de una capa Bias que con b=[0,1,2,3]\n",
    "layer=sn.Bias(features)\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "initializer = sn.initializers.Constant(-np.array(range(features)))\n",
    "# Verificar derivadas de una capa Bias que con b=[0,-1,-2,-3]\n",
    "layer=sn.Bias(features)\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
