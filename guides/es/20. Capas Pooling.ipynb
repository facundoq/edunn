{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capas Pooling\n",
    "\n",
    "La operación de _pooling_ ayuda a reducir la dimensionalidad espacial de los feature maps, así reduciendo la cantidad de parámetros de la red. Básicamente son convoluciones, donde el stride habitualmente es igual al tamaño del kernel y donde se calcula alguna función sobre todos los píxeles. Lo más usual es el máximo, el mínimo o el promedio. No solo reducen la dimensionalidad (así logrando un entrenamiento más rápido), sino que además generalmente ayudan en la clasificación, ya que permiten mayor eficacia al momento de generalizar.\n",
    "\n",
    "Una práctica comun para realizar una arquitectura convolucional es intercalar capas ``Conv2d`` con capas ``Pooling`` hasta llegar a capas ``Dense`` (Feed-Forward) que discriminarán las características aprendidas por las capas anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Forward\n",
    "\n",
    "La operación de _Max Pooling_ es similar a la de convolución, pero sin necesidad de realizar una suma ponderada elemento a elemento, ya que en este caso se selecciona el máximo elemento de cada región particular encerrada por el kernel.\n",
    "\n",
    "La explicación de su cálculo es sencilla a partir de un ejemplo, supongamos que $x \\in \\mathbb{R}^{(4\\times 4)}$ y $F=S=2$, de modo los $y_{ij}$ se definen como:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "y_{11} = \\max ( x_{11},x_{12},x_{21},x_{22} ) & y_{12} = \\max ( x_{13},x_{14},x_{23},x_{24} ) \\\\\n",
    "y_{21} = \\max ( x_{31},x_{32},x_{41},x_{42} ) & y_{22} = \\max ( x_{33},x_{34},x_{43},x_{44} )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Esto nos permite ver que el shape de salida de una capa Max Pool 2D está descripto por la misma ecuación que para el shape de salida de una capa convolucional.\n",
    "\n",
    "Implementa el método `forward` del modelo `MaxPool2d` en el archivo `edunn/models/pooling.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "x = np.random.rand(2,3,7,7)\n",
    "\n",
    "kernel_size,stride=2,2\n",
    "\n",
    "layer=nn.MaxPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "y=layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Backward\n",
    "\n",
    "Conociendo el gradiente de la función de error con respecto a los píxeles de salida, debemos calcular el gradiente total del error con respecto a los píxeles de entrada. Utilizando la regla de la cadena y derivadas parciales podemos llegar a la expresión:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial x_{mn}}=\\sum_{ij}\\frac{\\partial E}{\\partial y_{ij}}\\frac{\\partial y_{ij}}{\\partial x_{mn}} \n",
    "$$\n",
    "\n",
    "Tener en cuenta que las capas pooling no tienen parámetros, por lo que no hay que realizar el cálculo del gradiente del kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dE/dx`\n",
    "\n",
    "El cálculo de los gradientes del error $E$ con respecto al feature map de entrada $x$ se puede hacer derivando parcialmente como se explicó en guías anteriores.\n",
    "\n",
    "Continuando con el ejemplo previo, es evidente que $\\frac{\\partial y_{ij}}{\\partial x_{mn}}$ es distinto de cero solo si $x_{mn}$ es el máximo elemento de la región de $y_{ij}$, de este modo sucedería que si $x_{12}$ es el máximo elemento de la región que abarca $y_{11}$, entonces:\n",
    "\n",
    "$$\\frac{\\partial y_{11}}{\\partial x_{12}}=\\frac{\\partial x_{12}}{\\partial x_{12}}=1$$\n",
    "\n",
    "Y el resto de derivadas con respecto a los otros $x_{mn}$ de la primera región serán cero. La derivada de $E$ con respecto a $x_{12}$ se formula multiplicando el gradiente local por los gradientes provenientes de la siguiente capa, quedando únicamente $\\frac{\\partial E}{\\partial x_{12}}=\\frac{\\partial E}{\\partial y_{11}} \\cdot 1 \\neq 0$ para la región de $y_{11}$.\n",
    "\n",
    "* El mismo razonamiento es válido para todas las  $(i,j)$  regiones.\n",
    "\n",
    "![maxpool.png](img/maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = np.random.rand(*y.shape)\n",
    "\n",
    "# Propaga el gradiente hacia atrás a través de la convolución\n",
    "layer_grad = layer.backward(g)\n",
    "layer_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobaciones con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x).to(torch.double)\n",
    "\n",
    "pool = tnn.MaxPool2d(kernel_size=kernel_size, stride=stride, return_indices=True)\n",
    "\n",
    "x.requires_grad = True\n",
    "y_torch, indices = pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(y_torch.detach().numpy(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.from_numpy(g).to(torch.double)\n",
    "\n",
    "# Propagar el gradiente hacia atrás a través de la capa de Max Pooling\n",
    "y_torch.backward(g)\n",
    "\n",
    "# Imprimir el gradiente de la imagen de entrada\n",
    "print(\"Gradiente de la entrada (dE/dx):\")\n",
    "print(x.grad, x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(x.grad.numpy(),layer_grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "din=3 # dimensión de entrada\n",
    "dout=5 # dimensión de salida\n",
    "input_shape=(batch_size,din,11,11)\n",
    "\n",
    "# Verificar las derivadas de un modelo de Flatten\n",
    "# con valores aleatorios de `w`, `b`, y `x`, la entrada\n",
    "layer=nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples,tolerance=1e-5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Forward\n",
    "\n",
    "La operación de _Average Pooling_ es similar a la de convolución, donde se calcula el promedio de los elementos que conforman a la región particular encerrada por el kernel.\n",
    "\n",
    "La explicación de su cálculo es sencilla a partir de un ejemplo, supongamos que $x \\in \\mathbb{R}^{(4\\times 4)}$, $F=2$ y $S=1$, de modo los $y_{ij}$ se definen como:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "y_{11} = \\text{avg} ( x_{11},x_{12},x_{21},x_{22} ) & \n",
    "y_{12} = \\text{avg} ( x_{12},x_{13},x_{22},x_{23} ) & \n",
    "y_{13} = \\text{avg} ( x_{13},x_{14},x_{23},x_{24} ) \\\\\n",
    "\n",
    "y_{21} = \\text{avg} ( x_{21},x_{22},x_{31},x_{32} ) & \n",
    "y_{22} = \\text{avg} ( x_{22},x_{23},x_{32},x_{33} ) & \n",
    "y_{23} = \\text{avg} ( x_{23},x_{24},x_{33},x_{34} ) \\\\\n",
    "\n",
    "y_{31} = \\text{avg} ( x_{31},x_{32},x_{41},x_{42} ) & \n",
    "y_{32} = \\text{avg} ( x_{32},x_{33},x_{42},x_{43} ) & \n",
    "y_{33} = \\text{avg} ( x_{33},x_{34},x_{43},x_{44} )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Esto nos permite ver que el shape de salida de una capa Avg Pool 2D está descripto por la misma ecuación que para el shape de salida de una capa convolucional.\n",
    "\n",
    "Implementa el método `forward` del modelo `AvgPool2d` en el archivo `edunn/models/pooling.py`.\n",
    "\n",
    "> TIP: puedes reutilizar la función de forward de la capa `MaxPool2d` y cambiar `np.max` por `np.average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "x = np.random.rand(2,3,7,7)\n",
    "# x = np.random.rand(1,1,4,4)\n",
    "\n",
    "kernel_size,stride=2,1\n",
    "\n",
    "layer=nn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "y=layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dE/dx`\n",
    "\n",
    "El cálculo de los gradientes del error $E$ con respecto al feature map de entrada $x$ se puede hacer derivando parcialmente como se explicó en guías anteriores.\n",
    "\n",
    "<!-- Al tener que realizar el cálculo de la derivada de una matriz con respecto a otra, se considera cada elemento de la matriz como una variable independiente. Por lo tanto, la derivada de un elemento de la matriz $y$ con respecto a un elemento de la matriz $x$ considerará todas las posibles rutas por las que ese elemento de $x$ puede influir en el elemento de $y$.\n",
    "\n",
    "De este modo, $\\frac{\\partial y}{\\partial x_{mn}} = \\sum_{ij} \\frac{\\partial y_{ij}}{\\partial x_{mn}}$, por la regla de la cadena para derivadas parciales, que nos indica que la derivada de una función con respecto a una variable es la suma de las derivadas de esa función a través de todas las rutas posibles (pudiendo contener términos que sean cero debido a la no dependencia de determinados $y_{ij}$ con $x_{mn}$). -->\n",
    "\n",
    "Continuando con el ejemplo previo, es evidente que $\\frac{\\partial y_{ij}}{\\partial x_{mn}}$ es distinto de cero solo si $x_{mn}$ pertenece a la región de $y_{ij}$, donde además será igual a $\\frac{1}{F_H \\cdot F_W}$, entonces para calcular por ejemplo $\\frac{\\partial y}{\\partial x_{12}}$:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_{12}} = \\frac{\\partial y_{11}}{\\partial x_{12}} + \\frac{\\partial y_{12}}{\\partial x_{12}} + \\frac{\\partial y_{21}}{\\partial x_{12}} + \\frac{\\partial y_{22}}{\\partial x_{12}} = \\frac{1}{4} + \\frac{1}{4} + 0 + 0$$\n",
    "\n",
    "ya que:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_{11} &= \\frac { x_{11}+\\color{red}{x_{12}}\\color{default}+x_{21}+x_{22} }{F_H \\cdot F_W} \\Rightarrow \n",
    "    \\frac{\\partial y_{11}}{\\partial x_{12}} = \\frac{1}{4} \\frac{\\partial x_{12}}{\\partial x_{12}} = \\frac{1}{4} \\\\\n",
    "y_{12} &= \\frac { \\color{red}{x_{12}}\\color{default}+x_{13}+x_{22}+x_{23} }{F_H \\cdot F_W} \\Rightarrow\n",
    "    \\frac{\\partial y_{12}}{\\partial x_{12}} = \\frac{1}{4} \\frac{\\partial x_{12}}{\\partial x_{12}} = \\frac{1}{4} \\\\\n",
    "y_{21} &= \\frac { x_{21}+x_{22}+x_{31}+x_{32} }{F_H \\cdot F_W} \\Rightarrow \n",
    "    \\frac{\\partial y_{21}}{\\partial x_{12}} = 0 \\\\\n",
    "y_{22} &= \\frac { x_{22}+x_{23}+x_{32}+x_{33} }{F_H \\cdot F_W} \\Rightarrow \n",
    "    \\frac{\\partial y_{22}}{\\partial x_{12}} = 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "De este modo, el resto de derivadas con respecto a los otros $x_{mn}$ de la primera región serán **en un comienzo** iguales a $\\frac{1}{F_H \\cdot F_W}$ y según su influencia en la salida debido al valor de $S$ se irán incrementando en esa misma cantidad. \n",
    "\n",
    "Esto último mencionado se puede visualizar mejor si $S=2$, donde así es evidente que para la primera región, el resultado parcial de $\\frac{\\partial y}{\\partial x_{mn}}$ para $m,n \\in \\{1,2\\}$ ante la no dependencia entre regiones dadas por el kernel es:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1/4 & 1/4 & 0 & 0 \\\\\n",
    "1/4 & 1/4 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* El mismo razonamiento es válido para todas las  $(i,j)$  regiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = np.random.rand(*y.shape)\n",
    "# g = np.ones_like(y)\n",
    "\n",
    "# Propaga el gradiente hacia atrás a través de la convolución\n",
    "layer_grad = layer.backward(g)\n",
    "layer_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobaciones con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x).to(torch.double)\n",
    "\n",
    "pool = tnn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "x.requires_grad = True\n",
    "y_torch = pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(y_torch.detach().numpy(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.from_numpy(g).to(torch.double)\n",
    "\n",
    "# Propagar el gradiente hacia atrás a través de la capa de Max Pooling\n",
    "y_torch.backward(g)\n",
    "\n",
    "# Imprimir el gradiente de la imagen de entrada\n",
    "print(\"Gradiente de la entrada (dE/dx):\")\n",
    "print(x.grad, x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(x.grad.numpy(),layer_grad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "din=3 # dimensión de entrada\n",
    "dout=5 # dimensión de salida\n",
    "input_shape=(batch_size,din,11,11)\n",
    "\n",
    "# Verificar las derivadas de un modelo de Flatten\n",
    "# con valores aleatorios de `w`, `b`, y `x`, la entrada\n",
    "layer=nn.AvgPool2d(kernel_size=3, stride=3)\n",
    "\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples,tolerance=1e-5)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
