{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplenn import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa MultiplyConstant\n",
    "\n",
    "En este ejercicio debés implementar la capa `MultiplyConstant`,  que multiplica a cada una de sus entradas por un valor constante para generar su salida. Funciona de forma similar a `AddConstant`, pero en este caso multiplica en lugar de sumar, y por ende sus derivadas son ligeramente más complicadas.\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[3.5,-7.2,5.3]` y la capa `MultiplyConstant` se crea con la constante `2`, `y` será `[7.0,-14.4,10.6]`.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "El método `forward` calcula la salida `y` en base a la entrada `x`, como explicamos antes. En términos formales,  si la constante a sumar es $C$ y la entrada a la capa es $x = [x_1,x_2,...,x_n] $, entonces la salida $y$ es:\n",
    "\n",
    "$\n",
    "y([x_1,x2,...,x_n])= [x_1*C,x_2*C,...,x_n*C]\n",
    "$\n",
    "\n",
    "Comenzamos con el método `forward` de la clase `MultiplyConstant`, que podrás encontrar en el archivo `activations.py` de la carpeta `simplenn/models`. Debés completar el código entre los comentarios:\n",
    "\n",
    "````### COMPLETAR INICIO ###````\n",
    "\n",
    "y\n",
    "\n",
    "````### COMPLETAR FIN ###````\n",
    "\n",
    "Y luego verificar con la siguiente celda una capa que multiplica por 2 y otra que multiplica por -2. Si ambos chequeos son correctos, verás dos mensajes de <span style='background-color:green;color:white; '>éxito (success)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3.5,-7.2,5.3],\n",
    "             [-3.5,7.2,-5.3]])\n",
    "\n",
    "layer=sn.MultiplyConstant(2)\n",
    "y = np.array([[  7.,  -14.4,  10.6,],\n",
    "              [ -7.,   14.4, -10.6]])\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "layer=sn.MultiplyConstant(-2)\n",
    "y = -np.array([[  7.,  -14.4,  10.6,],\n",
    "              [ -7.,   14.4, -10.6]])\n",
    "utils.check_same(y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Además del cálculo de la salida de la capa, la misma debe poder propagar hacia atrás el gradiente del error de la red. Para eso, debés implementar el método `backward` que recibe $\\frac{δE}{δy}$, es decir, las derivadas parciales del error respecto a la salida (gradiente) de esta capa , y devolver $\\frac{δE}{δx}$, las derivadas parciales del error respecto de las entradas de esta capa. \n",
    "\n",
    "Para la capa `AddConstant` el cálculo del gradiente es fácil, ya que como:\n",
    "\n",
    "$\n",
    "y(x_1,x_2,...,x_n)= (x_1*C,x_2*C,...,x_n*C)\n",
    "$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$y_i(x)=x_i*C$\n",
    "\n",
    "Y entonces\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i}$\n",
    "\n",
    "Como $y_i$ solo depende de $x_i$, podemos reescribir lo anterior como\n",
    "\n",
    "$ \\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\frac{δE}{δy_i} * \\frac{δy_i}{δx_i} $\n",
    "\n",
    "Dado que $y_i(x)=x_i*C$, entonces $\\frac{δy_i}{δx_i} = C$ y por ende:\n",
    "$ \\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\frac{δE}{δy_i} * C $\n",
    "\n",
    "Escribiendo entonces en forma vectorial para el vector x:\n",
    "\n",
    "$ \\frac{δE}{δx} = [ \\frac{δE}{δy_1} *C, \\frac{δE}{δy_2} *C, ..., \\frac{δE}{δy_n}*C ] = \\frac{δE}{δy}*C $\n",
    "\n",
    "Con lo cual la capa simplemente propaga los gradientes de la capa siguiente, pero multiplicados por C.\n",
    "\n",
    "Completar el código en la función `backward` de la capa `MultiplyConstant` y verificar con la celda de abajo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[104m\u001b[30mMultiplyConstant_2 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1000 partial derivatives checked (100 random input samples)\n",
      "\u001b[104m\u001b[30mMultiplyConstant_3 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1000 partial derivatives checked (100 random input samples)\n"
     ]
    }
   ],
   "source": [
    "from simplenn.utils import check_gradient\n",
    "\n",
    "\n",
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape=(5,2)\n",
    "\n",
    "layer=sn.MultiplyConstant(3)\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "layer=sn.MultiplyConstant(-4)\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
