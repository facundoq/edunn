{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Error Linear Unit\n",
    "\n",
    "GELU es una función de activación que se utiliza en las capas ocultas de las redes neuronales. Se utiliza para introducir la no linealidad en los modelos de redes neuronales y ayuda a decidir qué neuronas deben activarse. \n",
    "\n",
    "GELU combina las ventajas de otras funciones de activación populares como ReLU y ELU. Proporciona una saturación suave tanto para los valores positivos como para los negativos, lo que puede ayudar a mitigar el problema del desvanecimiento del gradiente.\n",
    "\n",
    "## Método Forward\n",
    "\n",
    "Existen distintas formas de expresar tal función, una de ellas es utilizando la función de distribución acumulativa (CDF) y la [función de error](https://en.wikipedia.org/wiki/Error_function):\n",
    "\n",
    "$$ \\text{GELU} = x \\cdot \\text{CDF}(x) = x \\cdot \\frac{1+\\text{erf}\\big(\\frac{1}{\\sqrt{2}}\\big)}{2} $$\n",
    "\n",
    "Esto se debe a que la `CDF` puede ser expresada utilizando la `erf`, donde se escala la función lineal `x` con la `CDF`. Esta versión de GELU es computacionalmente más eficiente y es la que se utiliza en la biblioteca de aprendizaje profundo de PyTorch.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/gelu.png\" width=\"25%\" style=\"display:inline-block\">\n",
    "</center>\n",
    "\n",
    "> NOTA: actualmente la función `erf` no es encuentra implementada en `numpy`, puedes tomarte el tiempo de implementarla numéricamente o utilizar la implementación dada por `from scipy.special import erf`.\n",
    "\n",
    "## Método Backward\n",
    "\n",
    "Su cálculo es más sencillo de lo que parece, ya que en el mismo no utilizaremos la expresión dependiente de la `erf`, sino la que depende expresamente de `CDF`.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\text{x} \\cdot \\text{CDF}(\\text{x}))' &= \\text{x}' \\cdot \\text{CDF}(\\text{x}) + \\text{x} \\cdot \\text{CDF}'(\\text{x}) \\\\\n",
    "&= \\text{CDF}(\\text{x}) + \\text{x} \\cdot \\text{PDF}(\\text{x})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Esto se debe al teorema fundamental del cálculo, donde por definición: \n",
    "\n",
    "- Derivar la CDF te da la PDF: $\\text{PDF}(x) = \\frac{d}{dx} \\text{CDF}(x)$\n",
    "- Integrar la PDF te da la CDF: $\\text{CDF}(x) = \\int_{-\\infty}^{x} \\text{PDF}(t) dt$\n",
    "\n",
    "La `PDF` que utilizaremos es la de la distribución normal o Gaussiana. Nuevamente, `numpy` no provee su cálculo en relación a los valores de un vector de entrada, por ello deberás implentar la función `normal_pdf` en el archivo `activations.py`.\n",
    "\n",
    "Esta es la fórmula para la función de densidad de probabilidad (PDF) de una distribución normal:\n",
    "\n",
    "$$\n",
    "f(x, \\mu=0, \\sigma=1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "donde $x$ es la variable aleatoria, $\\mu$ es la media de la distribución y $\\sigma$ es la desviación estándar de la distribución. \n",
    "\n",
    "<!-- La PDF (Función de Densidad de Probabilidad) da la probabilidad de que una variable aleatoria tome un valor específico, mientras que la CDF (Función de Distribución Acumulativa) da la probabilidad de que una variable aleatoria tome un valor menor o igual a un valor dado. -->\n",
    "\n",
    "<!-- 1. **PDF (Función de Densidad de Probabilidad)**: Es una función que describe la probabilidad relativa de que una variable aleatoria tome un valor dado. La probabilidad de que la variable aleatoria caiga dentro de un rango particular se dada por el área bajo la gráfica de la función de densidad en ese intervalo.\n",
    "\n",
    "2. **CDF (Función de Distribución Acumulativa)**: Es una función que indica la probabilidad de que una variable aleatoria sea menor o igual a un valor dado. Es la integral de la PDF hasta ese valor.\n",
    "\n",
    "- La PDF puede tomar valores mayores a 1, mientras que la CDF nunca puede tomar valores mayores a 1. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "din=10\n",
    "batch_size=2\n",
    "\n",
    "x = np.random.rand(batch_size,din)\n",
    "\n",
    "layer=nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot values\n",
    "nn.plot.plot_activation_function(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = np.random.rand(*y.shape)\n",
    "\n",
    "# Propaga el gradiente hacia atrás a través de la convolución\n",
    "layer_grad = layer.backward(g)\n",
    "layer_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "\n",
    "x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "# Definimos la capa GELU\n",
    "gelu = tnn.GELU()\n",
    "\n",
    "x.requires_grad = True\n",
    "\n",
    "y_torch = gelu(x)\n",
    "y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(y_torch.detach().numpy(),y,tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = torch.from_numpy(g).to(torch.double)\n",
    "\n",
    "# Propaga el gradiente hacia atrás\n",
    "x.grad = None  # Limpiamos los gradientes existentes\n",
    "y_torch.backward(g)\n",
    "\n",
    "# Imprime el gradiente de la imagen de entrada\n",
    "print(\"Gradiente de la entrada (δE/δx):\")\n",
    "print(x.grad, x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(x.grad.numpy(),layer_grad[0],tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Cantidad de ejemplos aleatorios y tamaño de los mismo gpara generar \n",
    "# muestras de x y verificar las derivadas\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Verificar derivadas de una función GELU\n",
    "layer=nn.GELU()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "nn.plot.plot_activation_function(nn.GELU(),backward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('captum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "27cd8888c451505594cd6ce93183113956b3735aa05f73d7c3cf078349bc9fda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
