{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de activación\n",
    "\n",
    "Una `red neuronal` es un modelo que combina  transformaciones de varias capas como las de Regresión Lineal o Regresión Logística. Pero para definir una red, debemos utilizar _funciones de activación_, que permiten hacer transformaciones no lineales de los datos. De otra forma, la combinación de dos capas de Regresión Lineal, por ejemplo, es equivalente a tener una sola, ya que solo va a poder representar una recta. En ese caso, nuestra red no va a ser más potente, y solo será más ineficiente.\n",
    "\n",
    "Hay distintos tipos de funciones de activación: vamos a ver tres de las más comunes, `relu`, `sigmoid` (o `logística`) y `tanh` (tangente hiperbólica). Cada una tiene distintas propiedades:\n",
    "\n",
    "* `relu`: es eficiente para calcular tanto su salida como su derivada, y aunque su no-linealidad es simple sirve para darle mayor poder de aproximación a una red. Por otro lado, su salida está en el rango $[0,\\infty)$, con lo cual no sirve para algunas tareas\n",
    "* `sigmoid`: es eficiente, aunque no tanto como `relu`, y como su rango es $(0,1)$ permite codificar un valor que puede interpretarse como una probabilidad\n",
    "* `tanh`: Es similar a `sigmoid`, pero su rango es $(-1,1)$, con lo cual permite codificar valor como positivos y negativos y así generar representaciones con feedback de esos signos en la red.\n",
    "\n",
    "<img src=\"img/relu.png\" width=\"25%\" style=\"display:inline-block\"> <img src=\"img/sigmoid.png\" width=\"25%\" style=\"display:inline-block\"> <img src=\"img/tanh.png\" width=\"25%\" style=\"display:inline-block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa ReLU\n",
    "\n",
    "La función `ReLU` (Rectified Linear Unit) es extremadamente simple.\n",
    "\n",
    "<img src=\"img/relu.png\" width=50%>\n",
    "\n",
    "Formalmente,\n",
    "\n",
    "$$ReLU(x) = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "    1 & \\text{if}  x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "De forma más corta pero un poco más difícil de entender, podemos escribir `ReLU` como:\n",
    "$$ReLU(x) = max(0,x)$$\n",
    "\n",
    "En términos de código, calcular `relu` para un solo valor es simple:\n",
    "\n",
    "````python\n",
    "def relu(x:float):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "````\n",
    "\n",
    "No obstante, en este caso tené en cuenta que recibirás un tensor de valores. De todas formas `ReLU`, como las otras funciones de activación, es fácil de cálcular ya que se aplica _elemento a elemento_. Es decir, `ReLU` de un vector es equivalente a aplicar `ReLU` a cada uno de sus valores, o sea:\n",
    "\n",
    "$$ReLU( (-2,4,7)=( ReLU(-2), ReLU(4), ReLU(7)) = (0,4,7)$$\n",
    "\n",
    "Para el caso de una matriz o un tensor en general, se procede de igual forma.\n",
    "\n",
    "Implementá el método `forward` de la clase `ReLU` en el archivo `simplenn/models/activations.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5,-7.2,5.3],\n",
    "             [-3.5,7.2,-5.3]])\n",
    "\n",
    "layer=sn.ReLU()\n",
    "y = np.array([[3.5,0,5.3],\n",
    "             [0,7.2,0]])\n",
    "sn.utils.check_same(y,layer.forward(x))\n",
    "\n",
    "# plot values\n",
    "sn.plot.plot_activation_function(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward de `ReLU`\n",
    "\n",
    "Afortunadamente, el método `backward` de `ReLU` es sencillo. Procediendo por casos, podemos ver que si el forward es :\n",
    "\n",
    "$$ReLU(x) = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "    x & \\text{if}  x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Podemos derivar cada caso por separado, y entonces:\n",
    "$$\\frac{d ReLU(x)}{dx} = \\begin{cases}\n",
    "    0 & \\text{if } x \\le 0 \\\\ \n",
    "    1 & \\text{if}  x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "En el caso de $x=0$, en verdad la derivada de $ReLU$ no está definida; no obstante, en este caso podemos redefinir dicha derivada como 0 (o 1), y no afectará realmente a la optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplenn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Cantidad de ejemplos aleatorios y tamaño de los mismo gpara generar \n",
    "# muestras de x y verificar las derivadas\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Verificar derivadas de una función ReLU\n",
    "layer=sn.ReLU()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "sn.plot.plot_activation_function(sn.ReLU(),backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Sigmoid\n",
    "\n",
    "La función `sigmoid` (sigmoidea o también conocida como logística) convierte cualquier valor al intervalo $(0,1)$. \n",
    "\n",
    "\n",
    "<img src=\"img/sigmoid.png\" width=50%>\n",
    "\n",
    "Su definición es:\n",
    "\n",
    "$$\n",
    "Sigmoid(x)= \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "De esta forma, por ejemplo, tenemos:\n",
    "\n",
    "$$ sigmoid(0)=\\frac{1}{1+1}=\\frac{1}{2}=0.5$$\n",
    "$$ sigmoid(1)=\\frac{1}{1+e^{-1}}=\\frac{1}{1+0.36}=0.73$$\n",
    "$$ sigmoid(-1)=\\frac{1}{1+e^{-(-1)}}=\\frac{1}{1+2.71}=0.26$$\n",
    "\n",
    "Como vemos entonces, el balance de la función está en el valor $0$, para el cual la salida es $0.5$; valores mayores a $0$ causan salidas mayores, y viceversa.\n",
    "\n",
    "\n",
    "Implementá el método `forward` de la clase `Sigmoid` en el archivo `simplenn/models/activations.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,-1]])\n",
    "\n",
    "layer=sn.Sigmoid()\n",
    "y = np.array([[0.5,0.73105858,0.26894142]])\n",
    "sn.utils.check_same(y,layer.forward(x),tol=1e-6)\n",
    "sn.plot.plot_activation_function(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward de `Sigmoid`\n",
    "\n",
    "Las derivadas del método `backward` de `Sigmoid` son sencillas, pero además vamos a querer llevarlas a una forma simplificada que hace más eficiente su cálculo.\n",
    "\n",
    "$$\n",
    "Sigmoid(x)= \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d Sigmoid(x)}{dx} \n",
    "&= \\frac{d \\frac{1}{1+e^{-x}}}{dx} =\\frac{d (1+e^{-x})^{-1}}{dx} \\\\\n",
    "&= \\frac{d (1+e^{-x})^{-1}}{d(1+e^{-x})} \\frac{d (1+e^{-x})}{dx} & \\text{(regla de la cadena con $g(x)=1+e^{-x}$)} \\\\\n",
    "&= \\frac{d  (1+e^{-x})^{-1} }{d(1+e^{-x})} (-e^{-x}) & \\text{(derivada de $1+e^{-x}$)} \\\\\n",
    "&=  -(1+e^{-x})^{-2} (-e^{-x}) & \\text{(derivada de $g(x)^{-1}=-g(x)^{-2}$)} \\\\\n",
    "&=  (1+e^{-x})^{-2} e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 e^{-x}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "En este punto, tenemos una fórmula para la derivada de $Sigmoid$, pero como decíamos antes, vamos a llevarla a una fórmula más simple que solo utilice el valor original de $Sigmoid$. De esta forma, nos evitaremos volver a realizar operaciones de exponenciación o división que son más costosas computacionalmente. Para eso, comenzamos con la última línea de la derivación anterior:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d Sigmoid(x)}{dx} &=  (1+e^{-x})^{-2} e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 e^{-x}\\\\\n",
    "&=  Sigmoid(x)^2 (1-1+e^{-x})\\\\\n",
    "&=  Sigmoid(x)^2 (1-Sigmoid(x)^{-1})\\\\\n",
    "&=  Sigmoid(x) Sigmoid(x) (1-Sigmoid(x)^{-1})\\\\\n",
    "&=  Sigmoid(x) [Sigmoid(x) (1-Sigmoid(x)^{-1})]\\\\\n",
    "&=  Sigmoid(x) [Sigmoid(x) * 1- Sigmoid(x) Sigmoid(x)^{-1}]\\\\\n",
    "&=  Sigmoid(x) (Sigmoid(x) -1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Como podemos ver, esta fórmula final $Sigmoid'(x) = Sigmoid(x)  (Sigmoid(x)-1)$ nos dice que si almacenamos el valor de $Sigmoid(x)$ en el paso `forward`, entonces para el `backward` la derivada se calcula simplemente como $Sigmoid(x) (Sigmoid(x)-1)$ que solo requiere una suma y una multiplicación de vectores.\n",
    "\n",
    "Implementá el método `backward` de la clase `Sigmoid` y verificalo con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplenn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Cantidad de ejemplos aleatorios y tamaño de los mismo gpara generar \n",
    "# muestras de x y verificar las derivadas\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Verificary derivadas de una función Sigmoid\n",
    "layer=sn.Sigmoid()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "sn.plot.plot_activation_function(layer,backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa `TanH`\n",
    "\n",
    "La función `tanh` (tangente hiperbólica) convierte cualquier valor al intervalo $(-1,1)$. \n",
    "\n",
    "<img src=\"img/tanh.png\" width=35%>\n",
    "\n",
    "$tanh$ es una función geométrica [hiperbólica](https://en.wikipedia.org/wiki/Hyperbolic_functions). Al igual que $tan(x)=\\frac{cos(x)}{sen(x)}$, $tanh$ se define a partir del coseno y seno hiperbólicos:\n",
    "\n",
    "$$\n",
    "tanh(x)= \\frac{cosh(x)}{senh(x)} = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}[t]\n",
    "tanh(0)  &= \\frac{1-1}{1+1} &&= \\frac{0}{2} &&= 0 \\\\\n",
    "tanh(1)  &= \\frac{e-e^{-1}}{e+e^{-1}} &&= \\frac{2.35}{3.08} &&= 0.76 \\\\\n",
    "tanh(1) &= \\frac{e^{-1}-e}{e^{-1}+e} &&= \\frac{-2.35}{3.08} &&= -0.76\n",
    "\\end{aligned} \n",
    " $$\n",
    "\n",
    "El balance de la función está en el valor $0$, para el cual la salida es $0$; valores mayores a $0$ causan salidas mayores, y viceversa, por eso $tanh$ es una función impar ($tanh(x)=-tanh(-x)$).\n",
    "\n",
    "$TanH$ es muy similar a la función $Sigmoid$. De hecho, con el siguiente gráfico podemos observar que $TanH$ es simplemente $Sigmoid$, pero:\n",
    "\n",
    "* Multiplicada por dos (para convertir el rango $(0,1)$ al rango $(0,2)$)\n",
    "* Restándole 1 (para convertir el rango $(0,2)$ al rango $(-1,1)$)\n",
    "* Multiplicando a $x$ por 2, para que la curva de ambas sea igual\n",
    "\n",
    "<img src=\"img/sigmoid.png\" width=35%> \n",
    "\n",
    "En verdad, [se puede definir](http://facundoq.github.io/guides/sigmoid_tanh) a $tanh$ en base a $Sigmoid$:\n",
    "$$\n",
    "tanh(x) = sigmoid(2x)*2-1\n",
    "$$\n",
    "\n",
    "\n",
    "Esta forma será mucho más cómoda para la implementación, ya que podemos reutilizar la capa `Sigmoid` tanto para el forward como el backward.\n",
    "\n",
    "Implementá el método `forward` de la clase `TanH` en el archivo `simplenn/models/activations.py`  _utilizando la capa `Sigmoid`_ (te ayudamos ya definiendo una variable para ello). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0.5,-0.5]])\n",
    "\n",
    "layer=sn.TanH()\n",
    "y = np.array([[ 0., 0.46211716, -0.46211716]])\n",
    "sn.utils.check_same(y,layer.forward(x),tol=1e-6)\n",
    "\n",
    "sn.plot.plot_activation_function(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward de `TanH`\n",
    "\n",
    "Las derivadas del método `backward` de `TanH` pueden obtenerse en base a las de `Sigmoid`. Dado que\n",
    "\n",
    "$$\n",
    "tanh(x) = sigmoid(2x)*2-1\n",
    "$$\n",
    "\n",
    "Entonces:\n",
    "\n",
    "$$\n",
    "tanh'(x) = (sigmoid'(2x)*2)*2 = sigmoid'(2x)*4\n",
    "$$\n",
    "\n",
    "Es decir, la derivada de $tanh$ consta simplemente de multiplicar por dos la derivada de $sigmoid$.\n",
    "\n",
    "Implementá el método `backward` de la clase `TanH` y verificalo con el siguiente código:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplenn.utils import check_gradient\n",
    "\n",
    "\n",
    "# Cantidad de ejemplos aleatorios y tamaño de los mismo gpara generar \n",
    "# muestras de x y verificar las derivadas\n",
    "samples = 100\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Verificary derivadas de una función Sigmoid\n",
    "layer=sn.TanH()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "sn.plot.plot_activation_function(layer,backward=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
