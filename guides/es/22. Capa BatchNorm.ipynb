{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Batch Normalization\n",
    "\n",
    "La normalización por lotes aborda el problema de la inicialización deficiente de las redes neuronales. Se puede interpretar como hacer un preprocesamiento en cada capa de la red. Obliga a las activaciones en una red a adoptar una distribución gaussiana unitaria al comienzo del entrenamiento. Esto asegura que todas las neuronas tengan aproximadamente la misma distribución de salida en la red y mejora la tasa de convergencia.\n",
    "\n",
    "Explicar el por qué la distribución de las activaciones en una red importa excede el propósito de estas guías, pero de ser de tu interés podés referirte a las páginas 46 — 62 en las [diapositivas de la conferencia](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture07.pdf) ofrecidas por el curso de la universidad de Stanford."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Forward\n",
    "\n",
    "Digamos que tenemos un lote de activaciones $x$ en una capa, la versión de $x$ con media cero y varianza unitaria $\\hat{x}$ es:\n",
    "\n",
    "$$\\hat{x}^{(k)}=\\frac{x^{(k)}-\\mathbb{E}[x^{(k)}]}{\\sqrt{\\text{Var}[x^{(k)}]}}$$\n",
    "\n",
    "Esta es en realidad una operación diferenciable, por eso podemos aplicar la normalización por lotes en el entrenamiento.\n",
    "\n",
    "El cálculo de ésta se resume en computar la media $\\mu_\\mathcal{B}$ y varianza $\\sigma_\\mathcal{B}^2$ de un mini-batch de $\\mathcal{B}=\\{x_1, \\dots, x_N\\}$. Los parámetros aprendibles de la capa son $\\gamma$ y $\\beta$ que son utilizados para escalar y desplazar los valores normalizados.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_\\mathcal{B} &= \\frac{1}{N} \\sum_{i=1}^{N} x_i & \\text{(mini-batch mean)} \\\\\n",
    "\\sigma_\\mathcal{B}^2 &= \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu_\\mathcal{B})^2 & \\text{(mini-batch variance)} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i - \\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2+\\epsilon}} & \\text{(normalize)} \\\\\n",
    "\\text{\\textbf{BN}}_{\\gamma,\\beta}(x_i) &\\stackrel{\\text{def}}{=} \\gamma \\hat{x}_i + \\beta = y_i & \\text{(scale and shift)} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> NOTA: En la implementación, insertamos la capa `BatchNorm` justo después de una capa `Dense` o una capa `Conv2d`, y antes de las capas no lineales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "din=10\n",
    "batch_size=2\n",
    "\n",
    "x = np.random.rand(batch_size,din)\n",
    "w = np.random.rand(din)\n",
    "b = np.random.rand(din)\n",
    "                       \n",
    "gamma_initializer = nn.initializers.Constant(w)\n",
    "beta_initializer = nn.initializers.Constant(b)\n",
    "\n",
    "layer=nn.BatchNorm(num_features=din, gamma_initializer=gamma_initializer, beta_initializer=beta_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Backward\n",
    "\n",
    "### `dEdβ`\n",
    "\n",
    "El cálculo de los gradientes del error $E$ con respecto al parámetro $\\beta$ se puede hacer derivando parcialmente como se explicó en guías anteriores:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\beta} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\beta}\n",
    "$$\n",
    "\n",
    "Como $y$ es un vector de $N$ elementos, tenemos que sumar por todos sus valores para aplicar la regla de la cadena:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\beta} = \\frac{\\partial E}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial \\beta} + \\cdots + \\frac{\\partial E}{\\partial y_N} \\cdot \\frac{\\partial y_N}{\\partial \\beta} \n",
    "\\qquad \\text{donde} \\qquad\n",
    "\\frac{\\partial y_i}{\\partial \\beta} = \\frac{\\partial (\\gamma \\hat{x}_i + \\beta)}{\\partial \\beta} = 1\n",
    "$$\n",
    "\n",
    "de este modo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\beta} = \\sum\\limits_{i=1}^N \\frac{\\partial E}{\\partial y_i} \\cdot 1\n",
    "$$\n",
    "\n",
    "### `dEdγ`\n",
    "\n",
    "El cálculo de los gradientes del error $E$ con respecto al parámetro $\\gamma$ se puede hacer derivando parcialmente como se explicó en guías anteriores:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\gamma} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\gamma}\n",
    "$$\n",
    "\n",
    "Como $y$ es un vector de $N$ elementos, tenemos que sumar por todos sus valores para aplicar la regla de la cadena:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\gamma} = \\frac{\\partial E}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial \\gamma} + \\cdots + \\frac{\\partial E}{\\partial y_N} \\cdot \\frac{\\partial y_N}{\\partial \\gamma} \n",
    "\\qquad \\text{donde} \\qquad\n",
    "\\frac{\\partial y_i}{\\partial \\gamma} = \\frac{\\partial (\\gamma \\hat{x}_i + \\beta)}{\\partial \\gamma} = \\hat{x}_i\n",
    "$$\n",
    "\n",
    "de este modo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\gamma} = \\sum\\limits_{i=1}^N \\frac{\\partial E}{\\partial y_i} \\cdot \\hat{x}_i\n",
    "$$\n",
    "\n",
    "### `dEdx`\n",
    "\n",
    "<!-- Utilizando la regla de la cadena para el cálculo diferencial, esta nos dice que la derivada de una función compuesta es el producto de las derivadas de las funciones que la componen. -->\n",
    "\n",
    "Teniendo en cuenta de qué depende cada función:\n",
    "\n",
    "<center>\n",
    "\n",
    "||||\n",
    "|:-:|:-:|:-:|\n",
    "|$E(y)$|$y(\\hat{x},\\gamma,\\beta)$|$\\hat{x}(\\mu,\\sigma^2,x)$|\n",
    "\n",
    "</center>\n",
    "\n",
    "Obtenemos que:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E}{\\partial x_i} = \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial E}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial E}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "En las siguientes subsecciones calcularemos la expresión correspondiente para el gradiente de cada componente.\n",
    "\n",
    "#### `dEdx̂`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\hat{x}_i} = \\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial \\hat{x}_i} = \\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial (\\gamma \\hat{x}_i + \\beta)}{\\partial \\hat{x}_i} = \\frac{\\partial E}{\\partial y_i} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "#### `dEdμ`\n",
    "\n",
    "Notar que $\\sigma^2$ se puede escribir en función de $\\mu$, es por ello que $E$ depende de $\\mu$ a traves de dos variables: $\\hat{x}_i$​ y $\\sigma^2$.\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial E}{\\partial \\mu} = \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{\\partial \\hat{x}_i}{\\partial \\mu} + \\frac{\\partial E}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial\\mu}\n",
    "$$\n",
    "\n",
    "Calculando las derivadas parciales para `dx̂dμ` y `dσ²dμ`:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{x}_i = \\frac{(x_i - \\mu)}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "&\\qquad \\Rightarrow \\qquad\n",
    "\\dfrac{\\partial \\hat{x}_i}{\\partial \\mu} = \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "\\sigma^2 = \\frac{1}{N} \\sum\\limits_{i=1}^N (x_i - \\mu)^2\n",
    "&\\qquad \\Rightarrow \\qquad\n",
    "\\dfrac{\\partial \\sigma^2}{\\partial \\mu} = \\frac{1}{N} \\sum\\limits_{i=1}^N -2 \\cdot (x_i - \\mu) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Reemplazando éstas últimas y dejando como variables a los gradientes del error $E$, obtenemos:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial \\mu} &= \\bigg(\\sum\\limits_{i=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\bigg) + \\bigg( \\frac{\\partial E}{\\partial \\sigma^2} \\cdot \\frac{1}{N} \\sum\\limits_{i=1}^N -2(x_i - \\mu)   \\bigg) \\qquad \\\\\n",
    "&= \\bigg(\\sum\\limits_{i=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\bigg) + \\bigg( \\frac{\\partial E}{\\partial \\sigma^2} \\cdot (-2) \\cdot \\bigg( \\frac{1}{N} \\sum\\limits_{i=1}^N x_i - \\frac{1}{N} \\sum\\limits_{i=1}^N \\mu   \\bigg) \\bigg) \\qquad \\\\\n",
    "&= \\bigg(\\sum\\limits_{i=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\bigg) + \\bigg( \\frac{\\partial E}{\\partial \\sigma^2} \\cdot (-2) \\cdot \\underbrace{\\bigg( \\mu - \\frac{N \\cdot \\mu}{N} \\bigg)}_{0} \\bigg) \\qquad \\\\\n",
    "&= \\sum\\limits_{i=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}} \\qquad \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### `dEdσ²`\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\sigma^2} = \\frac{\\partial E}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x}}{\\partial \\sigma^2}$$\n",
    "\n",
    "Reescribiendo $\\hat{x}_i$ para que su derivada sea más fácil de calcular, vemos que $(x_i - \\mu)$ pasa a ser un factor constante, de modo que:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = (x_i - \\mu)(\\sigma^2 + \\epsilon)^{-0.5}\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "\\dfrac{\\partial \\hat{x}}{\\partial \\sigma^2} = -0.5 \\sum\\limits_{i=1}^N (x_i - \\mu) \\cdot (\\sigma^2 + \\epsilon)^{-1.5}\n",
    "$$\n",
    "\n",
    "#### `dEdx` (cont.)\n",
    "\n",
    "Calculando las derivadas parciales restantes (`dx̂dx`, `dμdx` y `dσ²dx`) de la expresión original obtenemos que:\n",
    "\n",
    "<center>\n",
    "\n",
    "||||\n",
    "|:-:|:-:|:-:|\n",
    "|$\\dfrac{\\partial \\hat{x}_i}{\\partial x_i} = \\dfrac{1}{\\sqrt{\\sigma^2 + \\epsilon}}$|$\\dfrac{\\partial \\mu}{\\partial x_i} = \\dfrac{1}{N}$|$\\dfrac{\\partial \\sigma^2}{\\partial x_i} = \\dfrac{2(x_i - \\mu)}{N}$|\n",
    "\n",
    "</center>\n",
    "\n",
    "Finalmente podemos calcular el gradiente del error $E$ con respecto a $x$ utilizando el siguiente truco:\n",
    "\n",
    "$$\n",
    "(\\sigma^2 + \\epsilon)^{-1.5} = (\\sigma^2 + \\epsilon)^{-0.5}(\\sigma^2 + \\epsilon)^{-1} = (\\sigma^2 + \\epsilon)^{-0.5} \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "de este modo:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial x_i} &= \\bigg(\\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\dfrac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\quad\\; \\bigg) + \\bigg(\\frac{\\partial E}{\\partial \\mu} \\cdot \\dfrac{1}{N} \\qquad\\qquad\\qquad\\!\\! \\bigg) + \\bigg(\\frac{\\partial E}{\\partial \\sigma^2} \\cdot \\dfrac{2(x_i - \\mu)}{N}\\bigg) \\qquad \\\\\n",
    "&= \\bigg(\\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot \\dfrac{1}{\\sqrt{\\sigma^2 + \\epsilon}} \\quad\\; \\bigg) + \\bigg(\\frac{1}{N} \\sum\\limits_{j=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_j} \\cdot \\frac{-1}{\\sqrt{\\sigma^2 + \\epsilon}}\\bigg) + \\bigg(-0.5 \\sum\\limits_{j=1}^N \\frac{\\partial E}{\\partial \\hat{x}_j} \\cdot (x_j - \\mu) \\cdot (\\sigma^2 + \\epsilon)^{-1.5} \\cdot \\dfrac{2(x_i - \\mu)}{N} \\bigg) \\qquad \\\\\n",
    "&= \\bigg(\\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot (\\sigma^2 + \\epsilon)^{-0.5} \\bigg) - \\bigg(\\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\sum\\limits_{j=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_j} \\;\\, \\bigg) - \\bigg(\\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\sum\\limits_{j=1}^N \\frac{\\partial E}{\\partial \\hat{x}_j} \\cdot \\frac{(x_j - \\mu)}{\\sqrt{\\sigma^2 + \\epsilon}} \\bigg )\\qquad \\\\\n",
    "&= \\bigg(\\frac{\\partial E}{\\partial \\hat{x}_i} \\cdot (\\sigma^2 + \\epsilon)^{-0.5} \\bigg) - \\bigg(\\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\sum\\limits_{j=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_j} \\;\\, \\bigg) - \\bigg(\\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\cdot \\hat{x}_i \\sum\\limits_{j=1}^N \\frac{\\partial E}{\\partial \\hat{x}_j} \\cdot \\hat{x}_j \\bigg )\\qquad \\\\\n",
    "&= \\boxed{\\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\bigg [N \\frac{\\partial E}{\\partial \\hat{x}_i} - \\sum\\limits_{j=1}^N  \\frac{\\partial E}{\\partial \\hat{x}_j} - \\hat{x}_i \\sum\\limits_{j=1}^N \\frac{\\partial E}{\\partial \\hat{x}_j} \\cdot \\hat{x}_j\\bigg ]} \\qquad \\\\\n",
    "&= \\frac{(\\sigma^2 + \\epsilon)^{-0.5}}{N} \\bigg [N \\frac{\\partial E}{\\partial y_i} \\cdot \\gamma - \\sum\\limits_{j=1}^N  \\frac{\\partial E}{\\partial y_j} \\cdot \\gamma - \\hat{x}_i \\sum\\limits_{j=1}^N \\frac{\\partial E}{\\partial y_j} \\cdot \\gamma \\cdot \\hat{x}_j\\bigg ] \\qquad \\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = np.random.rand(*y.shape)\n",
    "\n",
    "# Propaga el gradiente hacia atrás a través de la convolución\n",
    "layer_grad = layer.backward(g)\n",
    "layer_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "\n",
    "w = torch.from_numpy(w).to(torch.float)\n",
    "b = torch.from_numpy(b).to(torch.float)\n",
    "x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "batch_norm = tnn.BatchNorm1d(din)\n",
    "batch_norm.weight.data = w\n",
    "batch_norm.bias.data = b\n",
    "\n",
    "x.requires_grad = True\n",
    "batch_norm.weight.requires_grad = True\n",
    "batch_norm.bias.requires_grad = True\n",
    "\n",
    "y_torch = batch_norm(x)\n",
    "y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(y_torch.detach().numpy(),y,tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = torch.from_numpy(g).to(torch.double)\n",
    "\n",
    "# Propaga el gradiente hacia atrás\n",
    "x.grad = None  # Limpiamos los gradientes existentes\n",
    "y_torch.backward(g)\n",
    "\n",
    "# Imprime el gradiente de la imagen de entrada, gamma y beta\n",
    "print(\"Gradiente de la entrada (dE/dx):\")\n",
    "print(x.grad, x.grad.shape)\n",
    "print(\"\\nGradiente de gamma (dE/dγ):\")\n",
    "print(batch_norm.weight.grad, batch_norm.weight.grad.shape)\n",
    "print(\"\\nGradiente de beta (dE/dβ):\")\n",
    "print(batch_norm.bias.grad, batch_norm.bias.grad.shape)\n",
    "\n",
    "# Imprime los parámetros de la capa BatchNorm\n",
    "print(\"\\nParámetros de la capa BatchNorm:\")\n",
    "for name, param in batch_norm.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(x.grad.numpy(),layer_grad[0],tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(batch_norm.weight.grad.numpy(),layer_grad[1]['w'],tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_same(batch_norm.bias.grad.numpy(),layer_grad[1]['b'],tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "batch_size=2\n",
    "din=10 # dimensión de entrada\n",
    "input_shape=(batch_size,din)\n",
    "\n",
    "# Verificar las derivadas de un modelo de BatchNorm\n",
    "# con valores aleatorios de `w`, `b`, y `x`, la entrada\n",
    "layer=nn.BatchNorm(din)\n",
    "\n",
    "\n",
    "utils.check_gradient.common_layer(layer,input_shape,samples=samples,tolerance=1e-5)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('captum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "27cd8888c451505594cd6ce93183113956b3735aa05f73d7c3cf078349bc9fda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
