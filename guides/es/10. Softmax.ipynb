{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import edunn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Softmax\n",
    "\n",
    "\n",
    "Para problemas de clasificación, es útil tener un modelo que pueda generar distribuciones de probabilidad como salida. De esta forma, para un problema de `C` clases, el modelo puede tener como salida un vector de `C` elementos `f(x)=y` donde cada `y_i` es un valor entre 0 y 1 que indica la probabilidad de que el ejemplo `x` pertenezca a la clase `i`. Además, como `y` es una distribución', tenemos que $\\sum_{i=1}^C y_i =1$,\n",
    "\n",
    "Un modelo de regresión lineal puede generar un vector de `C` elementos con los _puntajes_ de cada clase, pero estos valores estarán en el rango $(-\\infty, +\\infty)$, con lo cual nunca podrían cumplir las propiedades de una distribución de probabilidad que mencionamos arriba.\n",
    "\n",
    "En este ejercicio debés implementar la capa `Softmax`, que justamente dado un vector `x` de `C` puntajes por clase, lo convierte en un vector `y` de probabilidades por clase. Para eso, implementa la función Softmax, con $y =(y_1,y_2,...,y_C) = Softmax((x_1,x_2,...,x_C)) = Softmax(x)$, donde cada $y_i$ es la probabilidad de la clase $i$.  Entonces, por ejemplo, dados puntajes de clase `[-5,100,100]`, la función `Softmax` generará las probabilidades `[0,0.5,0.5]`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "El método `forward` utiliza la siguiente fórmula para `y`:\n",
    "\n",
    "$$y= \n",
    "\\frac{[e^{x_1},...,e^{x_c}]}{e^{x_1}+...+e^{x_c}} = \n",
    "\\frac{[e^{x_1},...,e^{x_c}]}{N}$$\n",
    "\n",
    "O visto elemento por elemento, cada valor de $y$ se define como:\n",
    "$$y_i(x) =  \\frac{e^{x_i}}{e^{x_1}+...+e^{x_c}} $$\n",
    "\n",
    "Aquí, utilizamos la función exponencial ($e^x$) para convertir cada puntaje del vector `x` del rango $(-\\infty, +\\infty)$ al rango $(0, +\\infty)$, ya que la función exponencial tiene esa imagen. \n",
    "\n",
    "Además, $e^x$ es monótonamente creciente en $x$, entonces a valores mayores de $x$, valores mayores de $e^x$, con lo cual si un puntaje es alto, también lo será la probabilidad y viceversa. \n",
    "\n",
    "Ahora bien, además cada elemento se divido por el valor $N$, que sirve para normalizar los valores, obteniendo:\n",
    "1. Valores entre 0 y 1\n",
    "2. Que la suma de valores sea 1\n",
    "\n",
    "Es decir, los axiomas de una distribución de probabilidad como mencionamos anteriormente\n",
    "\n",
    "Implementá el método `forward` de la clase `Softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,100],\n",
    "             [0,100,0.0],\n",
    "             [100,100,0.0],\n",
    "             [50,50,0.0],\n",
    "             [1,1,1],],dtype=float)\n",
    "\n",
    "layer=nn.Softmax()\n",
    "y = np.array([[ 0, 0,  1],\n",
    "               [0, 1, 0.0],\n",
    "             [0.5, 0.5, 0.0],\n",
    "              [0.5, 0.5, 0.0],\n",
    "             [1/3,1/3,1/3]],dtype=float)\n",
    "\n",
    "utils.check_same(y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "El método `backward` de la función softmax requiere varios pasos, ya que debido a la normalización cada salida de la softmax depende de cada entrada.\n",
    "\n",
    "Para no hacer tan largo este cuaderno, los detalles del cálculo de la derivada están en un  [apunte en línea](http://facundoq.github.io/guides/softmax_derivada.html).\n",
    "\n",
    "Implementá el método `backward` de la clase `Softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "\n",
    "# number of random values of x and dEdy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds 3\n",
    "layer=nn.Softmax()\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
