{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from edunn import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa AddConstant\n",
    "\n",
    "En este ejercicio debés implementar la capa `AddConstant`, que agrega un valor constante a cada una de sus entradas para generar su salida.\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[3.5,-7.2,5.3]` y la capa AddConstant agrega el valor `3.0` a su entrada, entonces la salida `y` será `[6.5,-4.2,8.3]`.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n",
    "\n",
    "Esta capa funciona para arreglos de entrada de cualquier tamaño, ya sean vectores, matrices, o arreglos con más dimensiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "El método `forward` calcula la salida `y` en base a la entrada `x`, como explicamos antes. En términos formales, si la constante a sumar es $C$ y la entrada a la capa es $x = [x_1,x_2,...,x_n]$, entonces la salida $y$ es:\n",
    "\n",
    "$\n",
    "y([x_1,x_2,...,x_n])= [x_1+C,x_2+C,...,x_n+C]\n",
    "$\n",
    "\n",
    "Comenzamos con el método `forward` de la clase `AddConstant`. Debés completar el código entre los comentarios:\n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION START \"\"\"```\n",
    "\n",
    "y\n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION END \"\"\"```\n",
    "\n",
    "Entre esas líneas hay código por defecto (en este caso `y = np.zeros_like(x)`) solo para evitar errores del intérprete de Python hasta que escribas tu implementación. Podés borrar esa línea para tener una implementación limpia! \n",
    "\n",
    "No olvides ejecutar la celda después de agregar tu código!\n",
    "\n",
    "Luego, verificar con la siguiente celda una capa que suma 3 y otra que suma -3. Si ambos chequeos son correctos, verás dos mensajes de <span style='background-color:green;color:white; '>éxito (success)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn import Model\n",
    "\n",
    "class AddConstant(Model):\n",
    "    \"\"\"\n",
    "    A layer that adds a constant. This layer has NO parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: float, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param x: input vector/matrix\n",
    "        :return: `x + a`, constant value, stored in `self.value`\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "        y = np.zeros_like(x)\n",
    "        \"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "\n",
    "        return y\n",
    "\n",
    "    def backward(self, dE_dy: np.ndarray):\n",
    "        \"\"\" YOUR IMPLEMENTATION START \"\"\"\n",
    "        dE_dx = np.zeros_like(dE_dy)\n",
    "        \"\"\" YOUR IMPLEMENTATION END \"\"\"\n",
    "\n",
    "        dE_dp = {}  # no parameters, no derivatives\n",
    "        return dE_dx, dE_dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5, -7.2, 5.3],\n",
    "              [-3.5, 7.2, -5.3]])\n",
    "\n",
    "layer = AddConstant(3)\n",
    "y = np.array([[6.5, -4.2, 8.3],\n",
    "              [-0.5, 10.2, -2.3]])\n",
    "utils.check_same(y, layer.forward(x))\n",
    "\n",
    "layer = AddConstant(-3)\n",
    "y = np.array([[0.5, -10.2, 2.3],\n",
    "              [-6.5, 4.2, -8.3]])\n",
    "utils.check_same(y, layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Además del cálculo de la salida de la capa, la misma debe poder propagar hacia atrás el gradiente del error de la red. Para eso, debés implementar el método `backward` que recibe $\\frac{dE}{dy}$, es decir, las derivadas parciales del error respecto a la salida (gradiente) de esta capa , y devolver $\\frac{dE}{dx}$, las derivadas parciales del error respecto de las entradas de esta capa. \n",
    "\n",
    "Para la capa `AddConstant` el cálculo del gradiente de la salida $y$ respecto de la entrada $x$ es fácil, pero recordemos que tenemos que devolver el gradiente de $E$ respecto de $x$. Recordemos entonces la forma de la salida:\n",
    "\n",
    "$\n",
    "y([x_1,x_2,...,x_n])= [x_1+C,x_2+C,...,x_n+C]\n",
    "$\n",
    "\n",
    "Nuestro objetivo es calcular $\\frac{dE}{dx}$. Para eso, nos enfocaremos en $\\frac{dE}{dx_i}$, es decir la derivada del error respecto de una entrada en particular. \n",
    "Luego, aplicando regla de la cadena, podemos escribir $\\frac{dE}{dx_i}$ como:\n",
    "\n",
    "\n",
    "$\\frac{dE}{dx_i} = \\frac{dE}{dy} * \\frac{dy}{dx_i} = \\sum_j \\frac{dE}{dy_j} * \\frac{dy_j}{dx_i} $\n",
    "\n",
    "\n",
    "Nos conviene entonces ver cada elemento de la salida escrito como:\n",
    "\n",
    "$y_i(x)=x_i+C$\n",
    "\n",
    "Y por ende:\n",
    "\n",
    "$\\frac{dy_i}{dx_i} = 1 + 0  =1$\n",
    "\n",
    "Como no hay interacción entre elementos de distinto índice, es decir, $y_i$ solo depende de $x_i$, en la sumatoria anterior si $i\\neq j$  entonces $\\frac{dy_j}{dx_i}=0$. Por eso podemos quitar la sumatoria y ahora utilizar la regla de la cadena solo con $y_i$:\n",
    "\n",
    "$\\frac{dE}{dx_i} = \\frac{dE}{dy} * \\frac{dy}{dx_i} = \\sum_j \\frac{dE}{dy_j} * \\frac{dy_j}{dx_i} =  \\frac{dE}{dy_i} * \\frac{dy_i}{dx_i} $\n",
    "\n",
    "Sabiendo que  $\\frac{dy_i}{dx_i} = 1$\n",
    "\n",
    "$ \\frac{dE}{dx_i} = \\frac{dE}{dy} * \\frac{dy}{dx_i} = \\frac{dE}{dy_i} * 1 = \\frac{dE}{dy_i} $\n",
    "\n",
    "Escribiendo entonces en forma vectorial para el vector x:\n",
    "\n",
    "$ \\frac{dE}{dx} = [ \\frac{dE}{dy_1}, \\frac{dE}{dy_2}, ..., \\frac{dE}{dy_n} ] = \\frac{dE}{dy} $\n",
    "\n",
    "Con lo cual la capa simplemente propaga los gradientes de la capa siguiente.\n",
    "\n",
    "Notar que por simplicidad, en el código llamamos a estos vectores `dEdy` y `dEdx`. También aclaramos que en este caso $C$ es una constante y NO un parámetro de la red, por lo cual no debemos calcular $\\frac{dE}{dC}$.\n",
    "\n",
    "La verificación del gradiente se hace con la función `check_gradient_layer_random_sample`. Esta función genera muestras aleatorias de `x` y de `dEdy`, y luego compara el gradiente analítico (tu implementación) contra el gradiente numérico. \n",
    "\n",
    "El gradiente numérico _aproxima_ las derivadas parciales utilizando la fórmula de la derivada $\\frac{df(x)}{dx}= \\lim_{h→0} \\frac{f(x+h)-f(x)}{h}$ con un valor de $h$ muy pequeño ($h=10^{-12}$). En realidad, para una una mejor aproximación, utiliza la derivada _centrada_, cuya fórmula es $\\frac{df(x)}{dx}_{aprox}= \\frac{f(x+h)-f(x-h)}{2h}$. Esta técnica de _verificación de gradientes_ es una técnica estándar para comprobar la implementación correcta de una red neuronal.\n",
    "\n",
    "Completar el código en la función `backward` de la capa `AddConstant` entre los comentarios:\n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION START \"\"\"```\n",
    "\n",
    "y\n",
    "\n",
    "```\"\"\" YOUR IMPLEMENTATION END \"\"\"```\n",
    "\n",
    "No olvides ejecutar la celda después de agregar tu código!\n",
    "\n",
    "Y luego verificar con la siguiente celda una capa que suma 3 y otra que suma -3. Si ambos chequeos son correctos, verás dos mensajes de <span style='background-color:green;color:white; '>éxito (success)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edunn.utils import check_gradient\n",
    "\n",
    "# number of random values of x and dEdy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape = (5, 2)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds 3\n",
    "layer = AddConstant(3)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds -4\n",
    "layer = AddConstant(-4)\n",
    "check_gradient.common_layer(layer, input_shape, samples=samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre de capa\n",
    "\n",
    "Los nombres de la capa se asignan de forma automática al crearse un objeto de la misma, e idealmente deben ser únicos para poder diferenciar distintas capas aunque sean del mismo tipo.\n",
    "\n",
    "Por defecto, al ejecutar `AddConstant(3)` se crea un objeto de esta capa, y se le pone el nombre de la `AddConstant_i` donde `i` va incrementándose automáticamente a medida que creamos objetos de la misma clase `AddConstant`. \n",
    "\n",
    "También se puede especificar el nombre de la capa manualmente, para que quede fijo, utilizando el parámetro `name`, por ejemplo con `AddConstant(3, name=\"Una capa que suma 3\")`\n",
    "\n",
    "Todas las capas deben seguir la convención de tener un parámetro `name` para que la librería las identifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = AddConstant(3)\n",
    "print(c1.name)\n",
    "\n",
    "c2 = AddConstant(3)\n",
    "print(c2.name)\n",
    "\n",
    "c3 = AddConstant(3, name=\"Mi primera capa :)\")\n",
    "print(c3.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
