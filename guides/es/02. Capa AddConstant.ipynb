{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplenn import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa AddConstant\n",
    "\n",
    "En este ejercicio debés implementar la capa `AddConstant`, que agrega un valor constante a cada una de sus entradas para generar su salida.\n",
    "\n",
    "Por ejemplo, si la entrada `x` es `[3.5,-7.2,5.3]` y la capa AddConstant agrega el valor `3.0` a su entrada, entonces la salida `y` será `[6.5,-4.2,8.3]`.\n",
    "\n",
    "Tu objetivo es implementar los métodos `forward` y `backward` de esta capa, de modo de poder utilizarla en una red neuronal.\n",
    "\n",
    "Esta capa funciona para arreglos de entrada de cualquier tamaño, ya sean vectores, matrices, o arreglos con más dimensiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "El método `forward` calcula la salida `y` en base a la entrada `x`, como explicamos antes. En términos formales,  si la constante a sumar es $C$ y la entrada a la capa es $x = [x_1,x_2,...,x_n] $, entonces la salida $y$ es:\n",
    "\n",
    "$\n",
    "y([x_1,x2,...,x_n])= [x_1+C,x_2+C,...,x_n+C]\n",
    "$\n",
    "\n",
    "Comenzamos con el método `forward` de la clase `AddConstant`, que podrás encontrar en el archivo `activations.py` de la carpeta `simplenn/models`. Debés completar el código entre los comentarios:\n",
    "\n",
    "````### COMPLETAR INICIO ###````\n",
    "\n",
    "y\n",
    "\n",
    "````### COMPLETAR FIN ###````\n",
    "\n",
    "Y luego verificar con la siguiente celda una capa que suma 3 y otra que suma -3. Si ambos chequeos son correctos, verás dos mensajes de <span style='background-color:green;color:white; '>éxito (success)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n",
      "\u001b[42m\u001b[30mSUCCESS :)\u001b[0m Arrays are equal (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3.5,-7.2,5.3],\n",
    "             [-3.5,7.2,-5.3]])\n",
    "\n",
    "layer=sn.AddConstant(3)\n",
    "y = np.array([[ 6.5, -4.2,  8.3],\n",
    "               [-0.5, 10.2, -2.3]])\n",
    "utils.check_same(y,layer.forward(x))\n",
    "\n",
    "layer=sn.AddConstant(-3)\n",
    "y = np.array([[  0.5, -10.2,   2.3],\n",
    "             [ -6.5,   4.2,  -8.3]])\n",
    "utils.check_same(y,layer.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Además del cálculo de la salida de la capa, la misma debe poder propagar hacia atrás el gradiente del error de la red. Para eso, debés implementar el método `backward` que recibe $\\frac{δE}{δy}$, es decir, las derivadas parciales del error respecto a la salida (gradiente) de esta capa , y devolver $\\frac{δE}{δx}$, las derivadas parciales del error respecto de las entradas de esta capa. \n",
    "\n",
    "Para la capa `AddConstant` el cálculo del gradiente de la salida $y$ respecto de la entrada $x$ es fácil, pero recordemos que tenemos que devolver el gradiente de $E$ respecto de $x$. Recordemos entonces la forma de la salida:\n",
    "\n",
    "$\n",
    "y([x_1,x_2,...,x_n])= [x_1+C,x_2+C,...,x_n+C]\n",
    "$\n",
    "\n",
    "Nuestro objetivo es calcular $\\frac{δE}{δx}$. Para eso, nos enfocaremos en $\\frac{δE}{δx_i}$, es decir la derivada del error respecto de una entrada en particular. \n",
    "Luego, aplicando regla de la cadena, podemos escribir $\\frac{δE}{δx_i}$ como:\n",
    "\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\sum_j \\frac{δE}{δy_j} * \\frac{δy_j}{δx_i} $\n",
    "\n",
    "\n",
    "No conviene entonces ver cada elemento de la salida escrito como:\n",
    "\n",
    "$y_i(x)=x_i+C$\n",
    "\n",
    "Y por ende:\n",
    "\n",
    "$\\frac{δy_i}{δx_i} = 1 + 0  =1$\n",
    "\n",
    "Como no hay interacción entre elementos de distinto índice, es decir, $y_i$ solo depende de $x_i$, en la sumatoria anterior si $i\\neq j$  entonces $\\frac{δy_j}{δx_i}=0$. Por eso podemos quitar la sumatoria y ahora utilizar la regla de la cadena solo con $y_i$:\n",
    "\n",
    "$\\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\sum_j \\frac{δE}{δy_j} * \\frac{δy_j}{δx_i} =  \\frac{δE}{δy_i} * \\frac{δy_i}{δx_i} $\n",
    "\n",
    "Sabiendo que  $\\frac{δy_i}{δx_i} = 1$\n",
    "\n",
    "$ \\frac{δE}{δx_i} = \\frac{δE}{δy} * \\frac{δy}{δx_i} = \\frac{δE}{δy_i} * 1 = \\frac{δE}{δy_i} $\n",
    "\n",
    "Escribiendo entonces en forma vectorial para el vector x:\n",
    "\n",
    "$ \\frac{δE}{δx} = [ \\frac{δE}{δy_1}, \\frac{δE}{δy_2}, ..., \\frac{δE}{δy_n} ] = \\frac{δE}{δy} $\n",
    "\n",
    "Con lo cual la capa simplemente propaga los gradientes de la capa siguiente.\n",
    "\n",
    "Notar que por simplicidad, en el código llamamos a estos vectores `δEδy` y `δEδx`. También aclaramos que en este caso $C$ es una constante y NO un parámetro de la red, por lo cual no debemos calcular $\\frac{δE}{δC}$.\n",
    "\n",
    "La verificación del gradiente se hace con la función `check_gradient_layer_random_sample`. Esta función genera muestras aleatorias de `x` y de `δEδy`, y luego compara el gradiente analítico (tu implementación) contra el gradiente numérico. \n",
    "\n",
    "El gradiente numérico _aproxima_ las derivadas parciales utilizando la fórmula de la derivada $\\frac{δf(x)}{δx}= \\lim_{h→0} \\frac{f(x+h)-f(x)}{h}$ con un valor de $h$ muy pequeño ($h=10^{-12}$). En realidad, para una una mejor aproximación, utiliza la derivada _centrada_, cuya fórmula es $\\frac{δf(x)}{δx}_{aprox}= \\frac{f(x+h)-f(x-h)}{2h}$. Esta técnica de _verificación de gradientes_ es una técnica estándar para comprobar la implementación correcta de una red neuronal.\n",
    "\n",
    "Completar el código en la función `backward` de la capa `AddConstant` entre los comentarios:\n",
    "\n",
    "````### COMPLETAR INICIO ###````\n",
    "\n",
    "y\n",
    "\n",
    "````### COMPLETAR FIN ###````\n",
    "\n",
    "Y luego verificar con la siguiente celda una capa que suma 3 y otra que suma -3. Si ambos chequeos son correctos, verás dos mensajes de <span style='background-color:green;color:white; '>éxito (success)</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[104m\u001b[30mAddConstant_2 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1000 partial derivatives checked (100 random input samples)\n",
      "\u001b[104m\u001b[30mAddConstant_3 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 1000 partial derivatives checked (100 random input samples)\n"
     ]
    }
   ],
   "source": [
    "from simplenn.utils import check_gradient\n",
    "\n",
    "\n",
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "\n",
    "input_shape=(5,2)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds 3\n",
    "layer=sn.AddConstant(3)\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)\n",
    "\n",
    "# Test derivatives of an AddConstant layer that adds -4\n",
    "layer=sn.AddConstant(-4)\n",
    "check_gradient.common_layer(layer,input_shape,samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nombre de capa\n",
    "\n",
    "Los nombres de la capa se asignan de forma automática al crearse un objeto de la misma, e idealmente deben ser únicos para poder diferenciar distintas capas aunque sean del mismo tipo.\n",
    "\n",
    "Por defecto, al ejecutar `AddConstant(3)` se crea un objeto de esta capa, y se le pone el nombre de la `AddConstant_i` donde `i` va incrementándose automáticamente a medida que creamos objetos de la misma clase `AddConstant`. \n",
    "\n",
    "También se puede especificar el nombre de la capa manualmente, para que quede fijo, utilizando el parámetro `name`, por ejemplo con `AddConstant(3,name=\"Una capa que suma 3\")`\n",
    "\n",
    "Todas las capas deben seguir la convención de tener un parámetro `name` para que la librería las identifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddConstant_4\n",
      "AddConstant_5\n",
      "Mi primera capa\n"
     ]
    }
   ],
   "source": [
    "c1 = sn.AddConstant(3)\n",
    "print(c1.name)\n",
    "\n",
    "c2 = sn.AddConstant(3)\n",
    "print(c2.name)\n",
    "\n",
    "c3 = sn.AddConstant(3, \"Mi primera capa\")\n",
    "print(c3.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
