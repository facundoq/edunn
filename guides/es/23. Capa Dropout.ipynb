{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import edunn as nn\n",
    "from edunn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Dropout\n",
    "\n",
    "Dropout actúa como una técnica de _regularización_ que elimina o desactiva nodos en la propagación hacia adelante, lo que hace que la red sea menos propensa al sobreajuste al evitar que la red dependa demasiado de cualquier neurona individual. Esta capa no tiene parámetros.\n",
    "\n",
    "* En la propagación hacia adelante, las entradas se establecen en cero con una probabilidad $p$, y de lo contrario se escalan por $\\frac{1}{1-p}$.\n",
    "\n",
    "  - La propagación hacia adelante durante el entrenamiento solo se utiliza para configurar la red para la propagación hacia atrás, donde la red se modifica realmente.\n",
    "  \n",
    "  - Para cada neurona individual en la capa, podemos decir que $x \\sim B(1, p)$, ya que estamos considerando un solo \"experimento\" (la activación o desactivación de la neurona) con una probabilidad de éxito de $p$.\n",
    "\n",
    "* En la propagación hacia atrás, los gradientes para las mismas unidades eliminadas se anulan; otros gradientes se escalan por el mismo factor de $\\frac{1}{1-p}$.\n",
    "\n",
    "  - Es decir, si un nodo fue eliminado por la capa Dropout, entonces su influencia (el gradiente) en los pesos salientes es también 0 (ya que $0 * w_i = 0$). En resumen, la propagación hacia atrás funciona como siempre.\n",
    "\n",
    "\n",
    "> NOTA: tener en cuenta que durante la fase de prueba o validación, todas las neuronas están activas (es decir, no se aplica Dropout) para obtener una predicción basada en toda la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "din=10\n",
    "batch_size=2\n",
    "\n",
    "x = np.random.rand(batch_size,din)\n",
    "                       \n",
    "layer=nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897,\n",
       "        0.42310646, 0.9807642 , 0.68482974, 0.4809319 , 0.39211752],\n",
       "       [0.34317802, 0.72904971, 0.43857224, 0.0596779 , 0.39804426,\n",
       "        0.73799541, 0.18249173, 0.17545176, 0.53155137, 0.53182759]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.39293837, 0.57227867, 0.45370291, 1.10262954, 1.43893794,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.78423504],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.35090351, 1.06310275, 1.06365517]]),\n",
       " (2, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.2479059 , 0.23123679, 0.63457096, 0.82965242, 1.73261832,\n",
       "         0.        , 0.        , 0.        , 0.        , 1.22578905],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 1.36260153, 1.75091368, 1.02084467]]),\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = np.random.rand(*y.shape)\n",
    "\n",
    "# Propaga el gradiente hacia atrás a través de la convolución\n",
    "layer_grad = layer.backward(g)\n",
    "layer_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897,\n",
       "         0.42310646, 0.9807642 , 0.68482974, 0.4809319 , 0.39211752],\n",
       "        [0.34317802, 0.72904971, 0.43857224, 0.0596779 , 0.39804426,\n",
       "         0.73799541, 0.18249173, 0.17545176, 0.53155137, 0.53182759]]),\n",
       " (2, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from edunn.model import Phase\n",
    "layer.set_phase(Phase.Test)\n",
    "y = layer.forward(x)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones con PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3929, 0.5723, 0.0000, 1.1026, 1.4389, 0.8462, 0.0000, 0.0000, 0.9619,\n",
       "         0.0000],\n",
       "        [0.6864, 1.4581, 0.8771, 0.1194, 0.0000, 1.4760, 0.0000, 0.3509, 1.0631,\n",
       "         0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as tnn\n",
    "\n",
    "x = torch.from_numpy(x).to(torch.float)\n",
    "\n",
    "# Definimos la capa Dropout\n",
    "dropout = tnn.Dropout(p=0.5)\n",
    "\n",
    "x.requires_grad = True\n",
    "\n",
    "y_torch = dropout(x)\n",
    "y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente de la entrada (δE/δx):\n",
      "tensor([[1.2479, 0.2312, 0.0000, 0.8297, 1.7326, 0.5009, 0.0000, 0.0000, 1.0390,\n",
      "         0.0000],\n",
      "        [0.2413, 1.6527, 1.2061, 1.0901, 0.0000, 0.6082, 0.0000, 1.3626, 1.7509,\n",
      "         0.0000]]) torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define el gradiente de la salida\n",
    "g = torch.from_numpy(g).to(torch.double)\n",
    "\n",
    "# Propaga el gradiente hacia atrás\n",
    "x.grad = None  # Limpiamos los gradientes existentes\n",
    "y_torch.backward(g)\n",
    "\n",
    "# Imprime el gradiente de la imagen de entrada\n",
    "print(\"Gradiente de la entrada (δE/δx):\")\n",
    "print(x.grad, x.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6965, 0.2861, 0.2269, 0.5513, 0.7195, 0.4231, 0.9808, 0.6848, 0.4809,\n",
       "         0.3921],\n",
       "        [0.3432, 0.7290, 0.4386, 0.0597, 0.3980, 0.7380, 0.1825, 0.1755, 0.5316,\n",
       "         0.5318]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout.eval()\n",
    "y_torch = dropout(x)\n",
    "y_torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('captum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27cd8888c451505594cd6ce93183113956b3735aa05f73d7c3cf078349bc9fda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
