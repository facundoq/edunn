{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils\n",
    "import simplenn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa de Error Cuadrático\n",
    "\n",
    "En este ejercicio debés implementar la capa de error `SquaredError`, que permite calcular el error de un lote de ejemplos. \n",
    "\n",
    "Las capas de error son diferentes a las capas normales por dos motivos:\n",
    "\n",
    "1. No solo tienen como entrada la salida de la capa anterior, sino también el valor esperado de la capa anterior (`y` e `y_true`)\n",
    "2. Para un lote de $n$ ejemplos, su salida es de tamaño $n$. Es decir, indican el valor del error de cada ejemplo con un escalar (número real).\n",
    "\n",
    "Además, para simplificar, asumimos que las capas de error no tienen parámetros. No obstante, deben poder realizar la operación `backward` de modo de poder propagar hacia atrás el gradiente del error a la red.\n",
    "\n",
    "La capa de error cuadrático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método forward\n",
    "\n",
    "\n",
    "El método `forward` de la capa `SquaredError` simplemente debe calcular la distancia euclídea al cuadrado entre `y`, los valores producidos por la red, e `y_true`, el valor esperado por la misma.\n",
    "\n",
    "Por ejemplo, si $y=[2,-2]$ y $y_{true}=[3,3]$, entonces la salida de la capa es:\n",
    "\n",
    "$$E(y,y_{true})=d_2(y,y_{true})=d_2([2,-2],[3,3])=(2-3)^2+(-2-3)^2 = 1^2+(-5)^2=1+25=26$$\n",
    "\n",
    "En general, dados dos vectores $a=[a_1,\\dots,a_n]$ y $b=[b_1,\\dots,b_n]$, la distancia euclídea al cuadrado $d_2$ es:\n",
    "\n",
    "$$\n",
    "d_2(a,b)= d_2([a_1,\\dots,a_n],[b_1,\\dots,b_n]) =(a_1-b_1)^2+\\dots+(a_n-b_n)^2\n",
    "$$\n",
    "\n",
    "En el caso de un lote de ejemplos, el cálculo es independiente para cada ejemplo. Es importante entonces que la suma de las diferencias al cuadrado se haga por cada ejemplo (fila) y no por cada característica (columna).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m Arrays are equal :) (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[2,-2],\n",
    "             [-4,4]])\n",
    "y_true = np.array([[3,3],\n",
    "             [-5,2]])\n",
    "\n",
    "\n",
    "layer=sn.SquaredError()\n",
    "E=np.array([2,29])\n",
    "utils.check_same(E,layer.forward(y,y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método backward\n",
    "\n",
    "Ahora puedes calcular el error de una red, bien! Esta es la capa final de la red cuando se está entrenando. Por ende el método backward de una capa de error no recibe $\\frac{δE}{δy}$; de hecho, debe calcularlo directamente a partir de $y$, $y_{true}$, y la definición del error. Además tampoco hay parámetros.\n",
    "\n",
    "Por ende, en este caso, la derivada es simple. Solo debemos calcular $\\frac{δE}{δy}$, la derivada del error respecto a la salida calculada por la red, $y$.\n",
    "En este caso $E$ es simétrico respecto de sus entradas, así que llamemosla nuevamente $a$ y $b$, y entonces calculemos la derivada respecto del elemento $i$ de $a$ (la de $b$ sería igual):\n",
    "\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa_i} = \\frac{δ((a_1-b_1)^2+\\dots+(a_n-b_n)^2)}{δa_i} \\\\\n",
    "= \\frac{δ((a_i-b_i)^2)}{δa_i} = 2 (a_i-b_i) \\frac{δ((a_i-b_i))}{δa_i} \\\\\n",
    "= 2 (a_i-b_i) 1 = 2 (a_i-b_i)\n",
    "$$\n",
    "Generalizando para todo el vector $a$, entonces:\n",
    "$$\n",
    "\\frac{δE(a,b)}{δa} = 2 (a-b)\n",
    "$$\n",
    "Donde $a-b$ es una resta entre vectores.\n",
    "\n",
    "Nuevamente, como este error es por cada ejemplo, entonces los cálculos son independientes en cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[104m\u001b[30mSquaredError_3 layer:\u001b[0m\n",
      "\u001b[42m\u001b[30mSUCCESS\u001b[0m 600 partial derivatives checked, 100 random input samples)\n"
     ]
    }
   ],
   "source": [
    "from test import check_gradient\n",
    "\n",
    "# number of random values of x and δEδy to generate and test gradients\n",
    "samples = 100\n",
    "batch_size=2\n",
    "features_in=3\n",
    "features_out=5\n",
    "input_shape=(batch_size,features_in)\n",
    "\n",
    "\n",
    "layer=sn.SquaredError()\n",
    "check_gradient.check_gradient_squared_error(layer,input_shape,samples=samples)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error medio o promedio\n",
    "\n",
    "Si bien la capa SquaredError nos permite calcular los errores de cada ejemplo, para obtener una medida del error respecto a un lote o conjunto de ejemplos, tenemos que calcular el promedio de estos errores. Como este cálculo es independiente de la función de error, \n",
    "\n",
    "Nota: Muchas veces a la función de error _de cada ejemplo_ se la llama _loss_, para distinguirla del error promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[30m\u001b[2mSUCCESS\u001b[0m error is 15.5 :) (tolerance 1e-12)\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[2,-2],\n",
    "             [-4,4]])\n",
    "y_true = np.array([[3,3],\n",
    "             [-5,2]])\n",
    "\n",
    "\n",
    "layer=sn.MeanError(sn.SquaredError())\n",
    "E=15.5\n",
    "utils.check_same_float(E,layer.forward(y,y_true),title=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal\n",
    "\n",
    "Ahora que tenemos todos los elementos, podemos definir y entrenar nuestro primer modelo regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-598388f5ffb9>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-598388f5ffb9>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print((,))\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "x, y = datasets.load(\"study_regression_1d\")\n",
    "\n",
    "n, din = x\n",
    "n, dout = x\n",
    "\n",
    "model = sn.Sequential([\n",
    "    sn.Linear(din,dout),\n",
    "    sn.Bias(dout)\n",
    "])\n",
    "optimizer = sn.GradientDescent(lr=0.001)\n",
    "epochs = 10\n",
    "batch_size = 4\n",
    "\n",
    "error = sn.MeanError(sn.CrossEntropyWithLabels())\n",
    "history = model.fit(x,y,error,epochs,batch_size,optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "[[2. 2. 2. 2.]]\n",
      "[[4.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "a=np.ones((2,4))\n",
    "print(a)\n",
    "print(a.sum(axis=0,keepdims=True))\n",
    "print(a.sum(axis=1,keepdims=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
